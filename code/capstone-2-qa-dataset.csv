course;year;question;answer
Data Engineering Zoomcamp;2023;How much of an effort would it be to use AWS instead of GCP for assignments?;"Alexey_x000D_
Probably more than you want to put in. I mean, if you have time, why not? But we will not be able to support you. I struggle to come up with an estimate. Does anyone here on this call have an estimate?_x000D_
Victoria_x000D_
I think the hardest part is – you shouldn't be a data engineer if you’re taking this course. You shouldn't have the knowledge that we're teaching. And if you don't have the knowledge, you're trying to learn it, and then everything is showing you something else. On top of that, you want to learn something new on your own that you won't have support for. That's going to be really hard. If you really want AWS because they use AWS at your work, they're going to help you with that one. It's probably not going to be worth it. It's going to be very, very stressful. I would add at least two hours on top of the normal hours."
Machine Learning Zoomcamp;2022;For backend engineers, please suggest some practical hands-on machine learning courses online.;I mean, I don't know why you asked me that, because what do you expect me to answer? [chuckles] Of course, take this course that you're already taking.
Machine Learning Zoomcamp;2022;Is it acceptable to do homework 2 using Scikit Learn?;Yes, but I cannot guarantee that you will arrive at the same answer as without it. To be on the safe side, it’s better to use the materials from the course. Nothing is stopping you from trying it with Scikit Learn as well. If you think it's more useful for you to do it this way, then by all means do it. Again, homework is just for checking your understanding of the course content. You decide what you do with your time. I would recommend first understanding how to implement linear regression with NumPy. Then in week three, when we start covering SciKit Learn, you can just use it from that point on.
Data Engineering Zoomcamp;2023;Can you explain the difference between data engineering, data science, and data analysis?;Yes, I can. I already showed you an article, Roles in a Data Team. Just go through this article, where I cover all these roles. Of course, the opinion of what these roles do varies in the industry. This is my own experience, how I see these roles interact in many companies, specifically in Europe. But I think the ideas are more or less the same across different companies.
Data Engineering Zoomcamp;2023;What are your thoughts on IBM's data engineering course?;I do not know this course. Maybe you could share the link in Slack and we can discuss it.
Machine Learning Zoomcamp;2022;Do you use deep learning often at your work?;I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
Machine Learning Zoomcamp;2022;Is it okay to use my own Git repo and not the auto-created one?;Yes, it’s definitely okay. We actually had a problem with the script that assigns auto-generated ones. That's why that part was removed from the course instructions. Now you should create your own repo and use that for submitting the work. Sorry for the confusion. That happened before my vacation, so I thought, “Okay, I don't want to spend time debugging and figuring out what's wrong with the script. Let's just stick to the same approach we used previously – everyone creates a project in their own repo.” There are some downsides to that, but I wasn't in the mood of going through debugging. So please – it's okay. Please do that.
Machine Learning Zoomcamp;2022;Increasing n_estimators gives better AUC, but also increasing the AUC range between X_train and X_val. Can you give your opinion please?;By “increasing the AUC range,” you mean that the gap between X_train and X_val becomes wider. I would not worry about this too much, as long as your score on validation is good and that you're certain that your model is not overfitting. Then it's fine. Oftentimes, I don't even look at the training score. So that's fine. As long as your relation is good and you trust your validation, then it's fine. If you're uncertain about your validation, what you can do is run cross-validation. Then if you have five scores instead of one, and you can see the standard deviation of this course, you can be more certain that the model is behaving or not behaving well. Pay more attention to validation rather than train.
Data Engineering Zoomcamp;2023;Why not use Ansible Playbook for setup preparations?;Contributions are welcome. If you know how to do this, then please create a guide and then share it with us. You can also create a pull request and include the link in the course repo.
Machine Learning Zoomcamp;2022;Where can I get the given that we are supposed to work on?;You'll have to come up with a project idea yourself. We will help you, of course. We will share some datasets with you, but will need to go through these datasets and you will need to find a project yourself. Then if you're not sure if this is a good project or not, you can just, again, ask that in Slack. You can say “Okay, I found this project. Will this dataset be a good dataset for the project?” I can already tell you that datasets like Iris, Titanic, Buying Quality, MNIST – these datasets that you see in every tutorial – are not good for the project. Try to find something a little bit more unique.
Data Engineering Zoomcamp;2023;What are the advantages of starting a Spark cluster manually (starting master and worker in CLI) versus the one we started in the code? Is there any difference?;"Alexey_x000D_
No. It's just in one way, you start it manually by clicking a button, and in another (in CLI or in the code)  instead of clicking a button, you're using the CLI or you do it from code. I don't think there is any difference. Or maybe you have some concrete example in mind? If so please, ask in Slack, if you want to clarify."
Machine Learning Zoomcamp;2022;How to check homework evaluation?;We haven't evaluated your first homework yet, because you still have some time to submit your homework. After it is done, you will see the evaluation.
Machine Learning Zoomcamp;2022;What are the most terrible errors that data scientists could make nowadays? What should we be informed about?;"The most dangerous one… I can think of two. One is going for the most “exciting” and complex model, when a simple non-machine learning solution will work. For example, you can just do something like group_by and calculate_mean for each of the things. Let's say you want to predict the price of a car. What you can do is run a group_by_query from your data, and you can look at the model, make, and year, do group_by and then use this as a prediction. That's very simple. It doesn't need any machine learning at all. And it's already a good baseline. Then the logistic regression model that we covered is another good baseline that will probably improve over the previous baseline. Maybe that's enough for a start. You don't need a complex XGBoost model (maybe XGBoost is not that complex compared to deep learning). But it's very tempting to go with the fancy new solution and Skip the boring ones – the group_by one, linear regression, decision tree – skip them and go to the more “fun” ones. So that's something that data scientists sometimes do. _x000D_
Then another one is chasing new tools – again, trying new tools instead of focusing on the business problem. I think it's common across all engineers, not just data scientists, but also software engineers, frontend engineers, and so on. I would again, go to our YouTube channel, where there is a talk by Elena, How Your Machine Learning Project will Fail, where Elena talks about the many different reasons of how things can go wrong in a machine learning project. Then this one is somewhat similar, but here Doug talks about search projects, not just machine learning projects, but specifically search. The one with Elena is more general. So check that out. I think that covers the most “terrible” one."
Machine Learning Zoomcamp;2022;Any guide to document a project or repo professionally? Can you recommend a standard or reference example?;"I don't have a standard or reference example. Check out the projects from the past year. I think that will be a good answer and just pick the format you like and follow it. Another source of projects could be our project of the week. If you don't know, for example, this is something we'll have this week. So there is a plan, broken by what you need to do on each of the days and then you need to follow this plan. At the end, you will have a project. There is actually a section there with all the projects. If you take part in these projects, you can add links here. Then you can just go back to some of the projects and look at them. _x000D_
This is my project. I don't think this is a good example, but maybe another source of inspiration – a far better source of inspiration – would be projects from other students, from the previous cohort. There were real gems. This year, too – I saw really good projects. So go through them, find the ones you like, and maybe if you have time, you can create a template for that. You can say, “Okay, this is the reference example. I saw this in this project. I saw this in this project. I combined it.” And reference the examples. People will like it and people will appreciate it. You can also use it for your projects and then share it with the community. That's a good idea."
Machine Learning Zoomcamp;2022;When I train a model in a certain size, when using the prediction, can I use it with a different one?;I guess this is related to the other question. I don't know. Just check it and then you'll see.
Data Engineering Zoomcamp;2023;Is the course Linux friendly?;It is. Linux is the recommended operational system. But MacOs and Windows will also work.
Data Engineering Zoomcamp;2023;With local interfaces of Prefect, the flows seem to use up a lot of space in ~prefect/storage. Is there a good way to manage this space usage?;"Jeff_x000D_
Cleaning up resources on your computer is something that we don't have a direct thing that we do with Prefect, I don't believe. But you could have a script that is scheduled by a Prefect to run and clean out anything that's sitting someplace after a certain amount of time, for example. That's one option if you're saving a lot of results to your local file. If you want to see more about results, and just how they work, I encourage you to check out the data here. _x000D_
You can see information about persisting results and caching – this is one thing that came up on the homework. I know that because earlier in the class, we used a little bit of caching with Kalise. Sometimes, they need people to try to run something in Docker and wouldn't be able to access the cache because it was on your local machine. There are ways to clear that cache. You can refresh that now with a new version of Prefect itself. And you can read about that in the tasks. There are a number of strategies to do that and there's some information in the FAQ and in Slack about that. Refreshing the cache starts with 2.7.8, so make sure you have Prefect 2.7.8 or newer installed."
Machine Learning Zoomcamp;2022;For data science job applications, should one send a cover letter or only sending the CV is enough?;"That, again, depends. If a company wants to have a cover letter, send it. I don't read cover letters, to be honest, even though I’m a hiring manager – I don't read cover letters. I usually look at the CV. Quite often, actually, I try to not even look at the CV. The CV screening is done by a recruiter. She looks at the CV and she probably also looks at the cover letter (I don't know if she actually does). When I do the technical interview, I try not to bias myself and I don't look at the CV. I already trust the recruiter that she did the screening, so the candidate already satisfies the requirements, at least in terms of the keyword match. But I don't want to bias myself too much, that's why I look at the CV maybe after the interview. _x000D_
Sometimes, for example, I have a story where I wanted to apply for a computer vision position, which was in the real estate domain. They were working with some real estate images. I didn't have experience in computer vision. In my cover letter, I said “I don't have experience in computer vision, but worked on a project that is related to real estate and I can talk more about that in the interview.” They said, “Okay, come in.” I then went to the interview and I asked them if they invited me because of the cover letter and they said yes. So sometimes it makes sense. But for many cases – I personally don't care much about them."
Machine Learning Zoomcamp;2022;How to automate the process of transforming a column with different descriptions (string) for each single product into an individual description?;I do not understand this question, to be honest. I need an example. Maybe ask in Slack with… I usually don't like screenshots [chuckles] when you post them in Slack, because when I open a screenshot with code on my mobile, I cannot see anything. But maybe this is a case, when you can actually include a screenshot of what you mean and let's discuss it there.
Data Engineering Zoomcamp;2023;The batch processing is to ingest data (in week 5 this is done with PySpark) but isn't it the same if we use Prefect? Can we ingest data with orchestration?;Batch processing is not only used for ingesting data. That's one of the things. In general, any data transformation is done with batch processing. You can ingest data to your data warehouse, you can transform data that you have in your data warehouse, you can put then data into your data lake, you can get your data from a data lake, transform it to a data warehouse – there are thousands of different options and alternatives. For some cases Prefect is fine. It's lightweight and you can do simple stuff there. But sometimes you just need to process a lot of data and then this is where you would use PySpark. You would not try to do this in Prefect because it will put too much load on your Prefect agents. Usually, in practice, you want to keep them lightweight and delegate all the execution of work to some external computing environments such as PySpark, or Spark in general. Yeah, you can ingest data with orchestration. But when you look at the content of week 5, you'll see that it's much more than that. Prefect can be used to execute PySpark backdrops.
Machine Learning Zoomcamp;2022;What is your definition of a data scientist and what would you advise between that and ML engineer or data engineer (salary wise)?;Salary wise, I don't think it matters. There are all types of salaries. Regarding the difference – there is an article in DataTalks.Club, which is called Roles in a Data Team that describes all the possible roles and what exactly people in the team do. There is the data scientist, data engineer, and machine learning engineer. You can go through this article and see the main differences. But maybe I can just quickly tell you. The data scientist is somebody who works on modeling. This is the first four modules, and then trees, and then deep learning. If you focus on that and you know a bit of deployment, then you're good to go for data science. A machine learning engineer focuses more on engineering. An ML engineer should know some machine learning, not to the same extent as a data scientist, but they should still know some machine learning. And they focus more on deployment. Usually, the way it happens is that both data scientists and machine learning engineers work together on training the model, and then they both work on deployment. But in the first case, the data scientist is kind of the expert on modeling, while the machine learning engineer is the expert on deployment. But typically, both of them should work together. Then, when it comes to a data engineer, this is a person who prepares the data for them – or maybe not for them, but together with them. Let's say, without a data scientist in this process, it will take a lot more time. It's best when both data engineers and data scientists work on this together and then the data engineer explains what is needed for the machine learning project. So please go through this article and hopefully, it will answer all your questions.
Data Engineering Zoomcamp;2023;Can you explain the possible pipeline phase if we can use DBT in the final project?;"Victoria_x000D_
Going back to Spark versus DBT, it's really up to you. You can use whatever you want. The possible pipeline phase, if you wanted to use DBT, will look similar to how your project looked until week four. So you would use whatever scheduler you want. Something that you could use is Airflow – or for you could use Prefect – to load the data that you chose to the backups and then move them to BigQuery. Once you have them in BigQuery, you would use DBT to do the transformation like you’re using Web4 and then you would use any analytics tool of your choice. We saw a database in Looker studio."
Data Engineering Zoomcamp;2023;What's the preferred hashtag?;In the homework forum, you will see that it's #dezoomcamp
Data Engineering Zoomcamp;2023;Could I replace BigQuery with Snowflake? What infrastructure is the most frequent to use in the real world? Or how does one decide in this course?;"In this course, we just made it simpler for you – we use BigQuery because it's a part of Google Cloud Platform. When you register, you get free credits, so you don't need to think about this. In practice, I guess what’s really important here is that the concepts stay the same across different warehouses, whether it’s Redshift, BigQuery, Snowflake, Firebolt, or other stuff. The concepts are similar. _x000D_
BigQuery is quite popular, Snowflake is quite popular, so if you come into a company that uses a particular warehouse, just stick to that. If you're starting a new team and you need to decide, I don't have a good solution for you. You will need to really think about what you need, what kind of features there are available and then evaluate. The answer is – it depends. _x000D_
How to decide what to use in this course? In this course, just go with BigQuery. Play with BigQuery and that should be okay. That should be enough."
Machine Learning Zoomcamp;2022;Can I still be part of the deep learning class? I will appreciate it if you address this on the Telegram channel.;Which deep learning class are you talking about? Maybe I don't understand. If you're talking about the competition – you can still take part in the competition. There is still more than one month to go. It ends on February 1, 2023. One week before the end, you will not be able to submit. The last time to enter the competition will be one week before February 1, 2023. If you're talking about something else, perhaps this module, then it's always open. You can come back to this material when you have time and just take it.
Machine Learning Zoomcamp;2022;How can I see the feedback on my midterm project?;Check out the beginning of this video, where I showed how to do this
Machine Learning Zoomcamp;2022;I have a very big dataset (almost 100 gigabytes) in HDF format. Even loading it is becoming a headache. What is the best practice? Can you suggest something?;"That's also a very generic question. It depends on what kind of data you have. Can you take a sample from this data or do you need to use the entire dataset? Also, as I said, what kind of data is it? Is it images? Is it text? Is it tables? I would just read a part of this. I actually don't know if it's possible with the HDF format. Maybe you can just read the first couple of thousand rows? I don't know. The best practices could be, let's say – the first one is to take a sample from this. _x000D_
The second one is maybe seeing how you can parallelize it with something like Spark. Then again, I don't know exactly what HDF format is and if you can read with Spark. Maybe not. Maybe you will need to first turn it into something else and then read it with something like Spark. Then the third one would be to get a big machine with 500 megs of RAM, for example, and then just read it from there. The last option seems to be the easiest one if you actually need to read all the data. In some cases you might not. Is it related to neural networks? I think some of the data was saved in this format. _x000D_
I might be wrong, but I think you might iterate over this data and then read it in chunks. You don't need to read the entire thing. You just read one gigabyte at a time, for example. That's another strategy - chunking. This is actually something that Spark relies on. When dealing with large datasets, Spark chunks the dataset into small partitions, and then it processes each partition separately. Then it combines the result at the end."
Data Engineering Zoomcamp;2023;I'm a beginner in data engineering but have experience as an analyst working with Python, MySQL, and Tableau. Do you think I can do well here?;Yes. If Michael was here, he would probably confirm this because this is the background he has. He's an analyst. We also had other analysts who did pretty well in the course. So yeah, certainly you can do well. As a beginner in data engineering, I would say that we don't expect any data engineering knowledge for taking this course. If you're a beginner already – if you know some data engineering concepts – then it's more likely that you will do well in this course.
Machine Learning Zoomcamp;2022;Is there an interpretation for W0 (bias term)? Some weights can be negative, so it's not the minimum of the target. Can we interpret it somehow or consider it an abstraction?;Well, in case of classification, if the bias term is negative (for example, in case of churn) then it means that the client is more likely to not churn than churn. It’s similar in linear regression. Again, it depends. If your values are only positive, then negative bias terms can be suspicious. But if you sometimes have negative values and positive values in your data, then nothing is particularly wrong with that. So, it really depends on your problem.
Data Engineering Zoomcamp;2023;How can you help people that can't use GCP because of their location?;For everything we do in this course, it's possible to run things locally. In some cases, when you need to create a Google Cloud Storage bucket or when you need to create a BigQuery table, you cannot do this locally. But for many things, you can. You can set up your Postgres instance, and we show you how to do this. We showed this in the first week’s video. In the Prefect videos, we also show how to do things locally. Then, you will basically need to skip week 3 because you will not be able to use BigQuery. And then week 4, we show how to use DBT with Postgres. Then with Spark, we show how to run things locally. And for week 6, with Confluent, I think you can still get access to Confluent Cloud, but you can also run things locally – run Kafka locally.
Data Engineering Zoomcamp;2023;What are the types of live calls we will have apart from Office Hours? And what are the durations?;Just Office Hours. Like this particular one, not all of them will actually happen live. Sometimes I cannot be there, for example, so we do it asynchronously. But hopefully, you still get to ask your questions and I answer these questions. This is what we’re doing now.
Machine Learning Zoomcamp;2022;Do we have to post about the project publicly like we do to get seven points weekly?;"You don't have to. You can if you want. Remember, this is optional. This is something we talked about already. This is optional. If you want to share your progress publicly, you can do this. If you saw the introduction at the beginning when I was talking about the project – I was going through the forum. We have this thing called “Learning in public links.” The score is capped at 14 points. Every day, you share your progress and at the end, you have 14 links. _x000D_
By the way, I noticed that some people try to be smart and try to “game” the system by submitting non-relevant links. Please don't do that. I understand the desire to get extra points, but remember that this is actually anonymized. Nobody knows yet that this hash is you. That's one thing. Then the second thing is… why do you want to cheat? It's not clear to me. The person who I saw – they didn't cheat, but they tried to game the script that we have for evaluating. That person will lose all the learning in public links. They will not have any “learning in public” at all. Instead of seven points for each of the previous homeworks, they will get zero, and they will receive zero points for the rest of the course. So please don't do that. I don't see a point in this. I didn't think somebody would even try to do that. But life is full of surprises, right? Please, please don't do that. There is no point in doing this. Why do you want to do this? I don't understand. Yeah, but you've been warned."
Data Engineering Zoomcamp;2023;JeffIn the exercises, you make us copy data locally and then later uploaded to GCS / BQ. In an actual production environment, what would be the real approach?;"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. _x000D_
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. _x000D_
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
Machine Learning Zoomcamp;2022;If someone asks you â€œWhat are the red flags when you see a dataset?â€ What would be the best way to answer this? This was actually an interview question.;Well, a few things I always look at are missing values – that's the first thing. Then I also look at min/max values. Sometimes, the missing values are encoded with minus nine and nine or some other values – very low or very high values. You can see these if you do DataFrame.describe. You can also see if you have some categorical data in numerical columns or strings in numerical columns. Things like that. These things come to my mind immediately. Apart from that, it's just usually just doing exploratory data analysis and seeing if anything feels wrong. If something feels wrong, you just try to dig deeper until you understand what's going on there.
Machine Learning Zoomcamp;2022;7 points for not posting on social media is harsh. I'm recovering from mental health issues and try to avoid social media if I can. Please reconsider.;"I'm very sorry to hear about your mental health problems and I want to remind you that posting in social media is not required. You don't have to do this. In fact, many of the students from the previous iteration did not post anything on social media and still were able graduate from the course with a certificate. Since they did the projects, they ended up quite high on the leaderboard, which allowed them to be on the page with top 100 names. If this is what you're after – if you want to end up on that page – just keep on working, don't post on social media, and you'll be fine. _x000D_
Don't worry about the points because, again, nobody knows which of the hashes is you (only you know) and these points are virtual. So don't… it's not required to post on social media. If you don't feel like doing this, then don't. But I think it will be valuable for you. Maybe, after some time, when it becomes easier for you, I do recommend taking a look at social media and posting there. Not right now, but later it will be very valuable for your career."
Machine Learning Zoomcamp;2022;Why is shuffle=False in validation generation dataset but not in training, for deep learning?;"If you look at the notebook, yes, we use shuffle=false. For train, the default shuffle is true. Actually, if you think about this, when you go over a dataset once to evaluate it, it probably shouldn't matter in which order you go over it. That's what shuffling is controlling – when you go over a dataset, you want to shuffle it so the order is random. I think the less randomization you have in the validation dataset, the better. I think that's the reason I shuffle=false here. _x000D_
You can experiment with this – set it to true and to false – and then see if the scores you get are the same ones or not. But I think for validation, it's important that we try to stay away from randomization, just in case. I don't know what can go wrong here, but maybe something can go wrong. You’d rather be careful. That's the reason, I think."
Machine Learning Zoomcamp;2022;Tim, are you taking part in Hacktoberfest? You personally, or BentoML.;"Tim _x000D_
Yes, actually. One of our developers was asking about that the other day. I think we'll try to do something like that._x000D_
Alexey_x000D_
Because I think the idea behind Hacktoberfest – it's very similar to the “good first issue” that we saw. In Hacktoberfest, you need to add an issue, but then there is also an idea that you give some sort of guidance to the person who is contributing. You can tell them how they start, what they should do – it's a bit of hand-holding, but that is my understanding. I think to take part you just need to add an issue. I don't know, maybe you need to register somewhere explicitly."
Data Engineering Zoomcamp;2023;Not able to install TerraForm on Windows.;Sorry, I think they need to know more to help you. But maybe what will actually help you is going to week one, TerraForm GCP and we have the windows.md file that explains how to install Google Cloud SDK and how to install TerraForm. If this doesn't work, then please let us know in Slack. I've already seen a few threads that talk about problems on Windows. I'm quite certain you will find a solution there. If not, ask your question and don't forget to check frequently asked questions first.
Data Engineering Zoomcamp;2023;What is the difference between Airflow/Prefect and Google workflows? It seems like homework 2 can be done using Workflows too. When should we use Prefect and Workflows?;"Jeff_x000D_
As Alexey said many times in the class, you can use whatever tool you want for things. So if you want to accomplish things with Workflows, that's fine. There are different tools out there. I'd say the big difference is that Prefect is agnostic to any cloud. You can easily drop in different storage blocks if you want to, or have just different options. Anyone who's using Python first wants to be able to use lots of different tools. That's a good option there. With Workflows, you're kind of tied to Google's infrastructure at that point, in a pretty significant way. But use a tool that works for you. That’s my advice there."
Machine Learning Zoomcamp;2022;Why do we use Gunicorn and not just Flask?;If we use just Flask, then there is a warning that “Flask is not a production WSGI server.” So what you need to check is what WSGI is and read about this. I don't think I'll have the time or the knowledge to explain it here right now. But the thing with Flask is, it's not a WSGI server, but Gunicorn is. Well, Flask kind of is, but it's like a toy server, let's say – just for development. It will work on your computer only for testing, but it will not scale. It will not be able to process multiple requests coming in parallel. Gunicorn is much more scalable, parallelizable and just better to use. That's the main reason why Gunicorn is needed. For more details, maybe just check out this article – could be a good one. It explains what WSGI is, why we need this application framework (which is Flask) and a WSGI server, which is the actual server.
Machine Learning Zoomcamp;2022;Why does R have a better stats reputation?;For historical reasons. They have been around in this area (in this domain) longer than Python. Python has only relatively recently broken into the data world. Before, it was not used for data. I think it became popular because of IPython and Jupyter Notebooks. You could very quickly experiment with things. For regular software engineers, Python is closer than R and that's why, I guess, they started using Python more and more often. Then good libraries appeared in Python, and slowly Python overtook R. But because it was probably more for engineers, and engineers are not very good at statistics, typically, that's why I guess all the stat packages are still in R. Of course, there is “stats methods”. I don’t remember the name of the package, but I remember that the abbreviation is SM. I think you will be able to find it on Google. Maybe somebody in the chat will write the name of this package.
Machine Learning Zoomcamp;2022;If they have a feature column with yes and no, should they convert it into 1 and 0 integer or OneHotEncoding takes care of that?;I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
Data Engineering Zoomcamp;2023;Iâ€™m not able to join Slack.;I'll show you what to do in this case. When you go here to Slack, there is this form. Click on this form, fill it in, and we will invite you manually.
Data Engineering Zoomcamp;2023;Apart from the docs, where can I learn more about Prefect? Any books or course suggestions?;"Alexey_x000D_
YouTube channel, right?_x000D_
Jeff_x000D_
I'm teaching a two day course on Prefect next week in New York. If anyone's around New York – it's an in person course – you can come and join that. We have a couple in a few other places, too. Coming to London, Washington DC, and San Francisco are on the books. We don't yet have a remote one, but maybe we'll do something in the future on that. Let us know if that would be of interest to you. You can just reach out on Slack. That's the short answer. We don't have a book, although we've talked about working on it. The thing is, we are rapidly improving things, making changes. Just today in Slack chat, I was sharing with people that we have a new Timeline view. Since I recorded these videos, like 3-4 weeks ago, and now we have some new visualizations – we have all kinds of new features. We're still having lots and lots of improvements and updates and exciting things happening, so we need a little bit more standardization there to do something like a book. But that's the short answer. If you want to come join me, we can spend a couple of days together working on it._x000D_
Alexey_x000D_
So I know that you also have people in Berlin, so any chance you will have workshops in Berlin too?_x000D_
Jeff_x000D_
I think there's definitely a chance there. Anna, who you see in Slack is in Berlin and we've had a couple people asking about that. So if that's something that's of interest to people, do definitely reach out."
Machine Learning Zoomcamp;2022;Is there any relation between BentoML and Streamlit or Gradio?;"Tim _x000D_
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months._x000D_
Alexey_x000D_
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models._x000D_
Tim _x000D_
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think._x000D_
Alexey _x000D_
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that._x000D_
Tim _x000D_
Right. Oh, that's a good idea. I hadn't thought of that. Yeah._x000D_
Alexey_x000D_
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. _x000D_
Tim_x000D_
Oh, yeah, that would be cool. [chuckles]_x000D_
Alexey_x000D_
That could be a good Hackathon project, right?_x000D_
Tim _x000D_
Yeah, for sure._x000D_
Alexey_x000D_
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you._x000D_
Tim_x000D_
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. _x000D_
Alexey_x000D_
That's cool. _x000D_
Tim_x000D_
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
Data Engineering Zoomcamp;2023;If I don't do public learning, is it going to be a problem?;Not at all.
Machine Learning Zoomcamp;2022;When working with time series data for a regression problem, is it incorrect to shuffle during the train validation test split?;Yes. For a time series problem, you should never shuffle. For a time series problem, you should do a time-based split. Let's say you have data for one year, you take, for example, the first nine months for training, then two months for validation and one month for test. Or something like that. You need to do a time-based split here and shuffling is wrong. Because if you shuffle, what can happen is that you can accidentally kind of see into the future. In your training dataset, you might have a data point from December – your model will see into the future, and it's not supposed to do this.
Machine Learning Zoomcamp;2022;Cover letters get lost in the ATS (application tracking system). Cover letters are good for career changers. Use a cover letter in the email application and not on job board portals, unless asked.;That's a great suggestion – I cannot answer it better. I can only confirm that I don't look at them. We have an applicant tracking system and I can actually look at the cover letter there. But I don't – because I don't care much, to be honest.
Data Engineering Zoomcamp;2023;From an employerâ€™s perspective, along with this course, what else would you want to see from job applicants in their portfolio? What else should we work on?;Building projects. Maybe build another one, where instead of batch, you use streaming, maybe. If you're interested in a particular employer, try to build a project that is similar to what they’re working on. For example, if you want to apply for Spotify, try to find some music data and build a project for that.
Machine Learning Zoomcamp;2022;What would be the main skills/algorithms/tools that I need to demonstrate in my portfolio to get a computer vision job? Some key factors to highlight?;"To be honest, I don't know what the typical requirements for computer vision jobs are. They can vary from one company to another. I will give you a general piece of advice that I would give in any other case. You need to do some research about what the company is doing and understand what their problems are. What kinds of problems are they solving? Then try to solve a similar problem. For example, if it's a company like where I work – online classifieds – you can go to Kaggle and look at what kind of computer vision problems companies in the online classifieds domain are putting out there. _x000D_
Or just go through the tech blog and see what kind of articles that this company is publishing. Then try to build the project around that. For example, let's take OLX, the company where I work. You can find an article here, Fighting fraud with Triplet Loss. In this article, we show how we use computer vision to find duplicates – how to find image duplicates – and then this article describes that. So if you do something similar in your project, you can just add it to your CV and then talk about this during your interview. That's much better than just a random project. It will certainly help to pique attention. I guess that's the most important factor, try to do some research, find what kind of problems the company is interested in solving and then solve these problems. Then use it to build your portfolio."
Machine Learning Zoomcamp;2022;What do you mean by â€œdecision tree and XGBoost probabilities are not real probabilitiesâ€? What are they then? They are relativities at least, right?;The way the decision tree computes “probabilities” i.e. scores is that it looks at the leaf and in the leaf, it sees “What is the proportion of the customers, let's say, who defaulted?” And then this is the score. But the score does not necessarily represent a good probability from the theoretical point of view. From a practical one, it's fine. But from the theoretical point of view, there are some issues, let's say. Then please refer to this calibration stuff I referred to earlier, because I'm not ready to talk in more details about this right now. For simple projects, I think you will do fine with just using the output of this. Sometimes, as I said, your business requirements might require collaboration – that the output is consistent and that it's real probabilities. In terms of “real probability” remember that the definition of probability from probability theory is “How likely a thing is to happen?” “If I take this client, how likely is this client to leave?” The output of random forest might not be exactly this likelihood. It might be a score that you can use to understand the relative risk of this client to default or not. But it's not exactly a probability in the sense from the probability theory point of view.
Machine Learning Zoomcamp;2022;How to drive imbalanced classes, maybe some pattern or process with these classes?;"I'm trying to understand the question… So the question is “What to do if you have class imbalance, but instead of two classes, you have three classes?” For that, when you define your train_test_split in SciKit Learn they have a thing called stratify. You can put your target variable here and then what this function will do is use these data points (this target) to make sure that the distribution between train, test, and validation of this variable is the same. I'm not sure if what I say makes any sense to you, but maybe I should give more context. _x000D_
Your test, train, and validation should have the same amount of the same distribution for all – for example, if we are talking about binary classification, the amount of zeros and ones should be the same in both train, validation and test. And when we have class imbalance, then sometimes it's difficult to make sure that this happens. Because in a usual case, when we don't use stratification, what can happen is that we have more, let's say, positive examples in the train dataset than in validation – just by chance, because the positive examples are quite rare and stratification makes sure that it doesn't happen. Apart from that, you don't need to do anything else, to my knowledge. Maybe I misunderstood your question, so maybe ask a follow-up question in Slack."
Data Engineering Zoomcamp;2023;For week 1 (ingesting data) I keep on getting error messages.;"Alexey_x000D_
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ."
Data Engineering Zoomcamp;2023;I'm a database developer, but I want to migrate to data engineering. I already know basic Python. Do you think that it will be difficult?;"Alexey_x000D_
From what I see (And Ankush, maybe you can talk more about this) this is a perfect profile for switching to data engineering. Am I right?_x000D_
Ankush_x000D_
Yeah, I think this is a good time to be an engineer. And if you have Python knowledge and some database knowledge, you are already ahead of the curve. It won't be too difficult. It will be challenging, of course. But I hope this course helps you out. And I hope that overall, it would be really worth the effort. Because I think data engineering is a rewarding career for that."
Data Engineering Zoomcamp;2023;What Java aspects will be covered in the course? Will it be Java + Spark?;For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
Machine Learning Zoomcamp;2022;Can you suggest a book about Kubernetes?;I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
Machine Learning Zoomcamp;2022;Can we get clarity on reproducibility requirements? It may not be practical for everyone to run some scripts on their machines, such as neural nets.;As I said last time, you don't have to run it. I do encourage you to run it, to try it, to learn as much as possible. But understand that we have lives and not everyone can have time, or can deal with, problems in somebody else's models. I don't require you to run everything, I encourage you to run. Here “reproducibility” just means checking the code for basic errors, checking that the data is there, checking that it's clear what to do in the readme, there are no errors in the code – maybe you see stack traces, which is a clear indication that there are errors. You can also see the Docker file and check that there are no errors and things like that. So you don't have to run it, but if you have time, if you want to learn, please do run it.
Machine Learning Zoomcamp;2022;How should I choose the most efficient number of epochs? And what does it benefit me if it's less or more?;I guess you're talking about how to select the number of epochs. The way I do it is just run it for 100-200 epochs, and I leave it training. Then I add checkpointing. When I come back, I see what's happening. The problem sometimes with more epochs is the discrepancy you have between the training set, where it often overfits to the training set. You might have 100% accuracy on the training data, but then it's let's say, 70% accuracy on validation. That might be problematic. If I compare this to a case... I'll take some notes so I don't get confused. Let's say you have two models. You have Model A, which you trained for, something like 100 epochs and it has 100% accuracy on train and it has 60% accuracy on validation. That's your Model A. Then you have Model B, which you trained for 10 epochs. It has 70% accuracy on train and 60% accuracy on validation. Here, the difference between train and validation performance is much smaller for the second model – for Model B. It's only a 10% difference. But for the first one, it's 40%. I would go with Model B. I would be more sure that it’s not overfitting – it will not have any surprises on the test dataset. So that's the only concern. But if you add some dropout, if you add some data augmentation, then probably by epoch number 100, you will not have 100% error on train. Usually you don't, because of all this extra regularization stuff. Then it could be that the model that you trained for 100 epochs is more robust because it has seen more variations of your data. It's all case-dependent, I would say. But the main thing I would look at is the learning curve and how far apart the curve for the training error and the validation error are.
Machine Learning Zoomcamp;2022;Data Analysts and Data Scientists are not the same thing. If some of the role tasks overlap, itâ€™s usually not for ML. Maybe you can learn SQL machine learning.;Okay. Yeah
Machine Learning Zoomcamp;2022;How should I evaluate an RMSE to be considered low or high?;"This is very problem-specific. You cannot. If you're evaluating a project and you see that they have this RMSE, there's nothing you can do with this information. There is no criteria to even evaluate this. If you notice, there is no criterion that says, “Okay, the model quality is good or not.” So we don't evaluate this, because all of this is problem-specific. If this is for your project, what you can do is train a baseline. The baseline could be just predicting the average value. _x000D_
Let's say you have all the cars – you take the average price for these cars and then you compute the root mean squared error of this prediction. You just simply predict the average for all the cars. You will get some root mean squared error and then you train a simple model. Then this simple model, hopefully, should be an improvement over the baseline and then you can see what the improvement is. It might be, I don't know – 200, 300%. Then, because you already know the baseline value, you can compare the improvement over the baseline. This is how you can understand if it's low or high. If it's worse than the baseline, it's low, of course. But if it's better than the baseline, then it's better. You can see that, for example, XGBoost gives you even better performance than linear regression, and you can have a relative ranking of the models."
Machine Learning Zoomcamp;2022;Is Julia the future of data science?;Hmm. Probably not. I don't think so. Maybe it is, I don't know. Let's see in 10 years.
Data Engineering Zoomcamp;2023;Is it a good idea to have knowledge of data engineering if we already have knowledge about ML and MLOps?;Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
Data Engineering Zoomcamp;2023;Is it possible to use Prefectural Spark or Google Dataproc?;"Jeff_x000D_
We do have a Spark integration through a couple things. Let's look here in the collections again. We have an integration for you that you could use with Databricks. Spark often runs with Databricks. That's a good option there. Fugue is another tool that could be used. It unifies a number of different ways you could run code, but includes Spark and Dask and Ray also. So that could be a good tool. And if you wanted to write Spark code directly and use it with Prefect, you could integrate with something – we don't have something built out right there, but I think folks do kick off Spark jobs with Prefect. If you want to do things in a couple different ways, there are options there. We don't have a direct Dataproc module in Prefect GCP, or a direct block. But surely, there's an API there, and you can probably kick things back and forth."
Data Engineering Zoomcamp;2023;If anyone wants to contribute to any open source project like Prefect, where do they actually start? I don't understand how Prefectâ€™s source code works at the moment.;"Alexey_x000D_
You pick a project where you want to contribute and usually, they have some contributing guide. If you don't know which project to pick, we have a thing called Open Source Spotlight in our YouTube channel. Here, we invite different open source authors to talk about their tools. The tools are very different. They're not all data engineering-related or machine learning engineering-related. They're all different. You can perhaps go through the tools and if you like any of these tools, usually one of the questions I ask the author is “How can people contribute?” In each video, you will find out how you can contribute to this specific tool."
Data Engineering Zoomcamp;2023;Will it be possible to find a job as a junior in data engineering after completing the course?;Yes. As I’ve already said during the launch stream, you can start applying for jobs right now. You don't actually have to complete the course to find a job. Maybe you already know enough for you to convince an employer to hire you. By that, I mean you already know how to program, you know the command line, and things like that. It will be enough – it will be possible – and many people from the previous iteration of the course did that. They found a job. Some of them, not even as juniors.
Data Engineering Zoomcamp;2023;Looking at a DE portfolio, would every project be an own GCP project (i.e. a new VM for every project) or could one GCP project hold several IRL project setups?;Yeah, I think one project can host several things. I don't know exactly, to be honest. I think there is probably an official recommendation from GCP. Check that out.
Machine Learning Zoomcamp;2022;How does regularization affect the inference output in linear regression, such as the original t-statistics, f-statistics, standard errors?;The best way is just to test it. Take a linear regression, look at all these t-statistics, f-statistic, standard errors, and then play with the regularization parameter, and then see how it changes. Then you will see how it affects it and this effect will be different for different datasets. The answer is – it depends. Just go and test it yourself. To be honest, I don't remember exactly what the statistics show to give you an answer. But this is how I would approach this if I wanted to know it.
Machine Learning Zoomcamp;2022;I want to experiment with AWS outside of the free tier and have services turned on continuously. Any tips or guides to reach low cost (no more than $15/per month)?;"What you can do in AWS is – there is part with billing. You go to billing and you can set budget alerts. Then you can say, “Okay, my budget is $20 per month and if I reach 75% of that, then send me an alert.” You will get an alert and you will know “Okay, I actually spent too much money on this. Now I'm going to go to the AWS console and figure out where I actually spend my money.” And then you can turn off some services you don't want to use, for example Kinesis. We don't use Kinesis in this course, but we use it in the MLOps Zoomcamp course. There, we use Kinesis and it is relatively expensive. _x000D_
Another thing you can do, for example, we cover Kubernetes in this course, and you might accidentally forget to turn it off. Kubernetes is not covered by the free tier. If you want to run Kubernetes, you will have to pay. One thing that happened to me recently is that I killed the Kubernetes cluster, but somehow the notes from this cluster were still running. So they weren't killed – they weren't removed or terminated. Because I had this alert, I got an email saying, “Hey, it seems you spent $80, which is a lot higher than usual.” Then I went to the cost explorer and I saw, “Okay, I am spending money on this issue.” I went there, I noticed the instances that were running them and I killed them. Then I would propose to have multiple alerts like that. _x000D_
Maybe have one for $20, another for $50, another for $70, another for $100. When the first alert goes off, you go there and fix it – you want to make sure you stop spending the money. The second alert (the second threshold) will help you to make sure that you don’t keep spending the money. Also, I actually have some credits from AWS and I wanted to give them away for the participation in these projects. We wanted to do it for this one for recommenders. I didn't do this. _x000D_
For the next one – the next project of the week will be about Fast API and we should totally give these AWS credits to participants. So maybe keep an eye on that and remind me that I promised you that, in case I forget. Then you will get some credits. Not everyone – some of you will get them. If you just commit some project that is not really useful, you will not get credits. But we will probably decide which projects will get credits. I can give you like $50. Of course, it's not a lot, but you can experiment with different AWS stuff."
Machine Learning Zoomcamp;2022;Which companies could you recommend to apply if we successfully finish the courses?;"“CoursES,” plural? I don't know which courses you mean. Probably, I assume you mean the ML Zoomcamp? Maybe other Zoomcamps too? Actually, the answer is the same – if you like the company, just apply. Apply to companies that you like and that's it. I don't feel that there should be any specific answer to this question. It’s up to you. _x000D_
Look at the companies that are hiring around you in the country where you live or companies that are hiring remotely – look if they have positions that you like and apply for these positions. It’s as simple as that. Maybe you wanted to hear specific company names. But, I don't know."
Data Engineering Zoomcamp;2023;Since Prefect is newer in comparison to Airflow, is it being used in the industry particularly by large/mid-scale companies?;I don't have an answer to that. I think all companies use it. It doesn't matter what size the company is. But maybe the fraction of companies that use Prefect is still smaller compared to Airflow.
Machine Learning Zoomcamp;2022;What would be the best techniques to use to improve a linear regression model?;"Usually, it's not about the model in practice. It’s not about the model you use, or the algorithm you use, but it's more about features. You need to work on creating new features if you want to improve your model. Linear regression, for example, is a powerful model – but sometimes, because it's a linear model, there are some limitations. In many cases, for example, XGBoost will have better predictions, so you can use that. But again, it's usually a lot of manual analysis – exploratory data analysis, understanding which features make sense, and then maybe adding more features, experimenting with different features. _x000D_
Here, it's very important to have a good validation framework using which you can test your ideas. You think, “Okay, this feature will be useful.” You add this feature, you implement this feature, then you train a new model, and then you see how well it improves your score on validation. This is the best indicator that you're actually improving something. _x000D_
A short, one-line answer to this would be “The best technique is cross-validation or just validation."
Machine Learning Zoomcamp;2022;How are the take-home assignments that we get while applying for a data scientist position? Are they like what we have done in the courseâ€™s project?;Yeah, they're similar. Not all of them will ask you to deploy stuff, but for data science, it's often necessary to do simple exploratory data analysis then train the model and write some conclusions.
Machine Learning Zoomcamp;2022;What is a good sample or size of the dataset?;It depends. In some problems, you can already build a good model with 100 examples. In other cases, 10,000 might not be enough.
Machine Learning Zoomcamp;2022;What are your thoughts on Naive Bayes?;Naive Bayes is a really good model, but I find it very similar to logistic regression. In the end, when you compare the performance of Naive Bayes with logistic regression, it's very similar. It's a very nice model, but since we're covering logistic regression here anyway, I didn't see a lot of sense in including Naive Bayes as well. It has a very nice and elegant theory. You can check it out. It's really beautiful. But in the end, for all practical purposes, it's very similar to logistic regression.
Machine Learning Zoomcamp;2022;How to explain in simple words to stakeholders, what machine learning and neural networks are?;Well, that's why we have module one. There, the very first unit (the very first video) the example with cars – just use this example. You can use that to explain what machine learning is. When it comes to neural networks, we have module eight, which is about neural networks. You can use that to explain what a neural network is. But if you want a one sentence explanation – it's a bunch of algorithms that extract patterns from data. I don't know – will it make sense to your stakeholders? If it will, good. If it will not, then you have these other resources that I mentioned.
Machine Learning Zoomcamp;2022;How can we handle wrong values in a dataset? For example, extra large numbers in numerical columns or a set of letters in categorical ones?;You do exploratory data analysis – you look at the data, you see if anything is wrong there, and then you write some code for preparing this dataset. There is no silver bullet for that. You just need to look at the data to see if there are things that are wrong, and then see how you can programmatically fix these things with your code. For example, for extra large numbers, there are multiple things you can do. You can just throw away the observation completely, or you can do capping. Capping is when you say, “If this value is larger than X, you keep it at X.” There are many ways you can do it, but I think these two are the most typical ones.
Data Engineering Zoomcamp;2023;Why do we use Kafka and not pop/sub?;I will need to ask Ankush. Maybe ask that in Slack and then he will answer.
Data Engineering Zoomcamp;2023;Are we going to come up with our own individual projects?;Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
Data Engineering Zoomcamp;2023;How should I organize my data lake â€“ by departments, methodology? What are the best practices or any article to guide me?;"Alexey_x000D_
Ankush will probably be a better person to answer that. Maybe ask that in Slack and see what the answer is. From what I know (the way we organize it where my work is) each department or team has its own S3 bucket. Inside this S3 bucket, we have different folders, or prefixes, for different data sources – for different projects, let's say. And then inside each project, there could be different specific data sources. And in these data sources, data is usually partitioned by day, but there could be other partitions, too. So there are partitions inside these folders and then, eventually, there are parquet files. I hope that answers your question. There are probably articles about this. What I described is something that our data engineers did. I did not take part in that. I just use it. Maybe if anyone knows any good articles about that, please share them in Slack."
Machine Learning Zoomcamp;2022;What's the best way to start a project in a real-world scenario?;"Maybe you remember in module one, we talked about CRISP DM. This is a very good way to follow a project. There, you need to start with a goal, so “What kind of goal do you have in mind?” Then you need to think about how you will measure the success of this project. And then, based on that, you go through the rest of the steps. You're probably looking for a more detailed answer here. We can discuss again, but it all depends on your specific project. What do you already have in place? What do you want to do? _x000D_
It's all different, but the structure of how you approach the project and how you approach problems is very similar to CRISP DM. Usually, you start with the goal and then you find data that you need for this, then you build the pipelines, you take care of the data engineering part, and then you train your models. Then you evaluate this model, you deploy it. Maybe I missed a step here. But this is how you do this."
Machine Learning Zoomcamp;2022;Any hint on when to use techniques to deal with class imbalance (undersampling, oversampling, and similar like SMOTE)?;"We had a talk about this on our YouTube channel. As far as I remember, it was called Machine Learning Design Patterns. These patterns were mostly about engineering – engineering patterns – but the first pattern, as far as I remember, was about rebalancing. So check it out. Sara explains when to use these techniques. Here, the important part is that you apply these techniques only to your training dataset. You keep your validation dataset intact – you don't change it – because you want to have a reliable way of evaluating the performance of your model. You apply different techniques to your training dataset and then you see how exactly it changes the performance on your validation dataset. _x000D_
So you apply these techniques to train, experiment, try different ones, and then go with the one that has the best uplift for your score on validation. That's usually how you do this. This will tell you if you actually need any of these techniques, or whether just throwing all the data that you have into the model is good enough, so you don't need to make the process more complicated than it should be."
Data Engineering Zoomcamp;2023;Data lakes are like folders. We save data before/after transformation. Is this understanding correct? Then why save transformed data in a data lake rather than just a data warehouse?;"Alexey_x000D_
Yeah. They are kind of like folders and there are files in these folders. They can be stored in S3 or HDFS or Google Cloud Storage. Indeed, we save the data before transformation and if we want, we can also save the data after transformation in a data lake. As to why we save transformed data in a data lake rather than just a data warehouse – sometimes it's just cheaper. In a data warehouse like BigQuery or Snowflake, you need to pay a lot for storing the data there and for accessing the data. If you just save the data in a data lake, your indexing – the way how you can speed up the queries – is limited, so you will probably need to read all the files in a folder, but usually it's cheaper, especially if you are doing something like a full scan. When you need to access all the data for a day or inside a partition, then data lakes are usually much cheaper. Usually, that's why we save transformed data in the data lake – mostly because of that."
Machine Learning Zoomcamp;2022;Maybe you have some links to run a GPU with models in local?;"[chuckles] No, I don't. Just go to Google and you'll find a lot of them. I don't suggest spending time on that. If you want – not because I asked you to do it – because it's likely that you will spend a lot of time figuring out how it works and why the stuff in this article doesn't work on your machine. I've been there. I don't recommend it. _x000D_
I would suggest you try to go with Saturn cloud or something similar and avoid spending a week on figuring out how to set up cuDNN and all these things on your machine, especially if it's a Windows machine. It’s terrible. I did set up something like this on Windows as well and it wasn't easy. I don't recommend doing this. It’s better to spend this time on something else. [chuckles] But if you want – if you insist on doing this – the internet is your friend. _x000D_
Just go to Google and you will find a lot of resources. The catch there is that not all of them actually work and you'll have to figure out which tutorial works and which doesn't. I think on Ubuntu it’s actually simpler, at least according to some of the students who configured this. Maybe just go to Slack and ask, “Hey, I need help with configuring my local GPU.” Because I know at least two students did this. So if you're brave – do this."
Machine Learning Zoomcamp;2022;Does AWS ML services the best over Azure? I think the question is â€œAre the services in AWS better than in Azure?â€;I do not know. I have not used services from Azure to say that. I don't think they are better or worse. They're probably similar in terms of what they can do. They're just different in terms of how you use them and the interface. Just pick whatever you like more. I don't think there is any significant difference.
Data Engineering Zoomcamp;2023;When I deploy from a Python code, why don't I see the .yaml file being created in the current repo?;I don't have enough context to answer this question. I don't know. Ask in Slack, please.
Data Engineering Zoomcamp;2023;Must we really use Java in part 6? And will there be a part in our project that will require us to write Java?;I will defer this question to Ankush. He's unfortunately not here right now, so maybe ask him in Slack. From what I understood, it better describes the internals of Kafka. You don't need to know any Java. He explains everything you need. No, you will not need to use Java for your project. You can, but you don't have to.
Data Engineering Zoomcamp;2023;Will we learn how to productionize the setup of Docker to a server instead of locally?;I'm not sure what exactly you mean here. Probably, you’re talking about container orchestration platforms like Kubernetes. We did not cover it in this course. We do cover Kubernetes in our Machine Learning Zoomcamp. If you want to learn it, check it out.
Machine Learning Zoomcamp;2022;Iâ€™m working as a Java developer, but I'm much more interested in machine learning. Please suggest the best machine learning course online for me.;You are in the right place. I was a Java developer. I became interested in machine learning and this course is quintessence… basically all my experience and everything I needed to go through, to learn through, and I think that this is how I best learned, as a Java developer, as an engineer – by doing projects. This course was actually targeted at software engineers who wanted to become machine learning engineers. So you are in the best place.
Machine Learning Zoomcamp;2022;Do you recommend collecting our own data for the project?;That's a very good question. I do recommend it for your pet project. But for the midterm project, you will not have enough time to actually do this. For the midterm project, it’s better if you already have a dataset from somewhere. You will simply not have time (two weeks) to collect your dataset and then do the rest of things. You can start collecting your dataset right now. If this process will finish by the time the project starts. Otherwise, I would not recommend doing this. But if you want to do it for your portfolio project, for future employment, for a future job search, then it's a great idea and please do it. But be mindful of time, because you might not have enough time to finish the project.
Data Engineering Zoomcamp;2023;Is ELT only for large companies or can it be applied to small companies?;"Alexey_x000D_
Yeah, it can be applied to every company – to any company."
Data Engineering Zoomcamp;2023;Store raw data in GCS-> Pull & Transform Data-> Store in GCS-> Pull into BigQuery. Is this okay or the data after transformation should be only stored directly in BigQuery?;"Alexey_x000D_
To me, it looks fine. I mean, it really depends on your use case, but for me, this is perfectly fine. There could be reasons for skipping data in GCS and putting it directly to BigQuery. I mean first putting it into BigQuery and then transforming it there. So I think this is how you typically use DBT. I don't think there is any right or wrong way."
Machine Learning Zoomcamp;2022;I see another question in the live chat about Kedro. Have you used Kedro, Tim?;"Tim_x000D_
No, I haven't. _x000D_
Alexey_x000D_
Do you know what it is?_x000D_
Tim  _x000D_
No, actually._x000D_
Alexey_x000D_
This is a tool for building machine learning pipelines. We had a workshop about Kedro here on our channel. You can check it out here. The idea behind Kedro – this is for training your models. This is not for serving. This is for splitting… for the projects for the course that you will need to do, you need to come up with a train.py file, but Kedro is a way to modularize (to break into separate chunks) this train.py file such that these chunks are more modular, I guess. You can have a separate function for preparing your dataset, for feature engineering, for doing all these things, and Kedro gives you an opinionated way of actually doing this. They give you a template, you follow this template, and then they run this. It's a cool thing._x000D_
Tim_x000D_
Like a declarative ML pipeline. Got it._x000D_
Alexey _x000D_
Yeah, exactly. Eventually, at the end, it still runs on one machine. But it just gives you an easy way to split it into logical units, let's say, and also run it. It takes care of the pipeline orchestrator – it takes care of running things in the order unit._x000D_
Tim _x000D_
Right. I'd imagine there would be one final step once you've trained your model, if you like it – that would be a declarative BentoML deploy step._x000D_
Alexey_x000D_
Exactly. For example, if you use MLflow, you save your model in the registry, and then another step would be to export it to Bento. Or immediately to Bento, skipping MLflow. Then, probably, there could be a conditional – if performance in validation is above certain thresholds, then do that. Otherwise, don't do anything. If you're interested in Kedro, check out this workshop with Merel."
Machine Learning Zoomcamp;2022;How should I choose the number of convolutional layers or dense layers?;Yeah. [chuckles] It depends. Use validation to find out what the best number of convolutional layers or dense layers is. There is no “correct” answer to these questions. It’s all case-dependent. For this one, it probably makes sense to use existing architectures – to take things like Inception or ResNet, or Efficient Net – all these models that somebody else trained and just use that. If you really want to experiment with this, use validation to find out what works best.
Data Engineering Zoomcamp;2023;I like to contribute and help everyone in the Slack channel. Can I achieve being a member of the staff?;I don't know what you mean by that. If you want to help everyone in Slack, please do this. Nobody will say no. We're a community. We help each other. Please just go ahead and do this.
Machine Learning Zoomcamp;2022;How do the decision trees decide the split feature?;There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
Machine Learning Zoomcamp;2022;The tasks that we have done in this homework â€“ can this be considered as a real project? I guess a real project might be more complete, end-to-end.;"The homework is not meant to give you end-to-end project experience – it's meant to make sure you understand, or reiterate, all the topics we cover in the videos and in the course materials. We try to focus on the most important parts there and also maybe push you a little bit out of what we covered. So you need to do a bit of exploration yourself. Maybe there haven’t been as of yet, but there will be homework assignments where you will actually need to move a little bit outside of what was presented in the course, and then do a bit of Googling yourself. _x000D_
Homework assignments are not “real projects” in the sense that you probably shouldn't add this to your portfolio and call it “Hey, I predicted a car price dataset. I used this dataset and predicted a car price. Here's my portfolio project.” I wouldn't do this. But for the projects we will have as part of this course – yes, you can definitely do that and they will be more complete and end-to-end, definitely. Especially compared to what we do in the homework."
Data Engineering Zoomcamp;2023;Can we get a brief description of how Prefect works architecture-wise? It will be helpful to solve some path errors that arise.;"Jeff_x000D_
See if this maybe helps a little bit. We have a diagram here talking about your execution environment. This is your infrastructure and your agent running (in this assumption) locally and using Prefect Cloud or the Prefect Orion server. There is an API here on the server. That's what you're interfacing with. And there's a UI that displays information from that API server. The agent opens a connection with the API server and scheduled flow runs get returned and all the information that is needed there and that schedule flow run. Then the agent kicks off the flow run with the flow code in the infrastructure that's specified. Then the results of how that goes gets sent to the API server and that information gets displayed in the cloud API server. I'm just guessing here – maybe a little bit of this question is coming from, (just one thing that came up a time or two) like, “The stuff’s on GitHub. Why do I also need to have a path to the local file to show where my flow code is when I'm making the deployment?” The reason for that is – just to make sure that all of the needed files are available in the place where the deployment is built. This is kind of how things are assumed and needed to work at this point with Prefect."
Data Engineering Zoomcamp;2023;Is it possible to apply to jobs after the course?;"Alexey_x000D_
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. _x000D_
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work._x000D_
Luis_x000D_
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important._x000D_
Ankush_x000D_
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]_x000D_
_x000D_
Victoria_x000D_
Michael moneymaker. [chuckles]"
Data Engineering Zoomcamp;2023;After creating deployment and having an agent pick it up, what caused this error? ``` This run is scheduled and hasn't generated logs``` any way to solve this?;"Jeff_x000D_
I’m not sure, exactly. It might be the case that you made the deployment and it's scheduled and it's just waiting, but you are not connected to that workspace with the space where your agent is running. Agents are looking for work maybe locally, and you're connected to Prefect cloud, for example. That's one possibility. Just make sure that your agent is pulling for work from the workspace where you actually have scheduled the work to run. You can check that when you start your agent – it'll tell you what it's connected to, up at the top of the message that comes right back when you start the agent, in the CLI."
Machine Learning Zoomcamp;2022;How does CRISP-DM relate with MLOps?;"MLOps is a set of processes and tools for productionizing machine learning, while CRISP-DM is a process. CRISP-DM was invented something like 12 years ago, when MLOps didn't really exist. But surprisingly, it covers it pretty well. _x000D_
https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png_x000D_
It also depends on how exactly you interpret each of the steps here. Here is the evaluation and deployment phase, which are pretty related to MLOps. Evaluation is how exactly you evaluate your model – how you're doing an A/B test or something like this. And deployment is how you productionize your model. Many of these things are quite related to MLOps."
Data Engineering Zoomcamp;2023;Sometimes companies ask for a skill that I don't have, even though I'm good with most common data engineer technologies.;They may ask for a skill, but it doesn't mean that they really have to have it. You don't have to match the job description 100%. Companies usually realize that, let's say, if you know Prefect but don't know Airflow – if you know GCP but don't know AWS – it's still fine. These skills are transferable. Usually, employers will consider you anyway if you have experience with related technologies.
Data Engineering Zoomcamp;2023;What is the best tip you have for somebody who's just starting their data engineering career?;I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
Machine Learning Zoomcamp;2022;How do we decide if the best threshold for the classification, using the ROC curve or accuracy? As in the lesson we use 0.5 based on accuracy.;"With the ROC curve, it's not easy to find the best thresholds. I usually use accuracy or something else. First, you need to ask yourself what kind of metric you want to optimize. Usually, for example, if we want to optimize for precision and recall, we combine them in F1 score. And then, for example, I would evaluate my classifier against different thresholds and pick the one with the highest F1 score. _x000D_
It usually comes from your needs. If you want to have good precision, but not so bad recall, then you probably plot this precision versus recall on different thresholds, like you had to do for the homework. Based on that, you decide what the best threshold for your specific case is. This is how I usually do this."
Data Engineering Zoomcamp;2023;How to prepare for data engineering interviews?;I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
Data Engineering Zoomcamp;2023;During what time does the GCP billing update?;"Alexey_x000D_
I don't know. Maybe I'll put this question to Ankush as well. But check the website. I think this is one of the good examples when you can just use Google to find the answer."
Machine Learning Zoomcamp;2022;How can I save and load a BentoML model with features?;"I think you're referring to the problem we had, which is in the frequently asked questions. The problem that we had there was this one: “ValueError: training data did not have the following fields: age, amount”. I don't know how to solve this problem. I talked to Tim. Tim will talk to the BentoML team and they will figure this out. _x000D_
Maybe this is the backend – there will be an issue on GitHub. You can follow this issue and see when it will be solved. Or maybe there is an easy fix, such that you just need to specify the feature name somewhere. I don't know yet, but I asked Tim to help me find the answer. Once we have the answer, we'll share the answer with you."
Machine Learning Zoomcamp;2022;What is a good resource for practicing Python?;"The best resource for practicing Python is your terminal. Go to your terminal, open iPython and start practicing it there – or maybe not iPython but Jupyter Notebook. I guess this is not the answer you were looking for. Maybe try to think about what kind of simple things you can automate and automate them. There must be some things that you do manually every week (for example). Try to write a Python script to automate them. Maybe something similar might be… I don't know, I am running out of ideas right now. _x000D_
But if you just sit down for about 20 minutes and think “What kind of things can I automate? What kind of things can I do?” And then try to use Python for that. That's the best resource. Of course, maybe you want me to suggest a website where you can practice Python. Maybe LeetCode could be good. They have different problems. I don't even know how it works these days. You have some problem description and you try to solve it with Python. That could be useful, let's say, if you want to work at a company like Amazon because they will torture you with these kinds of problems during the interview. At the same time, you can also practice Python. _x000D_
Another resource could be Code Academy. I think they have some Python courses, so maybe you can check that out. I see that it says “pro” so you will probably need to pay for this one. They might have free ones. Anyway, I think the best way you can practice Python is by doing little projects – writing little scripts."
Data Engineering Zoomcamp;2023;I do not really understand why we need Spark. What can I do with Spark that I canâ€™t do with Prefect?;"Alexey_x000D_
Prefect is an orchestrator. If you need to produce a lot of data, Prefect might not be the best tool. Prefect really shines when it needs to execute tasks in a specific order, one after another. Usually the approach many people use at work, from what I see, is they use an orchestrator for orchestrating, and then you delegate all the compute to something external, like Kubernetes, AWS Batch, or Spark. Spark is an external thing. You submit a Spark job from your Prefect graph – from your Prefect DAG. Of course, you can do many things with Prefect, but when it comes to large datasets, then Prefect executors might simply not be able to handle that. This is when you need to use Spark. One does not exclude the other. You use Prefect for scaling Spark jobs, because maybe after this Spark job, you have another job that is not Spark – and Prefect knows how to execute them and in which order."
Machine Learning Zoomcamp;2022;Is getting into data science or data engineering harder than the other or about the same?;It really depends on your background. If you're already a software engineer, then maybe data engineering will be easier because you don't need to spend a lot of time studying machine learning. But it also depends on what you like more. It depends, as I said – Let's say if you have a PhD in physics, maybe data engineering will be more difficult for you than data science. But if you like data engineering more than data science, just go for it and do data engineering. If you like it more then it will not matter for you if it's harder or not.
Data Engineering Zoomcamp;2023;Why do we have to add PySpark to the Python path every time on a VM? Why can't we add this to the path permanently like how we added Java and Spark?;"Alexey_x000D_
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
Machine Learning Zoomcamp;2022;When will the capstone project start? Today (November 28, 2022)?;No. It was supposed to be today, but I changed it a little bit. If you go to the updated deadline calendar, you will see that the capstone project starts next week (December 5, 2022). For the evaluation, we give you a lot of time. During this time, depending on when you take your vacation, you will have some time to evaluate the project and then the capstone project starts next week. I think what happened is that when we added Bento ML here, it kind of messed up the whole schedule. The idea was to finish everything by Christmas but, unfortunately, we won't be able to finish everything by Christmas which probably means that if you do the midterm projects 1 and 2, you can get your certificate somewhere here or maybe at the end January. I don't know. Let's see. It will probably be easier to just give certificates at the end of January. Let's say if you finish everything by New Year’s, then you're free to stop taking the course. I know it was very difficult and time consuming. You might as well just finish everything and start the data engineering course or enjoy your life, or do many other things. We'll also have an article, which starts pretty soon. During the next Office Hours, I should probably talk more about that.
Machine Learning Zoomcamp;2022;Sounds like you're a recovering data scientist.;Maybe. I don't know what that means. But yeah. Maybe?
Data Engineering Zoomcamp;2023;How do I create a link where I can put my code to submit my homework?;Put your code to GitHub. Let's say I have my code with my homework. Let's say your homework is in this directory. Copy this one and put it in the form. That's how you do this.
Machine Learning Zoomcamp;2022;Besides the 30 hours of GPU offered in Saturn cloud, is Colab (free plan) a possible solution to execute the deep learning tasks in the session with GPUs?;"Colab is an option, yes. You can use that. Colab is a little bit unpredictable when it comes to… working. [chuckles] Sometimes it just stops working and then you have to restart and start training the model from scratch. Overall, it's good. It works. It's just that they might turn off your instance after one hour. It's not like there’s a rule that after one hour, your instance will die. It usually happens sporadically. If you want to have some guarantees with Colab, you probably have to pay. _x000D_
If you just want to play around with Colab with the free plan, there is another downside. I think if you want to save something in Colab, and your session dies, I'm not sure if you can recover the files. You need to save them in your Google drive somewhere, or I don't know how it works. So there is a bit of overhead when it comes to the free version of Colab. If you're happy to do this, by all means do it. I actually don't know how much the paid version costs. I think 30 hours will be enough – and remember that it's per month. Next month, you will get more hours."
Machine Learning Zoomcamp;2022;Best practices on how to handle heavy Pickl models (maybe 2-3 GB)?;"I would ask myself, “Why is my model that large?” Is it because it has like 5000 trees in my XGBoost model? Or 10,000? And if the answer is yes, I will try to cut the number of trees. Usually, in situations when I find myself having very large trees, it is when my learning rate is very low, so each three is only slightly improving the previous three. Here, what you can do is just increase the learning rate by three times (for example) and then reduce the number of trees by three times. Then you will have roughly the same performance or maybe a bit worse, but then your model will be one gigabyte instead of three. _x000D_
Another thing you can do is reduce the number of the depth of the model. And you can play with this more – so maybe increasing the learning rate and making the tree shallower. That could also help. That's typically how I would do it. Otherwise, if this is a linear model – the only reason your linear regression or logistic regression model could be that huge is because you have a lot of categorical variables. You have a weight for each of them, so maybe you can reduce the number of variables you have by using the techniques we talked about half an hour ago. Maybe you can talk about your specific case in Slack and we can figure out what the best way for you is."
Data Engineering Zoomcamp;2023;Do you need to know anything about Big Data before starting this course?;No.
Machine Learning Zoomcamp;2022;How can linear regression predict anything at all? For example, what if there are two features which are strongly correlated (district and price for m2)?;"You can hypothetically think about all the situations, but what I would do in your case is just set up a validation strategy and then test all your assumptions. Then, based on the performance of the model and the validation set, you can see if it's performing or not. It doesn't matter if there are correlated features, non-correlated features – you see the performance metric, (RMSE, in this case) and if this metric is good, you just go ahead and use this model. I hope that answered this question. _x000D_
If you want to have some sort of theoretical justification for how it works, then perhaps you should check some theoretical textbooks, like Elements of Statistical Learning or some other courses that focus on theory. Here, I want to show you how to use these things from a practical point of view and that's why I put so much emphasis on setting up this validation strategy. Because I think this is one of the most important things in machine learning. If you can reliably test your hypothesis, then you can do pretty much everything you want – anything you can imagine – you just go and try it on your validation dataset. And if it works, it works – you can just use it."
Machine Learning Zoomcamp;2022;How do you recommend dealing with imbalance in classification data, especially when the data is small?;"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. _x000D_
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
Data Engineering Zoomcamp;2023;I started self-paced learning with ML Zoomcamp. I'm thinking I should combine this Data Engineering Zoomcamp and do it in parallel. Any advice?;If you have time, then why not? But as I said, this course requires some commitment from many people lately. Doing two courses in parallel might be ambitious, but if you have time, then why not?
Data Engineering Zoomcamp;2023;How can we submit homework assignments?;"Alexey_x000D_
You will see links soon. I think we already have the first one, but we just need to put it on GitHub. We will communicate over everything in Telegram and then the automator will post the link from Telegram to Slack. So you will see if you're in Slack."
Data Engineering Zoomcamp;2023;Is it possible to make workflow orchestration tools serverless so that I do not need to host it, I just need to pay when flows run?;"Alexey_x000D_
I know that in AWS, they have step functions, which is a serverless workflow orchestrator. It’s fine if it works for you._x000D_
Jeff_x000D_
Yeah. Prefect has some serverless options too. Depending on your cloud provider, we support almost all of them. Here's a link to a post that Anna wrote. There are other patterns, too. We are working on something where – right now, you have to put your agent somewhere, so you could put your agent even on ECS, potentially. But we're looking at a managed solution where if someone just doesn't want to hassle with it, we can just deploy things for them using Prefect Cloud. But that's a few months off, probably. _x000D_
Alexey_x000D_
Here, what does this project do? It says streaming. That's something? _x000D_
Jeff_x000D_
Yeah, this one does a number of different things. If I recall, it's using Cloud Functions and it's using GitHub Actions, to do a CI/CD kind of deployment. There are a couple things going on there. But Anna has a whole repository and a walkthrough in that blog post. That's just one example of doing it._x000D_
Kalise_x000D_
Essentially, what it's gonna do is – the same way in the videos, how you can run a Prefect flow just by calling a Python function, you can run that wherever. And if you're connected to cloud, you're going to be able to visualize that. You're not going to have to host the UI, the Orion server. So if you had some sort of event happening and you just wanted to execute your Python function as a flow to get visibility, you can just do it right there. _x000D_
Alexey  _x000D_
I think in GCP, there is also something similar to step functions."
Data Engineering Zoomcamp;2023;Are there any specifications for the size of the project dataset? The one I'm interested in has only 2000 records.;"Alexey_x000D_
We don't explicitly have any limits on the size of the dataset, but 2000 is very low. Maybe you can find something bigger? Or maybe you can find a way to make it bigger – to make it more interesting? Typically, the engineers deal with more data than 2000 records. But I don't think we have any strict limitations on the size. I think we already had this discussion last year. I don't remember if we explicitly put any limits. Just use your own judgment. Imagine that a hiring manager is looking at your project. You probably want to make a good impression, right? And I don't think 2000 records will make a good impression, or a 100-kilobyte dataset will make a good impression. Megabytes? Eh. But if it's in gigabytes, it's already good."
Machine Learning Zoomcamp;2022;Do you often use interaction/polynomial terms when applying linear regression?;Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
Data Engineering Zoomcamp;2023;How many hours per week should I anticipate dedicating to the course?;From 5 to 20, depending on your experience. If you're already an experienced developer, software engineer, you know Python, you know cloud, it could be less than five. If you don't know any of these things and you never used a command line, then it's 20 hours. If some of these things apply to you, some don't, then probably around 10 hours.
Machine Learning Zoomcamp;2022;When I started, I followed chapter 1.6 and used Conda all throughout. Later, we used Pipenv and it had some problems due to multiple env managers. What is your usual environment manager?;"I use Anaconda for prototypes. I first create a prototype, just starting my Jupyter Notebook with the base environment from Anaconda. When the prototype is ready, the notebook is ready – I create the virtual environment. Of course, in theory, I should have first started creating the virtual environment, but I don't always know all the packages that I will need in advance – installing them one by one, I’m too lazy for that. So what I do is just use the base environment with all the packages I potentially need. But then, when this kind of proof of concept prototype is finished and I know exactly what I want to do – I want to turn this notebook into a script and this is when I create a Pipenv environment. _x000D_
So I start with Anaconda, I do everything for the prototype in Anaconda, and once the prototype is ready, I create a Pipenv environment ( a Pipenv file) and then put things there. Basically, the setup we had in this course is what I follow in practice as well. This is my usual environment manager – it's Anaconda for everything and for specific projects, I create Pipenv files. Again, in theory, maybe the recommended approach would be – when you start a project, you start with a clean virtual environment and then you first install NumPy, pandas, and whatnot (what you need) and then you add package by package when you create a prototype. I find it time consuming and I'm a lazy person, so that's why I cut corners a little bit."
Data Engineering Zoomcamp;2023;Can we add you on LinkedIn?;Right now, I'm reaching the limit of connections. I used to accept everyone who sends me a request, but now I don't. Maybe add a note of why you want to connect with me. But you can just follow me, so you don't really need to connect with me on LinkedIn. If you want to ask me a question, the best way of doing that would be to go to Slack and ask a question on the public channel. When I see it, I will try to answer it.
Machine Learning Zoomcamp;2022;What happens if we give Vectorizer unsubmitted information?;By “unsubmitted” you mean “unseen” information? One thing you can do is ask the Vectorizer to raise an error. It will see, “Okay, I have not seen this value before. Let's throw an error out.” And this is what it will do. A better thing would be to use Pydantic and BentoML. In BentoML, you can include this. One of the modules this week talks about that. It's called validation. Here, Tim shows how to add a Pydantic class. Pydantic is a special framework in Python that can validate your data. In this one here, we show how to add this validation to the input data. Actually, in the example, he just removes one of the fields from the request and the model still works – it's not throwing any exceptions. But this is not always what you want. In that module, we discuss how to deal with this.
Data Engineering Zoomcamp;2023;I can't find the ny_rides.json file to complete the environment setup.;When you create a project and then you create this role – the JSON file – in the Terraform BIOS, you can see it. And this is the JSON file with the credentials. If you go here, to the Terraform set up and take a look at this one. This is application credentials. You basically follow these instructions. This is the service account. You download the JSON file, and then this is the file.
Data Engineering Zoomcamp;2023;Will we see any fundamental topics, like database design, schemas, etc.?;"Alexey_x000D_
I think Victoria covers a little bit of this, right? _x000D_
Victoria_x000D_
Briefly, yes. Ankush, I believe there was an intro in week three. You start a little bit with data warehouse, and then what we do with DBT is use that schema and we talk a little bit about data modeling design. We talk specifically about the star schema and Kimball methodology._x000D_
Alexey_x000D_
We wanted to make this course more practical. This is a more theoretical topic and I think there are good resources where you can pick this up. We did not aim to replace these resources. We wanted to give you something that is very hands on._x000D_
Victoria_x000D_
I think it's still in the document with the questions. I wonder if we could do that again. We had prepared a list last year with some books and stuff like that. We could definitely share that as well to make sure you have it. _x000D_
Alexey_x000D_
I think the list was Awesome Data Engineering, I will look it up. _x000D_
Victoria_x000D_
Ah, yes! We have Awesome Data Engineering. It's bookmarked in the channel. There’s an amazing list of resources. We have a lot. _x000D_
Alexey_x000D_
The idea there was to put it to GitHub and do something like Awesome Data Engineering GitHub repo. There are maybe 10 already. Maybe it should be a more unique name. I actually don't see Kimball and other stuff. _x000D_
Victoria_x000D_
No, but I can add that part. _x000D_
Alexey_x000D_
Please do. We can even have a separate section which we can call Database Design or something and put it there."
Machine Learning Zoomcamp;2022;What's your say on the future of Google Vertex AI and Dataform?;Okay, what is Dataform? Manage data pipelines in BigQuery. The page looks good. [chuckles] But apart from that, I cannot tell you more. Future of Google Vertex AI? Well, Google Vertex AI seems quite cool. It simplifies many things. It has a future, but also, I haven't used it a lot. But I can compare it with SageMaker. It's a similar thing on AWS. With SageMaker, you can start using machine learning quite quickly. But then at some point, it becomes restrictive. There is only a certain way of doing things. I use SageMaker a lot and then, over time, I realized that I like the approach with Flask and Kubernetes a lot more, for example. But yeah, they look quite good and I think it's worth learning them if you want to do things fast. Why not?
Data Engineering Zoomcamp;2023;For the second project submission, will there be a peer review as well?;Yes, there will be peer review. For both projects attempts, there will be a peer review phase.
Data Engineering Zoomcamp;2023;Do you plan on creating a Zoomcamp targeting data analysts?;We do not. But if you want, or know somebody who would be interested, let's talk.
Data Engineering Zoomcamp;2023;Is it possible to schedule two tasks at different time intervals in the same DAG (e.g. one at every one hour, and another one every four hours in the same DAG)?;"Jeff_x000D_
Again, Prefect is DAGless, so you don't have to have a DAG. It gives you flexibility that if you want your things to run in a directed acyclic graph, you can structure things like that. But Python is much more flexible and workflows can be more flexible with Prefect. Maybe it's an Airflow kind of question – I'm not sure. You can schedule things to do what you want. With the recurring rules schedule, our schedule, you’ll have a good bit of flexibility there. It seems extremely, extremely flexible. I encourage you to check out current rules and see if there's something that works there for what you need. Worst case, you could have two different deployments and have them running on different schedules. That’s pretty lightweight, still."
Data Engineering Zoomcamp;2023;Can you please explain the Prefect Radar in UI (the concentric circles with green lines) and how to interpret them?;"Alexey_x000D_
I'll try to Google it so we have an example. If I just put it into Google, then we should have illustrations. I personally find it not the easiest thing to interpret. [image 1]_x000D_
Kalise _x000D_
We'll just kind of go over the high level. The intention of the Radar chart – as you can see, there's different rings of circles and it is because Prefect does not require a DAG anymore (Prefect 2). What that means is you can think of each of the dots as the nodes, or your tasks, or your sub-flows. Think of them as nodes and edges and how they're connected. As it expands out, it's going down, with the data dependencies, for example. A task could come back through the middle and call an inside ring that has already been called again. That is really why it’s circular. Now, like Jeff said, we also now have a timeline chart in Prefect Cloud as well, if you were to update your Prefect version. That was released as well. It's a timeline view._x000D_
Jeff_x000D_
It's pretty new. It's pretty cool. We have a picture in the release and I put a picture in Slack today. It’s even more updated with the nodes and edges. Now it has edges too. It's linking from one to the other in time, so that's even spiffier than that now. But it kind of gives you more of what you might think of as a fairly DAG-like view for cases where it makes sense to do that. Yeah, it's a little tricky. We've had some good feedback today on this. It's not intuitive, necessarily, to understand the Radar View chart. It's not something that we're used to seeing as a DAG, because things can be more than just DAGs. We wanted more than one way to show that everywhere. We're working on having kind of multiple different views for folks – some are basically easily understandable, and some that are for more advanced use cases."
Machine Learning Zoomcamp;2022;When training images, is it better to use them in color or in black and white?;You will probably not like this answer, but use validation to find out. Train two models – one model uses color, another uses black and white – you compare them and you will see which one performs better. You can also look at speed, perhaps, because this could be another important factor. I assume that the black and white one will train faster. But that's it. Just experiment and see you for your dataset, which one works better, and stick to it. In general, for any question about machine learning, Like, “Is it better to use X rather than Y?” The answer is always validation. Just try both and see whatever option performs better on the validation.
Data Engineering Zoomcamp;2023;What aspects of real world data engineering are covered in this course, along with projects?;"Alexey_x000D_
Pretty much every week is real world data engineering. I don't think we should spend too much time answering this."
Machine Learning Zoomcamp;2022;Shouldn't R-Squared or MAPE free us from the scale issue suffered by RMSE in regression?;"I wouldn't call it an issue, actually. It's more like a feature than a bug of RMSE. Let's say you have a model that predicts price and you want to know, on average, how wrong your model is in dollars. That's the purpose of RMSE. Or if you're predicting the age of somebody, then you want to know, on average, how wrong your model is in years. It gives us this understanding of how wrong the model is, on average, on the same scale as our target variable, while R-Squared and MAPE do not give us that. They serve different purposes. _x000D_
I never actually used R-Squared in practice. For me, it's always misleading. I cannot really interpret it quite well. But MAPE is a good one – it has its own problems and I think we talked about that in the last Office Hours. It's good if you look at multiple metrics at the same time – you look at RMSE, you look at MAPE, you perhaps look at mean absolute error (MAE) and then, based on that, you make some decisions whether to use this model or not."
Data Engineering Zoomcamp;2023;When are we talking about the solutions for the homework?;This question was probably asked before the solution was published. Just in case, I will show you where the solutions are. Go to cohorts, 2023 cohort, then the week 1 homework and we have the solution here. We already recorded the solution, so just go through this and you'll see the answers. It’s the same for the other homework assignments and solutions. For the Terraform part, there is no solution. It's just simply applying everything you saw in the lectures. For workflow orchestration, we will also have the solution here after the deadline.
Data Engineering Zoomcamp;2023;If we run into a problem, is this the correct sequence to follow? 1) Google doc frequently asked questions. 2) Ask in Slack. 3) Asked during Office Hours.;Yes. And the Office Hours step is probably more for the non-technical questions. If you have a large code snippet, you cannot really ask this during Office Hours.
Machine Learning Zoomcamp;2022;Can TensorFlow Lite be used in a lambda function without a Docker image? Only function-serverless?;It can be. Docker makes it a lot easier to do this. But it doesn't have to be Docker. With lambda, you will need to prepare a ZIP archive that has everything you need for deploying the lambda function. So you will need to put all your dependencies like NumPy, Pillow, and TensorFlow Lite inside a ZIP archive. You package everything in this ZIP archive, you put the ZIP archive to S3, and then you tell lambda, “Okay, this is where my files are. Get them.” You need to be careful when you prepare the ZIP archive. Let's say if you use Mac OS, and you're on the M1 chip then most likely, the ZIP archive you prepare on your Mac will not work on lambda. So you will probably need to do this inside Docker. Then inside Docker, you prepare a ZIP archive, and then you use the ZIP archive to upload this to AWS. If what I'm saying makes no sense to you, and it probably does not if it's the first time you use lambda, you can just, again, use Google, “AWS lambda python prepare a ZIP file with dependencies”. Yeah, I'm sure if you try this, this is what we need. When you do “pip install -r requirements” you put all the things in the requirements, and -t is the folder where it should install all the requirements. I would actually put it into a separate folder called “build,” for example. Then it will install all the dependencies to this build folder, and then you go to build and then you do ZIP to package everything. Then this is your ZIP file. That's what you need to do. I’m glad we talked about that. If it's not clear – if you're stuck – let's talk about this in Slack.
Machine Learning Zoomcamp;2022;Does an article substitute for a project?;An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
Machine Learning Zoomcamp;2022;Is ETA sometimes called epochs?;No, ETA is not called epochs. You will see epochs in the deep learning module. An epoch is typically one iteration over your entire dataset. I will not spend time now explaining what an epoch is. Maybe when it's time, you can check the deep learning video and if still not clear what an epoch is, then we can discuss it. Okay, I hope this explanation makes sense. If it doesn't – again, in the deep learning module, we will talk about a similar concept called “learning rate”. It's actually not similar. It's the same concept. There is actually an entire unit about this called learning rate. You can check that out. Maybe just check this unit separately from the rest and maybe it will help you understand ETA a little bit more.
Data Engineering Zoomcamp;2023;Can I make my public learning posts using other languages apart from English?;Yes, you can.
Machine Learning Zoomcamp;2022;How long is the extension for Homework 8?;Let's say two days. So Wednesday, November 23, 2022 at 11 PM Berlin time. That should be enough, hopefully. If it's not, let me know.
Machine Learning Zoomcamp;2022;What do you think should be an ideal tech stack for a data scientist/ML engineer?;"There is no silver bullet. Ideally, you need to have enough context to say “This is a good solution. This is not a good solution.” There is no one-size-fits-all tool. There is an ‘okay’ stack that works for many cases. For me, I would use Scikit Learn and XGBoost, for example. Some models from Scikit Learn, like linear or logistic regression, decision tree, random forest – and then I would use gradient boosted tree from XGBoost. Some people would go with LightGBM or CatBoost. Then I would deploy them with something like Flask and Kubernetes or with Lambda. That is usually sufficient for most cases. _x000D_
If we’re talking about web surfing, then even more often, I will deploy things with AWS Batch. AWS Batch is a thing where you can just take a Docker container and deploy it – then it will run periodically. Let's say if you want to run it every day, you can do this with AWS batch. The things we cover here, I think are quite applicable to the majority of problems. So you’ll probably cover most of the problems if you follow this course. I don't dare to call it ideal, but I think it's good enough to cover most of your needs."
Machine Learning Zoomcamp;2022;In the credit scoring dataset, if I'm not wrong, when we were splitting with the decision tree, when seniority was high, it tended more to â€œdefaultâ€ status. Is this an anomaly?;I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
Data Engineering Zoomcamp;2023;Are there any recommended readings for every week?;No, but you can suggest some stuff in Slack. If we see that this is useful, we can also include this in the project repo
Data Engineering Zoomcamp;2023;Will we cover Databricks at some point?;No, it's not in our plans
Machine Learning Zoomcamp;2022;I started ML Zoomcamp with Windows and it was difficult in deployment. What are the best system requirements for Section 7?;For Section 7, Windows should work. I tested it in, actually tested it in BSL, in Ubuntu. But on Windows, it should also work. If you want, I can test it, but I think that some students already did it and it worked on Windows. Remember that if you do this on Windows, (this is actually in our FAQ) 0.0.0.0. is not the same as local host. If you just run BentoML serve, and then the rest of the command, it will say that you need to open this URL (http://0.0.0.0:3000/) but this URL will not open – you need to replace the zeros with the local host. There is already a section in the FAQ about that. Apart from that, I don't expect to have any problems on Windows, so you can keep using Windows. As long as you have Docker installed, you should not have any problems.
Machine Learning Zoomcamp;2022;How do we test how good our validation framework is?;"A good validation framework will tell you…. that's a tricky one. You just need to try to make sure that it's representative of how the performance of your model will look in real life. So you just need to make it as real-life as possible. I'm afraid the answer here is that it's case-dependent. At the end, what is important is the number you get – the performance evaluation metric you get – from your validation framework. It should reflect how well your model will do on unseen data. Sometimes, just shuffling the data and taking a part of this is enough. _x000D_
Most of the time, you will also need to split your data by time. In most cases, you have a timestamp in your dataset, and in this case, it's better to use the timestamp  for splitting the data. Apart from that, maybe just use common sense. Try to develop a bit of domain knowledge about this dataset and ask yourself, “Okay, is this validation dataset representative enough of the problem we're trying to solve?” I don't actually know if there is a better answer to that. Maybe I'll think about this and tell you."
Machine Learning Zoomcamp;2022;When should we use TensorFlow Lite and when should we use TensorFlow Serving?;That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
Machine Learning Zoomcamp;2022;Can we use the datasets from homework to make a project portfolio with more work on them? Or are they too easy datasets?;I don't think it's a good idea to use the same datasets. I would go with something different. But if you really want to do this, I don't think I should say “no”. Please write in Slack about this – why you want to use this particular dataset and how it’s different from the homework we do. If it looks fine – yeah, why not? But don't restrict yourself to just these datasets. Try to find something, go to Kaggle, explore what’s there and maybe you will find something better.
Machine Learning Zoomcamp;2022;I chose a dataset problem above my skill level at this moment. The model is not giving good results. Will a bad model score affect my projectâ€™s evaluation?;"If you look at the criteria, you will see that model performance is not one of the criteria. If your model does not give you great performance, it will not affect the score that you will get at the end. This is intentional. The reason it's like that is because it's very hard to control this. The performance on the dataset of a model is very dataset-specific. For some datasets, this is how it is. In some cases, for example, for click prediction – if you want to predict if somebody will click on an ad or not, the performance of the model is usually bad. You cannot predict with very high probability if a person will click on an ad or not. But it doesn't mean that these models aren't useful. But the performance that you will observe on the validation dataset will be bad. And that's okay. _x000D_
This is just a characteristic (a feature) of this particular dataset – of this particular problem. That's why it's not a criterion, but you do need to try to train multiple models, as you see here (in the criteria for model training), and then try to tune their parameters. In the end, maybe you will not have a great model – you will not have great performance for this model – but at least you tried and this is what we evaluate here. We don't evaluate the performance, but whether you tune the parameters."
Data Engineering Zoomcamp;2023;What's the difference between MLOps and ML engineering?;We have an article about that at DataTalks.Club. If you go to the Articles section, there is an article called Roles in a Data Team. I discuss different roles, including machine learning engineering and MLOps engineering. MLOps is not really a role – yet, at least. Usually, MLOps is a set of tools and best practices for making sure it's easier to productionize machine learning models. Usually machine learning engineers apply these MLOps principles. But sometimes there is a separate role – it can be DevOps, site reliability engineers, or sometimes it's MLOps engineers – whose job is to make sure that the infrastructure for running machine learning projects is there and it's stable, reliable, and so on. For data engineering, the counterpart is DataOps. I think the concepts are pretty similar.
Data Engineering Zoomcamp;2023;Can we make the whole course on a Google Cloud VM?;Yes, you can.
Data Engineering Zoomcamp;2023;Is there a setup guide for all the software we need to use before starting the course?;Yes, there is such a thing. If you go here and scroll down, you will see the Tools section. If you go to week 1, there is a video, Setting up the environment on cloud VM, which explains everything you need to do. Then you can also check out the notes from the students. For example, notes from Alvaro are quite comprehensive. I remember that some students also took a lot of notes about the environment preparation setup, so you can take a look at the notes too.
Data Engineering Zoomcamp;2023;I'm a little confused about data pipelines. I'm curious which part of ETL is week 1? Extract or load?;The load part is when the data is already processed and you load it to a data warehouse. We don't touch a data warehouse in week 1. So I guess that may make it extract, or transform, or both. I guess that's the extract and transform part. The transformation we do is not really a transformation. We don't really do anything with the data. But if we did, then yeah, it would transform as well. I don't think it's really important. The important part is that we do something with the data and the data ends up in our database. Actually, we do load it to the database, so it's also load. I confused myself. So… [chuckles] I think it's all the parts. Anyways, let's discuss it in Slack.
Machine Learning Zoomcamp;2022;In which model will we need to make a decision about using cloud platform or local? Is it possible to do the entire course locally?;It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
Data Engineering Zoomcamp;2023;Do I have to bring over some blocks from one place to another?;"Jeff_x000D_
The answer is yes. It can be nice to define those blocks in code in Python. You can just run the script and boom, now they show up on your server – your new server, new workspace. We're also working on a way to make it a little easier to copy over all those blocks at once. That's something that, because of this course, I raised with some people and they are working on some things there. We're gonna try to make that as easy as possible. If you're using different workspaces or different servers, those blocks live on the server and you have to let them know about it."
Machine Learning Zoomcamp;2022;Can you explain a little bit what an â€œend-to-end projectâ€ really means? All the steps in the CRISP DM process?;"Exactly. This would be a really end-to-end project in any sense. So when you start with nothing and you end with a deployed and functioning system with monitoring, that is as end-to-end as possible. But then you can maybe try to make it a bit less end-to-end by getting some dataset that you already found or maybe collecting some dataset yourself? I don't think it will be end-to-end in this sense, but still. _x000D_
When you get some data, and then you deploy it, it's already better if you just train a model in Jupyter Notebook and call it a day. So just a Jupyter Notebook is not an end-to-end project, definitely, but going through all the steps in the CRISP DM process is definitely end-to-end."
Machine Learning Zoomcamp;2022;How often do you use hypothesis tests and p-values in your real data science job?;Quite often. I don't use them personally. We use them for running A/B tests. Let's take a churn prediction example – churn detection. We have one model for detecting churn and then we roll out a new model. Our ultimate goal is increasing revenue. So “What is our revenue in one month?” From these experiments, we know that our model has this precision and has this recall, it has this AUC, it has blah, blah, blah, all these metrics. But we don't always know how exactly these numbers translate into real revenue at the end – how much money the company will have. What we can do is take these two models and then send some of the customers to the old model (variant A), and some other customers to the new model (variant B). Then you can measure what the revenue will be after one month, let's say. That would be a hypothesis test. In this case, I think, it's a t-test that checks that the means are significantly different between two groups. And then p-value will tell you if you should reject this hypothesis or not – whether this new model is actually better, or whether they are the same. But I don't sit with pen and paper and calculate all these p-values. There is a statistical engine that does that for us. Of course, t-tests don't always work. There are some assumptions with this, like there’s normal distributions and these statistical engines usually select the best test for a particular situation. Maybe I will recommend for you to check our YouTube channel, where we have three talks about A/B tests. I was thinking about this one – Setting up an A/B testing Framework from Agnes. Agnes is my ex-colleague – we used to work together at OLX. She describes the framework we used at OLX for running A/B tests. This conversation with Jakob is about how his company runs A/B tests and this one with Sadiq shows how to set up the infra for doing this. It was a few weeks ago, so many of you probably have seen it. So I was referring to the one with Agnes. And actually, this one about Growthbook is another interesting one. They don't use traditional statistical tests – they use Bayesian tests. It's an open source library for doing these Bayesian A/B Test. So you can check that out too. Yeah, check this out. You'll find a lot of useful stuff. So the answer to your question is ‘yes,’ that happens often. Sometimes these questions actually come up during the interview, but not for ML engineers. I think data analysts and data scientists need to do these kinds of things more often than ML engineers.
Machine Learning Zoomcamp;2022;From your perspective, how is the market for data science and data engineering jobs in Germany?;Pretty good. Now there is a bit of recession – maybe not a bit, but the job market is also declining a little bit. So maybe there are not as many jobs as a year ago, but it's still possible to find a job. There are still job postings. At OLX (where I work) we’re hiring. Maybe we actually hire less in Germany than before, but still – companies are hiring.
Data Engineering Zoomcamp;2023;How do you emulate several terminals in the same session? What tool do you use?;If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
Data Engineering Zoomcamp;2023;What is the difference between calling a flow from a flow versus a function defined with @subflow?;"Jeff_x000D_
We don't have an @subflow decorator, so you wouldn't use that. But if you use some flow-decorated function and call another function with that flow also decorated, that's just kind of the way we use subflows. There is also another pattern where you can run deployments. You can call a function to run deployments called run_deployment from within a flow. That lets you just, “Alright, I want to run a bunch of things here. Maybe I have them in a loop or something.” That's a pretty cool way to go."
Machine Learning Zoomcamp;2022;Can you explain what exactly model.fit(x_train, y_train) does? Thanks.;"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. _x000D_
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
Data Engineering Zoomcamp;2023;Is the course Windows friendly?;It is, yes.
Machine Learning Zoomcamp;2022;If my target variable is continuous, can it affect the fact that it only has categorical variables for its prediction?;Basically, you only have categorical variables as features and your target variable is continuous. Yeah, there are no problems with that. Just train a linear regression, use one hot encoding, and the job is done – more or less. [chuckles]
Machine Learning Zoomcamp;2022;What part of data science/ML work do you work on every day?;"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. _x000D_
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you."
Machine Learning Zoomcamp;2022;I need to reorganize my homework directory in GitHub. This may relocate the location of homework 1 (already submitted). Is that fine?;It's totally fine. It's your GitHub repo – you can do whatever you want. You can delete it if you want.
Machine Learning Zoomcamp;2022;How to work with F1 and when to use it?;How F1 works, you hopefully learned from the homework. When to use it? I think always – when you want to use precision and recall (and you value them equally). When you want to have good precision and you also want to have good recall, then in this case, you need to use F1. In cases when you value precision more than recall, then you need to use a different approach – maybe just plotting things. There's actually a special case of F score called Fβ score (F beta score). Here, depending on beta, you give more weight to precision or recall. But usually the F1 score is good enough, because F1 gives equal weights to both.
Machine Learning Zoomcamp;2022;How should I find out whether my prediction model is working well or not using RMSE? Is there any way to change it to percentage?;There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
Machine Learning Zoomcamp;2022;How to improve the accuracy of the prediction? How to address the issue if model accuracy is drastically different between test set and validation set?;"If you have very good accuracy on the validation set, but you have very bad accuracy on the test set, you probably overfit. Maybe you should have a model that is less powerful. Then you should also check how exactly you split the data. Is there a difference in distribution between test and validation sets? Maybe this is the even more important thing to check. Does your data look different in the validation and test sets? If your data looks different in the test set from the validation set, then maybe you need to try to design your validation framework in such a way that it's similar to your test dataset. _x000D_
You should also ask yourself, “Is my test dataset really representative of unseen data or not?” It's probably that somewhere the model overfit, it may be using some data that is not present in the test dataset or maybe the model just got lucky. So then, based on what you think the issue is, you try to fix that. Sometimes it may even mean that you do your split differently – in such a way that validation, train, and test datasets may have the same distribution (depending on your problem)."
Machine Learning Zoomcamp;2022;What should we do when there is high bias or high variance to improve the model? What about test error?;"We don't cover high bias/high variance concepts. Again, check other theoretical courses from Andrew Ng. What you should do when you have errors is reduce the complexity of your models, for example. It really depends, right? Let's say you have a simple model, and you see that the simple model has this performance on your validation dataset. Then you can test that hypothesis, “Will getting a more complex model solve my problem or not?” Then you just try this more complex model and see what happens. Based on that, you make your decision. _x000D_
I don't know if I actually answered your question. Maybe a summary of my answer is “use validation and it will guide you in all these decisions.” And test error is, again, if you use your validation dataset many times, then at the end, you can't necessarily trust it because it might happen that the best model is the best just by pure luck. So that's why you have the test dataset to verify that it's not overfitting."
Machine Learning Zoomcamp;2022;Is it a good technique to train with small images and then use larger images for prediction?;It's an okay technique. I would still train the model on big images at the end as well. But, again, check it with validation and then you will see how different the models are.
Data Engineering Zoomcamp;2023;We have AWS, GCP, Azure, which we can work with in data engineering. Is the logic the same in all of them?;"Alexey_x000D_
Yeah, they're very similar. The services you have on AWS, you'll have exactly the same service, but with a different name in GCP and Azure."
Data Engineering Zoomcamp;2023;I can see that there are meetings scheduled for the next six weeks â€“ all of the resources are already on YouTube. What will we be discussing during those meetings?;It will be a Q&A like this one. You ask your questions and we answer. And that's pretty much it. Typically, it happens live. Sometimes it does not happen live. But the idea is to have a more or less live interaction.
Machine Learning Zoomcamp;2022;How can I know what the cutoff point is for a numerical variable? For example, when I use 0-2 months to compare churn with tenure? Was this trial error?;I don't think I understand your question, to be honest. Maybe I'll try to rephrase it… Or ask in Slack, maybe?
Data Engineering Zoomcamp;2023;In the exercises, you make us copy data locally and later upload to GCS/BigQuery. In an actual production environment, what would be the real approach?;"Alexey_x000D_
In an actual production environment, usually you have some processes that generate data, you capture this data and you put them in the data lake. Then the transformations you do take the data that you already have from the data lake and put it again in a data lake or in a data warehouse. In this case, the data lake is GCS and the data warehouse is BigQuery. This is what it typically looks like."
Machine Learning Zoomcamp;2022;When to use Bento ML instead of TensorFlow Serving?;I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
Data Engineering Zoomcamp;2023;Is the Data Engineering Zoomcamp playlist on YouTube complete or can we expect some new videos to be added?;"Alexey_x000D_
It's more or less complete. There is one thing we wanted to update, which are the TerraForm videos. Sejal, unfortunately, got sick. In Europe, there was a big outbreak of the flu. I was also one of them. Sejal caught it a bit later than others. So she's unfortunately now sick. She's recovering. We'll have some videos from her there. But fundamentally, not much will change. There are videos from her from last year, but in those videos, the code is already written – not everyone liked this approach, so we wanted to re-do this. And you will see these videos. Right now we have the old material, but that's the only change we still want to do. Depending on the problems you have right now, we will see. This is what we did last time. We saw the problems many students had and we recorded some videos to answer those problems._x000D_
Ankush_x000D_
I just wanted to add that I'm doing some more videos on Kafka. The current playlist is not complete on Kafka and I will be adding at least three, four more videos on that._x000D_
Alexey_x000D_
Anyone else is working on more videos right now?_x000D_
Victoria_x000D_
I was planning to do something like “What changed in the last year for DBT?” For example, now you can do Python there as well. But no update to the project."
Machine Learning Zoomcamp;2022;What are your top three or more ethical guidelines while applying machine learning to solve real world problems? Predicting repeated crime, recommendation, etc.;[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.
Machine Learning Zoomcamp;2022;Would you need machine learning for recommender systems? If yes, in which case?;For pretty much every case, I guess. But yeah, you definitely need machine learning for recommender systems. Maybe for a start – if you're just starting with a recommendation system for your platform – there are simple things that you could do, like seeing what the most popular thing in a certain age group is, or what the most popular thing in a certain category is, and then the command in that. But, quite quickly, you will realize that you need something more substantial, let's say, and that there is no way to do it without machine learning. So you will have to use machine learning. The answer is for any – for every case.
Machine Learning Zoomcamp;2022;In one of your podcasts, I heard the phrase â€œConverting a business problem into a machine learning problem.â€ Can you give us two brief examples to understand what it means?;"Imagine that you're running an ecommerce company – simple ecommerce. Our CEO comes and says, “Our users have big problems discovering items. How can we help them?” Then you need to understand “Okay, what does it mean ‘they have problems discovering items’?” You need to understand what problems they have and how we can help solve these problems. Once you understand the problems, then you can generate possible solutions. One example is coming up with a good search. Another solution could be coming up with good recommender systems. It also depends on a business goal. What do we want to optimize? Do we want to make sure that our users buy more or so that it's actually easier for them to find what they're looking for when they are just exploring? This is all input that comes from the business people – the stakeholders in the company – and the users. Taking all this input into account and thinking, “Okay, now I know what I need to work on to solve this problem. What can it look like?” And the solution could be designing or coming up with a recommender system that is on the item page that shows similar items. Or, for example, showing what people buy in addition to this item. So that's one example. _x000D_
Another example. Let's say we have a video hosting coming – like YouTube. All of a sudden, people start uploading some content that we don't want to have on the platform. We want to get rid of this, right? Business comes to us, the data scientists or software engineers, and says, “Can we somehow make sure this doesn't happen?” And then we can think, “Okay, what are the ways that we can make the bad content go away?” It could be introducing a content moderation platform, for example, where we apply some different machine learning algorithms to a video in order to understand that this video should not be on the platform. Instead of a video, it can be anything. _x000D_
So this process of listening to the stakeholders, understanding what the problem is, and then up to the point where you have an idea of how this could be implemented – this is what I call “converting a business problem into a machine learning problem.” I hope it makes sense now."
Machine Learning Zoomcamp;2022;Do you think that after finishing the ML Zoomcamp, it's possible to work on ML engineering freelance contracts or does one need years of professional experience?;"It's possible. The sky's the limit. It's up to you. How do you sell yourself? I will tell you that I did that. I didn't finish ML Zoomcamp, of course – I finished Coursera courses and I did freelancing then. Before getting my full time job, I freelanced for a couple of years. I was also studying at university and then, when I got my first data science job, I already had a bunch of portfolio projects from freelancing. So it's certainly possible. One caveat, though – the market in 2012 was very different from 2022. But I think the need for freelancers is still there. _x000D_
I don't know how to find freelancer contracts now. Maybe it's like the usual ones, like Fiverr, UpWork, things like that. You need to have professional experience somewhere. In my case, I had experience in Java and I learned ML. So my profile was – I was helping companies with machine learning in Java. I already knew Java. Maybe that's an important piece of information that you also need to know. If you don't have professional experience at all, it might be a bit difficult, but you should probably focus on portfolio projects, not only the years of experience. If you can show the potential client that you can solve their problems, they wouldn't care if you have worked before or not. They will see your portfolio and they will understand, “Okay, this person can actually do the thing I need.” _x000D_
Maybe one more thing – I took part in a Kaggle competition that was about detecting duplicates. And then, because I was on the leaderboard, a company who had the same problem reached out to me saying “We have the same problem. Can you work with us and help solve it?” They didn't care if I had any professional experience. They only cared about my solution. They already knew from Kaggle that I could solve this problem and that's why they reached out to me. The idea there is – it's not like I was just working on this problem alone and didn't tell anyone. I was actively sharing what I was doing in public. _x000D_
That's why I encourage everyone – every single person who is taking this course – to share their progress. Because if you don't, then nobody will know that you actually have the skills. But if you do, for example, you share something about a project you did and this project could be about detecting duplicates, for example. I don't think any of you did a project like that, but just as an example. Then maybe somebody who needs to solve this problem will see this post and will think “Okay, this person actually did the project on that. Let me contact them and see if they would be up to helping us.”"
Machine Learning Zoomcamp;2022;What are the general points between computer vision and NLP?;I don't think I understand the question. What do you mean by “general points”? Do you mean “What are the similarities between them?” For NLP, you can use some of the ideas from computer vision. For example, convolutional neural networks work for NLP as well. But that's pretty much the extent of what I know about that. [chuckles] I don't know much about NLP, to be honest – only the basics, like embeddings. I don't know how to answer that question.
Machine Learning Zoomcamp;2022;Why did we save our model to bin? How about Pickle and other formats? What is the difference between them?;"The extension of our file was bin simply because we decided to give it this name. It's actually Pickle, but it’s just when saving, I put “.bin” there. You could put anything there. You can put the “.pickle” you can put “.txt” – anything – it doesn't change the content of the file, which is a Pickle file. Other formats include – there's a bunch of different formats. There is Joblib, which is almost Pickle – there is PMML which is a special format for machine learning models. You can use them between different languages. _x000D_
For example, you create a model in SciKit Learn, and then you can save it and import it from a Java application. I don't use it, to be honest, because I usually write the training jobs in Python and then the serving scripts are also written in Python. So we don't have this problem. The difference is in the nuances and details. With Pickle – I don't remember if I mentioned that in the videos – there are security issues. For example, if somebody with malicious intent wants to execute some code on your computer, they can insert this malicious code into the Pickle file and when you load the pickle file, the code will be executed. Then potentially, the person (hacker) can overtake your computer. So load Pickle files only from the sources you trust. Other formats have similar pros and cons. PMML does not have this problem. But maybe it does the Py adoption. With Pickle, it's very simple – it's a built-in library in Python. You just import it, you don't need to install anything, – you just do pickle dump and then you have your file."
Data Engineering Zoomcamp;2023;Is it possible to tune the environment fully in GCP?;Yes, that's why we have the environment preparation video
Machine Learning Zoomcamp;2022;Can you provide the link to the Docker video that has been removed for setting up Docker? Thank you.;"I think it's confusing because there is no video 1.6 about setting up the environment. It's just a page. There was no video – that's why it wasn't removed, it just never existed. But if you actually want to find this Docker video, then you go here and here is the video. This video is not from this course – it is from a different course. But the steps you need to do here are very similar to what we need to do in this course. _x000D_
You can just go through this course and this video shows how you configure a remote machine on AWS. I hope that answers your question. It was never a part of this course. Maybe if somebody wants to contribute – maybe not everyone likes watching these videos – so if you want, you can just maybe go through this video and contribute a step-by-step guide for this. If you're interested – if somebody might need this."
Data Engineering Zoomcamp;2023;Any thoughts on ChatGPT? There are a lot of rumors about it taking over the jobs of software professionals, a lot of layoffs, etc.;"Alexey_x000D_
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
Machine Learning Zoomcamp;2022;Is ROC useful for multi-class?;I don't find it particularly useful to be honest. For binary, yes, it is definitely used. For multi-class… it becomes difficult to interpret, to be honest. So for multiclass, in my experience, accuracy works the best. The rest become more and more difficult to understand as the number of classes rises.
Machine Learning Zoomcamp;2022;What tools do you use to draw on your PC screen?;This question is so commonly asked that I think I should put it in the FAQ. I use a tool called Drawboard. I actually don't have it on this computer. This is a generic laptop. I don't have a touchscreen. But for Drawboard, you need to have a touchscreen. You actually need to have a pen. I have another tablet, which is Microsoft Surface tablet. This is also Windows and this is actually what I used for recording like 99% of the videos for this course. On this tablet, I have a pen. I just open Drawboard and I start drawing there. So it's a combination of tablet plus Drawboard.
Data Engineering Zoomcamp;2023;Can you paste the link of the taxi data or update the GitHub page?;It’s already there. It has been there for quite some time. So if you go here, there is a note – and this is the link.
Machine Learning Zoomcamp;2022;What is the difference between epochs and iterations?;One epoch consists of multiple iterations. Remember, in the code – if you go to the course and open deep learning and open a notebook. Go to where we start training a model. This thing (batch_size=32) batch size. When we specify batch size, it means that we take our entire dataset, and we chunk this dataset into batches of 32 images. It's actually called step, not iteration. Then each step would be fitting a model on that batch, specifically. And when you go over the entire set of chunks, then you finish your epoch, usually. Usually, an epoch is going over the entire dataset once. It's not strictly correct. You can define epochs differently. You can define epoch as 100 steps or 1000 steps – it’s up to you. But usually, in practice, you say that one epoch is one iteration over the entire dataset. I guess when I say iteration/epoch, to me, it seems like I can use them interchangeably. But the difference between epoch and step is what I just described. Maybe if you find a different definition, please share it in Slack.
Machine Learning Zoomcamp;2022;Can data science jobs be done remotely for freshers?;"Yes, of course, it can. The sky's the limit. But I think it will be quite challenging for somebody without experience to get a data science job remotely. Then again, it means remotely within the same country – then it's possible. For example, at OLX (the company where I work) we had both interns and juniors who were hired to work remotely – if they are in the same city or in the same country because we're hiring remotely. No problems with that. But let's say if you're based in one country and you want to work remotely for an American company, or a European company, then it might be tricky. _x000D_
Not every company is ready to do that. There are companies who do that. You probably need to find these “remote-first” companies that hire everywhere and then you will need to convince them that you are good enough, even though you don't have experience so that you can work for them. It’s tricky. This is one of the reasons we actively encourage you to do this “learning in public” part. If you do this often, then you will be able to get a job as a fresher, even though you might not have “formal” experience. But because you did all these projects, you shared your learning and maybe some people started following you, then companies might notice you. Then it will be easier for them to hire you. _x000D_
I think this is actually what happened last year. I don't promise it will happen this year, too – but last year, there was a company, Delphi, who hired two interns. The company is Berlin-based and one of the interns was in Milan, Italy, and the other intern was in Ukraine. So it wasn't the same country, yet they managed to hire them. So I think it's possible. It was actually possible with this course. I don't know if we will have this kind of setup this year. _x000D_
I haven't heard from any company who wants to hire interns this year. If you're working at such a company and you need interns, then please get in touch with me. People who graduate from this course will be really good interns, I am sure. But you can maybe help hire people like this person who asked the question."
Machine Learning Zoomcamp;2022;Can you elaborate on when to use SciKit Learnâ€™s OneHotEncoder versus Dictionary Vectorizer?;I personally prefer Dictionary Vectorizer because, later in module five, we will put it inside a web service and the web service receives a JSON object – usually it's a dictionary. So we can just take whatever we get as a request, put this inside the Dictionary Vectorizer and then get a matrix. But with OneHotEncoder, I find it more difficult to use – you need to do more. Usually, it's just a matter of taste, I would say. I think if I go back to last year’s Office Hours, in one of the videos I talk about OneHotEncoding. I basically show how to use it. I don’t remember which one, so I think I should go through this playlist and see. I think I described it somewhere. But my personal preference is to use Dictionary Vectorizer because it is simpler to use and you can see how much more things you need to do for OneHotEncoder. But again, this is just personal preference. With both of them, at the end, you actually get the same result, so it doesn't matter.
Data Engineering Zoomcamp;2023;Could you share with us some real-world cleaning strategies, good practices, checklists, and what to look for when at the â€œtransformâ€ step?;"Jeff_x000D_
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
Data Engineering Zoomcamp;2023;Is it possible to do the course in Scala instead of Python?;For the homework assignments, we expect that you do them in Python. Prefect, to my knowledge, does not work in Scala. For example, for Spark, you can use Scala. For Kafka, you can use Scala. If you want to use Scala, use it. For your final project, you can use Scala as well, just add as much documentation as possible for people who will review your projects, because they most likely don't know Scala.
Data Engineering Zoomcamp;2023;Do we leave the GCP instances running until the course completion?;I wouldn't. You can just stop the instance without terminating it and then you can resume it. It's effectively doing the same thing as powering off your machine and then turning it on when you're back and want to work.
Data Engineering Zoomcamp;2023;I am new in the data engineering space and I want to learn from A to Z here, but Iâ€™m still confused about where I should pick this up and start my learning effectively.;You are in the right place. That's why we have the course. Just follow week 1 to week 7 or to week 6, do the project, and you'll be fine.
Data Engineering Zoomcamp;2023;How can we contribute to the course?;The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
Machine Learning Zoomcamp;2022;In addition to image classification, what are some other use cases for neural networks? Can neural networks be used on text?;They can definitely be used on text. I think if you just Google “neural networks for text”, or “deep learning for text”, you will find a lot of material. That’s a good way to start. There is also a Stanford course called Natural Language Processing with Deep Learning. It's very similar to the course I was referring to in the image classification module. But this one is more advanced – it's more difficult – and it focuses more on text. That is also a good resource. You can start learning about that by learning what text embedding is, and things like Word2vec, word embeddings, Word2vec growth and things like that. Then you can slowly progress to more complex models.
Machine Learning Zoomcamp;2022;In my project deployment, I want to post requests not in the JSON format, but text used to collect my data (scrape). How can I do this? Write a function on service.py?;Yeah. For example, do you use Flask? In Flask, you can see what kind of data... But wait, why don't you just put the data – the text – in your JSON? Your JSON file will look like this curly bracket and text is a parameter and then the text that you have, you just put it as a value and you send that. That will make your life simpler, I think, because you might also want to add some extra data (some metadata) about the text. So I would actually suggest you stick to JSON and put your text and JSON. But Google is your friend. You can just check for the framework you use – if it's Flask, then check for “Flask text input content type” – something like that. Or the same with Bento. With Bento, it's actually more explicit. If you remember, we defined the type – it could be JSON, it could be a numpy array. They probably also support text, but you will need to check the docs.
Machine Learning Zoomcamp;2022;I think this one is related to what we were talking about. â€œShould data scientists be able to deploy models?â€ Then the question is â€œML engineers should be able to work on the whole machine learning pipeline, not only deployment.â€ What do you think, Tim?;"Tim _x000D_
I think “ML engineer” is a fairly new title. I do think it implies that they should work the whole pipeline. [chuckles] I am not sure that the industry is going to end up with these individual people who can do the whole thing and be really good at it. That doesn't seem like a scalable approach to me. I think eventually you'll find that the tools will get better, or good enough, where people can specialize and not have to know every part of the pipeline. Not know every part of the pipeline in a really deep way, but I think it's helpful for everyone to know the pipeline at a high and medium level to be able to collaborate better. Collaboration, especially in ML, is difficult because there's so many people involved._x000D_
Alexey _x000D_
What I see in practice is that often data pipelines are built by data engineers and then there is a bit of feature pre-processing, feature engineering – all that – and usually, the data scientists take care of that. And then ML engineers focus more on deployment and then scaling and then DevOps, MLOps people support more on the infrastructure side, like “How can we make sure that Kubernetes is running, that all the logs are saved, that we can monitor the whole thing?” At least this is what I see around me in the company where I work. I talked to a few other companies and they have a similar setup. Sometimes ML engineers work in detail on training pipelines as well, but I think it's more common that data scientists do this. At least from what I see. I haven’t interviewed many companies – I just talked to a bunch of people here in Berlin when I went to have lunch. I asked, “Okay, tell me about how your organization works. What kind of roles do you have? What kind of responsibilities?” And this is what they tell me._x000D_
Tim _x000D_
I think as the industry matures, the specializations will become clearer. 15 years ago in software development, DevOps didn't even exist and software engineers were supposed to be doing the builds and the testing – everything. Then we realized, “Okay, we need somebody who kind of bridges Ops and development.” That became DevOps. And then the software engineers, as we progressed in data, were doing all the transformations and the pipelining and everything else. But then we said, “Oh, actually, we need data engineers.” So that became another thing. I think as the industry matures, the specializations will become more clear. But there will always be the early people in the industry who benefit and companies who benefit from people who work the whole pipeline._x000D_
Alexey_x000D_
I think there was a term “full stack data scientist,” right? A person who can do it all. I don't know if it’s still a thing. I think a couple of years ago it was, but now we have ML engineers. Before, it wasn't like a very prominent role but now we see more and more ML engineers."
Machine Learning Zoomcamp;2022;What types of projects are expected for data analyst jobs? Which industries have the most opportunities for data analysts in terms of machine learning?;Well, from my point of view (I might be mistaken because I'm not a data analyst) data analysts usually do not work with machine learning. They sometimes do, but it's not their main focus. Some analysts might use machine learning as an extra tool for doing what they need to do, like forecast sales, for example. But that's not their main domain expertise, let's say – not their main area. Machine learning is more a data scientist kind of thing. That being said, to answer your question, “What types of projects are expected for data analyst jobs?” They are usually about analytics. You get a dataset and then you need to, let's say, build a report from this dataset to get some business insights. Again, I don't work as a data analyst. That's pretty much using SQL and some data visualization tools to turn a dataset you have into something understandable and digestible. If you build a project like that – you can take any of the datasets we use and if you build some cool visualization for that –that's a good kind of project for a data analyst role.
Machine Learning Zoomcamp;2022;Let's say that after finishing courses, we launched a free web-app, how can we make it faster, and how to manage high traffic? Are there any free tools?;"Again, by “coursES” I’m not sure what you mean – probably Zoomcamps? If we are talking about the ML Zoomcamp – here, in addition to Flask, Docker, and or everything we've covered in week 5, we will also learn about BentoML in week seven. BentoML can actually help with making your web application faster. We will also talk about serverless – I think week 8 or 9 (I don't remember). Serverless can also scale up easily, so the more traffic you have, Amazon AWS will automatically scale it up and make sure that your lambda function can handle all the traffic. _x000D_
Other cloud providers have similar things to lambdas. In Google, I think it's Cloud Functions in Azure, it’s Cloud functions – I don't remember the exact names. But I think most of the big cloud providers have some sort of serverless option, so you can use that. Then there’s another option, which we will also cover, which is Kubernetes. In Kubernetes, there is an option to do autoscaling. That can work too. _x000D_
In this course, we do not cover autoscaling in the video materials – maybe we will have this as a homework assignment. We'll see. But the materials we will have in this course will give you enough foundation to actually go and read more about this topic and learn how autoscaling in Kubernetes works, and then implement this. But I think the simplest approach would be to first check out BentoML and then serverless could be also a good option."
Data Engineering Zoomcamp;2023;Is there any guide to set up on a local machine?;You can follow the same setup here. Apart from creating a virtual machine and doing port forwarding, everything else that you do in this video also applies to a local Linux environment. Maybe you will also need to install Google Cloud SDK, because on a virtual machine, you already have it – you don't need to install it. Locally, you will need to install it. Apart from that, you can just follow the same stuff for a Linux computer. For Windows or Mac, I don't think we have a guideline.
Data Engineering Zoomcamp;2023;I read a book that said the data engineering lifecycle is generation, storage, ingestion, transforming, and serving data. Do we do ingestion because we use an open dataset?;I don't understand the question, to be honest. We don't have the generation part. The generation part is what was done already for us by the New York Taxi Limousine Company. Storage – yeah, they also store it. They host the data. And yeah, we cover ingestion, transform, and serving with the dashboard.
Machine Learning Zoomcamp;2022;Please consider adding links to video 1.2/06-environment.md in the Deployment model as a prerequisite. Also please add that video 5.5 will explain more.;Maybe you can just go ahead to our repo and create a pull request. That will help. That is certainly useful and you will help immensely if you just go and create a pull request with that and we will just accept it. Great idea. Thank you.
Machine Learning Zoomcamp;2022;I see that there is a question about Bento â€“ whether it can be integrated with AutoML tools like PyCaret and the like.;"Alexey_x000D_
Actually, we didn't cover AutoML here in this course, so maybe a little bit of background. Tim, you probably can tell us a few words about what AutoML is for those who don't know._x000D_
Tim_x000D_
AutoML is automatically training and generating a whole bunch of machine learning models and then seeing which one is the best. It's kind of brute force machine learning. _x000D_
Alexey_x000D_
Here, for example, in module six, when we were talking about XGBoost – remember the approach we took? It's called manual tuning. We started with tuning the learning rate ETA, then we tuned the depth parameter, then we tuned to some other parameter (I don't remember which one). Then at the end, we arrived at the solution that is good. But with AutoML, you don't need to worry about this. So you give AutoML a data frame, or numpy.array – you give it some criteria, then you throw it at the AutoML, you go drink some tea, and then you come back five minutes or five hours later and then you have the perfect model. Right?_x000D_
Tim_x000D_
Yep. Yeah, BentoML doesn't really... We rely on the ML training frameworks to do the training part. Once you have the model itself, that's when you come in and you say “BentoML.save”. You can integrate it pretty easily with a lot of different training frameworks, as long as you can save the model at the end. MLflow is a tool that generates a lot of models and experiments and once you find the best one, essentially, you save that one with BentoML and that's the one that you deploy to your service._x000D_
Alexey_x000D_
And speaking of that (short shameless plug). We have a course called MLOps Zoomcamp. In this course, AutoML wasn't really a topic, but we used it for illustrating some of the concepts from experiment tracking. In this module, we talked about experiment tracking. Here, we used Hyperopt, which is a library for finding the best XGBoost model. Then we got the best model and we saved it with MLflow."
Data Engineering Zoomcamp;2023;Will this course show me more than tools? I mean learning to do ETL, system design, etc.?;This is a very practical course and because of the nature of the course, it is very tool-oriented. But, again, we do not just show the tools, but also explain the main concepts. These concepts are transferable between tools. For example, we cover Prefect, but when you start working, or when you’re at work, you encounter a different orchestrator like Dagter or Airflow or something else, you will already know the concepts. Therefore, it will not be too difficult for you. But yeah, the focus is more on tools rather than other things. We don't really cover system design here.
Data Engineering Zoomcamp;2023;Is Google Cloud Platform mandatory?;"Alexey_x000D_
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally._x000D_
Ankush_x000D_
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that._x000D_
Alexey  _x000D_
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally."
Machine Learning Zoomcamp;2022;If I want to train my dataset for a capstone project, where should I start now? I am looking for a â€œHello Worldâ€ to create my first dataset.;"By “train,” I assume you mean you want to collect your dataset. What you should start with is asking yourself what kind of problem you want to solve. Once you identify the problem, you then need to think about “Where can I get this data? Do I need to collect this data yourself? Or can I scrape it? Or can I maybe buy it or find it somewhere?” So that's what you need to ask yourself. Then with the goal in mind, you go to the internet, you try to find the dataset. Maybe you don't find anything so then you can see if there is a website that has this data. For example, if you want to collect a dataset for house prices, then you go to a website that lists houses and then you can scrape this information from the website, or scrape information from Amazon, or scrape information from some other resource. So then you can scrape this data and in this way, you collect the dataset. _x000D_
If the information you are looking for does not exist at all, then you will probably need to collect a dataset yourself. It really depends on the nature of the dataset. You can ask some crowdsourcing platforms, such as Amazon Mechanical Turk or Toloka. There are actually multiple companies like this (that do crowdsourcing). With Toloka, we actually had a video on our channel. It was a workshop with Toloka. I think with Toloka, we probably have another workshop soon, as well. So keep an eye on this. But yeah, so you can use crowdsourcing, or you can just go and collect it yourself. It's a very generic recommendation, because I don't know what exactly you want to do. Maybe you can share more information and we can talk about this in Slack."
Machine Learning Zoomcamp;2022;Does predict_proba give out real probabilities? If not, how to convert them?;Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
Machine Learning Zoomcamp;2022;Can you talk more about using GCP for completing the course? I was able to use Vertex AI to create a pipeline, and it was really cool.;I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
Data Engineering Zoomcamp;2023;Is there an option to host an Office Hours episode outside of the office hours?;Maybe tell us more in Slack what exactly you mean by that and what you want to see there? What kind of content?
Machine Learning Zoomcamp;2022;What is the reason behind BentoML generating a dedicated directory of models? (This is what we just talked about.);"Tim_x000D_
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it_x000D_
Alexey_x000D_
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”_x000D_
Tim_x000D_
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models._x000D_
Alexey _x000D_
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments._x000D_
Tim_x000D_
Right."
Machine Learning Zoomcamp;2022;How would you go about adjusting the price data for inflation?;"Here, what we can do is – we have a process for collecting data. Let's say we run a website where we sell cars and we can see that inflation is there because the prices that people set for selling the cars are growing. What we can do is, every month we can just use this data that we have for retraining our model. Perhaps we can – I don't know if it makes sense or if people do this – perhaps for the prices in the past, we can retroactively adjust this based on inflation. _x000D_
Let's say that if we know that this year in Europe, the inflation is 9% or something like this, or we know that for cars specifically the inflation is this number, then perhaps we can adjust it. But I don't know if people actually do this."
Machine Learning Zoomcamp;2022;Apart from this course content, what are the other skill sets that I need to learn to get into a ML-related job?;"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. _x000D_
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
Machine Learning Zoomcamp;2022;Do you run a private mentorship program (probably paid) that one can join?;I do not. I was actually thinking about doing this, but it will probably be quite expensive. If any of you are interested in this, maybe write to me in Slack and then I can think about how to organize that. But so far, there is no “official” program for that.
Machine Learning Zoomcamp;2022;What happened with the Zillow failure. Is it a data science case?;I don't know much about this, to be honest. I know that their model for predicting the price of real estate went rogue. So it started predicting some… I won’t say more, because I'm not super sure about what exactly happened there. I wasn't really following. I know that the company lost a lot of money because of a rogue model. I think if you just Google that you will find an explanation.
Machine Learning Zoomcamp;2022;I'd like to review more than three projects. Is this welcomed and will it be rewarded with extra points?;"You're more than welcome to do this, and your peers will appreciate it. But we capped the number of points you get for that at three projects. So you will not get more than three points for each project. You will get nine points for evaluating the projects, but you will not get 12 points. So it's more than welcome – you will learn a lot by doing this – but you will not get extra points. _x000D_
You shouldn't evaluate the projects just to get the points, you should evaluate them because you want to help your peers and you want to learn something new. This is a very good motivation and you will be rewarded with new knowledge. This is how I should have answered that. [chuckles] Not with points."
Data Engineering Zoomcamp;2023;Does anyone know any good resources for clean coding in Python?;"Michael_x000D_
Actually, I do have a resource – Real Python. They do have some paid content, but they also have a lot of free content that I look at frequently. Also, if you go to sites like LeetCode, or do the Advent of Code – not necessarily to solve the problems, but looking at others’ solutions – you can see some very good examples of clean code. Type Hinting is something that I’ve picked up a few years back from Advent of Code. I still don't do it all the time, but when you see more experienced people write code, you kind of know what you're aiming for. And I think that gives you a lot of direction._x000D_
Alexey_x000D_
I think in this video, Getting a Data Engineering Job, Jeff also talks about learning good coding practices. I remember that the project he recommended to look at, if you want to learn more about good coding, was Prefect. He said, “Yeah, go check out Prefect. It's a great way to learn how to write good Python code.”I guess it's a good plug, right? [chuckles] _x000D_
Then there is also a book called Clean Code. It's about Java. It's a very nice book. It's actually not about Java, it's about clean code, but the examples there are in Java. For me, it was very useful when I was starting coding. But I was a Java developer. What I know is there are examples in this book in Python. Even though the book is about Java, there are GitHub repos where the same concepts are illustrated with Python. You can check that out, too._x000D_
Jeff_x000D_
Yeah, I haven't read this book. But there's also a Clean Code in Python book that is out there. It has good reviews on Amazon. It could be worth checking out."
Machine Learning Zoomcamp;2022;What to do after courses? Where do you see us after courses?;Working, I guess. So yeah – work after courses. That's where I see you.
Machine Learning Zoomcamp;2022;There is a question about when the deadline to finish the midterm project is.;"Alexey_x000D_
I want to show you again how to find all the deadlines. You go to ML Zoomcamp – this is our _x000D_
course page. Then you go to cohorts, and then you go to 2022, which is this cohort, where you will find the deadline calendar. Click on this link and you will see that these are the deadlines. For the midterm project, the deadline is the 7th of November and then there will also be an evaluation phase and it's due on 14th of November. Then we will learn about neural networks, serverless deep learning, and Kubernetes. That's the plan right now. Things may change in the future. This is not set in stone right now. But this is to give you a rough understanding of where we will be. Most likely, we will stick to this – but things happen."
Machine Learning Zoomcamp;2022;Would you recommend doing Andrew Ngâ€™s updated machine learning specialization after this course?;Maybe. I haven't seen the updated machine learning specialization. I don't know what that is and what kind of content there is. I took the course back in 2012 (I think) when it was still using Okta. I do recommend taking this course. You still need to do interviews. Let's say you're looking for a job and your goal is to get a job – you start going to the interviews and you see what questions they ask. If these questions are about theory, then I think the best way to learn this theory – the theory that wasn't covered in this course, such as various bias trade-offs, gradient descent, we didn't cover many things – if you want to learn these things, then I don't have better recommendations than Andrew Ng’s course, at least the old one. But I think the updated one should be even better, right? That's why they updated it.
Data Engineering Zoomcamp;2023;Are there any rewards for the best students?;Yeah, we can pat you on your shoulder. [chuckles] There’s the position on the leaderboard. The leaderboard is anonymized so nobody will know that this hash belongs to you, but if you want, at the end we will have a forum where you can share your actual contact information – your actual information about you and you will be the first in this public leaderboard. This is how I can kind of give you a reward for that. But apart from that – eternal glory? Maybe that?
Machine Learning Zoomcamp;2022;Do you consider that by completing the homework of this course we have a portfolio to apply for an MLE or AS role?;I don't know what AS is – applied scientist? Yes and no. I would, again, put emphasis on projects. Course homework is okay too. When looking at someone’s portfolio and I see that this is a homework project, I would ask myself, “How much of this was guided? How much of this did the candidate actually do themselves?” With projects it’s different. Projects are usually more independent. Thus, with projects I will have a lot less questions for you. Homework assignments are fine too, but they are more meant to make sure that you understand the material. They are good for you, but… maybe it makes sense to include them in your portfolio as well. But do projects – that's much better.
Machine Learning Zoomcamp;2022;On a scale from 1 to 10, how weird is it to have a Zoom call where you're the only one with a camera?;It's super weird. I got used to this, but the first time I did it, it was super weird. It's really strange. On a scale from 1 to 10? I don't know. 8, maybe. It's much more interactive (or less weird) when there is somebody on the other end of this call. But I know that you're actually listening there. I just don't see you. Which makes it maybe a little bit less weird.
Machine Learning Zoomcamp;2022;Is linear regression used more for prediction or inference purposes (in the ML community)?;What is the difference between prediction and inference? I don't think I know. To me, they both sound synonymous. They're used for both, I guess.
Machine Learning Zoomcamp;2022;Is there a way to read all the answered questions from this Slido?;I'm not sure what you mean. Do you want to take a look at all the questions? Or you're interested in the answers too? I think there is a way to export the questions because it's the same Slido link for all the sessions. I'm also working on actually turning all my answers into text. There will be a table from AirTable with all the answers. Maybe I'll share it later. It's not finished yet. But yeah, there will be all the answers I gave in text form. As you can imagine, of course, it's very time consuming. It takes a bit of time, but I will share this thing with you soon.
Machine Learning Zoomcamp;2022;If we are going to be late on the midterm project, is there an extension or second submission date?;"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. _x000D_
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. _x000D_
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
Machine Learning Zoomcamp;2022;In the real world, after churn, risk customers are found and measures are taken (discounts, for example) the model will become irrelevant, right? How is this handled?;Why do you think it will be irrelevant? Do you think we'll just identify all churned users, bribe them with discounts, and live happily ever after? At some point, other users will consider leaving too, so we actually need to run this model regularly. We will need to update this model regularly – retrain – because the model will probably make mistakes. So it's probably a never-ending process here. We will always need to keep an eye on this model – probably in an automated way. Maybe we have a process that automatically retrains it every half a year or something. But I don't think just deploying this model and using it will solve the churn problem. Maybe I misunderstood your question. I don't know.
Machine Learning Zoomcamp;2022;Is it OK to make capstone project 1 similar to midterm project (without deep learning, serverless, etc.)?;Yeah, it's okay. You don't have to use deep learning for the capstone project. As I said, if you just use the material for the first seven modules for your capstone project, you'll be fine. You don't have to use deep learning.
Machine Learning Zoomcamp;2022;Will you do a data engineering Zoomcamp in 2023?;Yes, we will do this. I think I announced it last week. So yes, we will have it. I still have to make announcements on social media. But if you just want to register, you can go to the Data Engineering Zoomcamp on GitHub or just click here, and then you can sign up. You will get a notification closer to the starting date. For the ML Zoomcamp, we have a telegram channel. You can sign up for that too.
Data Engineering Zoomcamp;2023;Does DataTalks.Club have career services?;Not really. But I'm not sure what you mean by that. What kind of services do you need? It's a community. If you need some help from the community, you can ask the community for help. It could be a review of your CV, a review of your project, or just general career advice. For example, you could say, “I am a data analyst. I want to become a data engineer. I'm taking this course. What else can I do to be more attractive to potential employers?” For example, ask a question like that and then we have a career questions channel where you can ask this kind of stuff.
Data Engineering Zoomcamp;2023;Do BigQuery run and query times change based on the format like CSV/Parquet? For example, can using Parquet lead to lesser data scans/query time for external tables?;"Alexey_x000D_
I think it depends. If it's an external table, then yes. If it's internal, I don't really know how it works, but I think in this case (I might be wrong) the data is already stored internally in BigQuery and you can do partitioning, clustering on that to have fewer scans and faster query time."
Machine Learning Zoomcamp;2022;What library do you use for data visualization?;Seaborn.
Data Engineering Zoomcamp;2023;Is Prefect limited to the number of cores that Python can use or is it more flexible, like solutions such as Argo Workflows that work on top of Kubernetes?;"Jeff_x000D_
There, really, the sky's the limit in terms of the ability to paralyze work as you go. We have integrations with Prefect and Dask and the ability to use Kubernetes to scale things out. Absolutely. Check out the collections catalog to see what we have right now. You can also make your own custom block if you want to, from a custom collection, if you want to contribute back to everyone else. So, Ray and Dask are two task runners that we have – two libraries that are popular, that we integrate with, where you can spread out your operations over multiple cores and over multiple machines. Those can be popular for those kinds of use cases._x000D_
We saw how to use a Docker block. We also have a Kubernetes block. So that is something you can pull. We use the infrastructure of the Docker container. You can use Kubernetes Job. That’s kind of the easiest way to go with things. That's totally available right here, and then things will kick off in your cluster."
Data Engineering Zoomcamp;2023;Can you please explain what is the best way to save data when it is collected with parsing â€“ in BigQuery or collecting it in Postgres first?;"Alexey_x000D_
I would actually suggest neither. I would actually suggest something like a data lake first. And then from this data lake, you can parse it – you can transform it and you can save it in BigQuery, Postgres, whatever data storage you want."
Data Engineering Zoomcamp;2023;What will the job market be like in 2023?;"Alexey_x000D_
That’s a very difficult question. I don't know if I should attempt doing any forecasting. Anyone else want to answer that question?_x000D_
Luis_x000D_
I can answer because… Well, I don't know if I can, but if I may. I left my previous company in the last week of the year (2022) and I already am in lots of processes, including in the final process to be a data engineer. Today, it's the 16th of January, 2023. The market is completely imbalanced, in the good sense – for us. [chuckles] _x000D_
Alexey_x000D_
So there’s hope that, even though there is a recession and layoffs, it's not difficult to find a job. That's what you're saying?_x000D_
Luis_x000D_
Exactly, exactly. Completely. Actually, the pandemic was a good thing for us in IT, because a lot of companies are recruiting remotely, so you don't even need to go to the United States to work for that company, or other similar situations."
Machine Learning Zoomcamp;2022;Will there be any guidance for interview preparation after the capstone project?;If you need it, we can organize a session for that. Why not? I think we can do something like that. We have some materials. We can probably see how to best organize that for the interview. But Google is your friend – I'm sure you can find a lot of information on Google.
Machine Learning Zoomcamp;2022;GCP is more recommended, but Iâ€™m saving it for data engineering bootcamp.;Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
Machine Learning Zoomcamp;2022;Can I get the certificate if I didn't send the midterm project?;"You can. Remember that we have three projects and you need to complete two of them to get a certificate. This means that if you miss this one – if you didn't finish this one – you have to do Capstone 1 and Capstone 2 projects. If you pass them, you will get a certificate. Also, let's say you just joined the course now – you can still catch up._x000D_
 You can watch the videos and by the time we have the Capstone 1 project, you can already start doing it. Then we will have a bit of a break (for Christmas, New Year, and so on) and if you don't have any plans for that, you can keep on watching lectures, and then also work on your second project. Then you'll be fine."
Machine Learning Zoomcamp;2022;Are there alternatives for cross-validation and hyperparameter tuning in deep learning?;In deep learning, if you saw the lectures already, you might have noticed that we don't use cross-validation there, simply because it's too time consuming. We set aside a validation dataset and we use this to guide us – to select the best parameter. That's the usual approach I follow, because with proper cross-validation, with creating three folds, it’s simply too time-consuming. That's why I follow a simpler approach. I guess that is the alternative.
Data Engineering Zoomcamp;2023;What are the advantages and disadvantages of BigQuery and AWS Athena?;BigQuery is a data warehouse and it's optimized. It's usually faster. AWS Athena is a data lake. You will see the difference in week 2 – there is a video that explains what a data lake is. Then in week 3, you will see what the data warehouse is. But AWS Athena is more like a data lake. You can still run all these queries, but maybe they will be slower and I also think they will be cheaper. For analytical queries that – for queries where you want to get results quickly, I usually use BigQuery. Again, like these are two different clouds, two different technologies. The counterpart of BigQuery would be Redshift. But I think Redshift is slower than BigQuery as well.
Machine Learning Zoomcamp;2022;How and where can we start freelancing in machine learning after the courses. What are your recommendations after courses?;"When I was freelancing, it was through a website that’s similar to Upwork. You can try that. Or Fiver or something like this. But, depending on where you live, you can just use LinkedIn for that and if somebody wants to invite you for an interview, you can ask them, “Hey, do you consider freelancers?” Some of the recruiters who reach out to you will say, “Yes, we do consider freelancers.” And then you can just start doing this. I don't think I can give you a better recommendation, because I'm not a freelancer myself. _x000D_
Maybe what you can do is go to DataTalks.Club site’s podcasts, where we have two podcast episodes about freelancing. The first one is Freelancing and Consulting with Data Engineering and then the other one is Freelancing in Machine Learning. In both cases, the guests talk about finding your first client, how they started freelancing, what they do. They know more about this than me."
Machine Learning Zoomcamp;2022;Do you have experience with ClickHouse?;I actually do not.
Machine Learning Zoomcamp;2022;Is there any penalty for joining late?;There is no penalty for joining late. You can join at any point of time. You can actually take the course at your own pace. Nobody will penalize you for that. So take your time. Again, I will refer to this list of frequently asked questions. Please check it out.
Machine Learning Zoomcamp;2022;What does CI mean?;"Alexey_x000D_
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
Machine Learning Zoomcamp;2022;Do you have any courses except Zoomcamp?;I don't know what that means. We have three courses, which are all Zoomcamps. You can just find the links in the course project. So yes and no, I guess?
Machine Learning Zoomcamp;2022;How to know what metric is useful with use cases?;"For each use case, indeed, you need to think about what kind of business problem you’re solving and then based on that, come up with a metric. In most cases, precision and recall and F1 score are useful. Accuracy – most of them are good. Maybe what you can do is go to Kaggle where there are a lot of competitions. You can go through these competitions and see what kind of evaluation metric they use. _x000D_
First you can try to understand what the problem is about and then you can see what kind of evaluation metric they use for this competition. This way, over time, you will build your intuition regarding what kind of metric to use in which situation? It's not always great. I think Kaggle has some limitations, at least it used to have them – it has a very limited set of metrics. Sometimes, competition organizers would use a metric that didn't make much sense. But it doesn't happen very often. I think right now, the platform supports very customized metrics for each use case. So check out Kaggle."
Machine Learning Zoomcamp;2022;The metrics that we have seen could be used for multi-class?;"Yes or no. Accuracy can be used for multi-class classification – precision, recall and ROC curve, they're all for binary classification. But it's still possible to extend them for multi-class. First of all, there is a thing called one versus all classification. In this case, let's say we have three classes. [Image for reference] We have the green, blue and red classes. _x000D_
First, we actually see this problem as three binary classification problems and we train a separate model for each. For example, we can train a model for distinguishing green from the rest – maybe that will be our first logistic regression model. Then we can train a model for distinguishing the blue squares from the rest. And then we can have a third model for distinguishing the red crosses from the rest. This is called the “one vs all” approach. When we do this, we will end up with three binary models, and then we can compute precision, recall, F1 score, ROC, for each of these models separately and then we can merge them. We can take an average and see what the precision and recall are for this average. So that's one of the approaches. _x000D_
There are other approaches you can take. Let's say, ROC curve SciKit Learn, there is a way to extend it. [Image for reference] You see, for this roc_auc_score, they can actually do this for multi-class cases. There are different ways of how you can take average between these three classifiers. These micro/macro weighted samples – you can just read this description, this recommendation and see what the different ways of doing this are. I think this is a common approach. This is the one I just described – you compute them independently and then you kind of take the average."
Machine Learning Zoomcamp;2022;Maybe there are videos or material on the channel about graph algorithms or case studies with network analysis?;Yes, I think so. There was a talk about graph analytics. We had two, actually. Getting Started with Network Analytics in Python from Eric and then Modeling the Human Brain by Jessie. So these two. It’s funny that YouTube found it even though it actually doesn't mention the word “graph” in the title, nor in the description. But Jessie talks a lot about graphs. It's actually her research area. I think she graduated already. And she has a PhD now. She was talking about her PhD project. We also had a Book of the Week about graphs. Practitioners Guide to Graph Data. Via this link you can see all the questions and answers that we asked. You can check this book out. This book is more about graph databases, rather than graph learning.
Data Engineering Zoomcamp;2023;I joined the course late. I'm still struggling with Docker installation on Windows 10.;Oh, you did not join the course late (Week 1). Late would be like week 6. You're not the first one who says that they’re struggling with Docker. Windows and Mac OS for Docker are often problematic. Linux is the easiest way. If you’re still struggling with it, go with a GCP VM.
Machine Learning Zoomcamp;2022;What are some other EDA/DA could we do on the housing dataset?;This is such a broad question, I don't even know how to answer that. You need to come up with a question and then try to answer this question with data. So only your imagination is the limit here. Right now, maybe my imagination about this is not very good [chuckles]. I cannot just go ahead and generate a lot of ideas of what you can do. But maybe just open this dataset, take a look at this, stare at it for a couple of minutes, and then you will get a question. Then try to answer this question with data.
Machine Learning Zoomcamp;2022;What should my approach be if I'm juggling between both data engineering and machine learning Zoomcamps? Also will there be another round of DE peer-project review?;So for the first part “What should be my approach if I'm juggling between both?” Well, I don't know how much time you have. If you don't have a lot of time, I would suggest focusing on one and then come back to the other one. Actually, I need to make an announcement. As the course team from the Data Engineering Zoomcamp, we actually discussed it, and I'm happy to announce that there will be another iteration of Data Engineering Zoomcamp. Maybe what you can do now is just focus on ML Zoomcamp, and then come back to Data Engineering Zoomcamp when we launch it in January. There will be another announcement, but now you can know it. So, yes, we'll have another iteration in January.
Machine Learning Zoomcamp;2022;Please briefly explain what to expect with the recommender system coming up in Project of the Week this week? How does it work? I want to follow along and observe.;"I am glad you asked. Project of the Week has nothing to do with the course to the extent that it's just different initiatives that are run in our community. We have a course and we also have Project of the Week. We also have webinars, we also have podcasts. They are all different activities. For the Project of the Week, this week, we'll have a project about recommender systems. The idea there is that every day, you get a set of tasks. On Wednesday, you get this set of tasks. You need to come up with an idea. You need to find a dataset for this idea, and you need to share your progress. Then on day two, you will need to go through the suggested materials and also find some materials on your own and do this thing – learn about the basics, do exploratory data analysis, understand this data that you found and share your progress. As you can see on the project page, every day, you get a set of tasks and you need to do them. So it's not like we're telling you exactly what to do like in the Zoomcamp (in the videos we tell you that “This is the exact sequence of actions you need to execute and this is the result you get.”) With the Project of the Week, it's more independent. We just give you four bullet points and then it's up to you to actually do these things. And then, of course, you share your progress. The idea behind this project is that, at the end, you have a project that you can add to your portfolio. This is not a project that you just took from the tutorial and followed along, but this is a project that you did with some guidance. These are two very different things. That's the idea behind Project of the Week. For the project that we'll have as a part of the course, the idea is somewhat similar, except you will get less guidance – you will have more independence, let's say. The guidance we have for the project in the course is the form of a bunch of criteria that you need to satisfy. Then it's up to you to come up with a plan. But here, for Project the Week, we have a suggested plan that you can follow and hopefully, at the end, come up with a result. Keep in mind that for Project of the Week, some things might not go according to plan and that’s fine. This is just a suggested plan. We don't know if it will actually work exactly like we outlined day by day. It's more just to give you some guidance, but it’s up to you how exactly to approach that. _x000D_
There is a comment that says “I did not know about this.” Yes, we have a channel in our slack. It's called #project-of-the-week and it starts this week Wednesday. You'll notice that there's the digit one in the name “2022-10-19-recommenders-1.md”. We'll also have a follow-up project about recommender systems. If you want to take part in this (by “take part” I mean, propose your own topic) for example, some of you were asking about time series, some of you were asking about NLP, some of you were asking about other things in the comments in live chat. Daniel mentioned audio – If you want to learn any of these libraries, we can do a Project of the Week to learn these libraries or methods or approaches or whatever."
Machine Learning Zoomcamp;2022;If we have a web deployment (for example, AWS Lambda) do we still have to create a Docker container? Or is that a specific task that we have to show we can do?;"You saw that Docker is one of the criteria for the project. So if you don't create a Docker file, you will not get the points for that criterion. Let's say, if you deploy to PythonAnywhere – I think for PythonAnywhere, you don't need to create a Docker file. Maybe you do… I don't remember. But, indeed, there are environments where you don't need to create a Docker container. You can still do that – you will just not get points for the containerization part. _x000D_
I think it's quite important to know how to Dockerize your image, even if for your specific project, you don't need that. We want to make sure you also know how to do this. That's why it's part of the evaluation criteria. Knowing Docker is quite important, I think, in today's world. If you want to work as a data scientist, or an ML engineer or any kind of data engineer, you have to know about containerization. So that's why. _x000D_
Also, for Lambda, there are different options. Yes, you don't necessarily need Docker. But you can also do it with Docker. There is an option where you serve a container through Lambda – it's possible. If you want to use AWS Lambda, I would actually suggest following this approach."
Machine Learning Zoomcamp;2022;With evaluating our model in validation, what to do if our validation model is high accuracy and validation on test is very low?;When you have a very good score on validation, but a very low score on the test, it means your model became lucky. Remember, we had this explanation at the very first module – we had a lesson about model selection. Sometimes, the model can be lucky and it doesn't necessarily translate well to the test. It happens. What can also happen is that sometimes, validation and test datasets are different. For example, if you speed split your dataset by time – so you have a dataset for one year, and then for training, you use everything from January till September, then validation is October, November, and test is December. So you evaluate your model in validation and October, November looks great – but then we know that in December, it's Christmas time and many models that you trained during the normal months are different. For them, this December could be a surprise. So maybe you have something like that in the test data. It's normal. For these cases, you just need to think about how exactly you can build a validation dataset and training dataset in such a way that they are similar. For example, if you have a lot of data, then maybe you can use the previous year for validation or for testing. Something like this. It's all problem-specific. Usually it's an example of overfitting.
Machine Learning Zoomcamp;2022;What is a good target for accuracy in data modeling?;There is no good target. It's all case dependent. In some cases, if you have 90% accuracy, this is very suspicious – you probably should be worried and check your data for issues. Sometimes, you can have even 50% accuracy – in some cases it’s good. It also depends on what the skewness of the data is, when you have a lot of examples of one class, but very few for another class – like a class imbalance. It also depends on that. As you remember, “accuracy” is actually a misleading metric. In some cases, you should look at things like precision and recall – and you shouldn't consider looking at accuracy at all.
Machine Learning Zoomcamp;2022;What is your favorite project from past 2021 Zoomcamp?;I don't have a “favorite” project, but please check out the demos from the students of the previous cohort. And please check the midterm project link that I shared. You can also find it in the midterm page of the course. And maybe just come up with your own favorite. You can also share it in Slack, actually. So maybe you can see, “Okay, I went through all these projects. I really like this one,” and you can just share it with others.
Data Engineering Zoomcamp;2023;I love the testimonials of those who completed the course. If you can get graduates on the videos to speak and share their experience. I think that will be very motivational.;Yeah, good idea. Thank you.
Machine Learning Zoomcamp;2022;Somewhere, while talking about regularization, you mentioned a mathematical book, the name of which starts with â€œelementaryâ€ or â€œelements.â€;It's “Elements of Statistical Learning.” This is the book. It's actually free. If you click on the link, you can get the book. It's legally free – it's published by the authors for free. Yeah, it's a great book. For example, we talked about the normal equation. It’s called “least squares”. Least squares is what we call a “normal equation”. They start with that. I will not spend time doing this now. But check out this book. It's quite mathematical, but you might like it.
Data Engineering Zoomcamp;2023;How many finishers do you expect at the end of this Zoomcamp?;Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
Data Engineering Zoomcamp;2023;I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?;"Alexey_x000D_
It’s up to you. If you really want to get a certificate, then you can maybe put more time into watching the videos. If you don't care about the certificate, just take the course at your own pace. When you finish it, you finish it. There is no right or wrong answer. Just do it the way you feel doing it and focus on learning. With the certificate, there is actually a second chance to get it. For example, if you start taking the course now and you're not on time with the first project, it's not a big deal because there is another project attempt and you can use that. Before that comes around, you probably will finish watching all the videos."
Machine Learning Zoomcamp;2022;How do you know if you should choose a data engineer or a data scientist as your career path? What if you're confused between the two?;"I don’t know… throw a coin and the coin will tell you what to do if you don't know. [chuckles] And then if the coin tells you that you should be a data engineer and you feel like “Umm… maybe not.” Then go with the other option. I don't have a better suggestion here. I can tell you to follow your passion and follow your heart… But yeah. [chuckles] I don't know. You’ll eventually need to make these decisions yourself. Just do a project in data engineering, do a project in data science, and then ask yourself what you enjoyed doing more. _x000D_
Also talk to people – talk to a few data engineers, people who already work as data engineers, people who already work as data scientists, and ask them what they don't like about their job. And then ask yourself, “Would you enjoy doing things that they dislike? Or would you be okay doing these things as well?” For example, what I don't like in the work of a data scientist is all this parameter tuning and parameter optimization, trying different features – all this stuff I find boring, but some people really love it. If some people tell you, “Eh, I don't like this part,” and you think, “Okay, it's actually not bad for me,” then maybe this is the right path for you. _x000D_
In the end, if you join as a data scientist, it doesn't mean that the door for a data engineer is closed for you. As a data scientist, you will get to do a lot of data engineering as well. I, as a data scientist, also do data engineering. I also do machine learning engineering. The same is not always true for data engineers. If you get hired as a data engineer, maybe you will get to do data science, maybe not. It depends on the team. So then, what you can explore is the so-called full-stack data scientist. But again, titles sometimes don't mean much. Don't try to force yourself into one of the predefined boxes. Sometimes you can work ‘officially’ as a software engineer and do both data science and data engineering and enjoy this work."
Data Engineering Zoomcamp;2023;During which weeks will we do the project?;"Alexey_x000D_
I will show you. You go to our 2023 cohort, and you look at the deadline calendar. It will tell you during which weeks you do the project."
Machine Learning Zoomcamp;2022;I missed the last two homework assignments. Can I still catch up? Can I still get the certificate?;Yes. I will remind you that homework assignments are not required to get the certificate. You need to pass two projects out of three. If you do that, you will get a certificate. The important part for the project is to know how to deploy a model. So that’s what you will have to catch up on if you want to pass your project. If you don't catch up right now, you can spend the time we allocated for the midterm project towards doing this. Then you can work on the capstone 1 project and capstone 2 project and then submit these two projects and then pass the course and then get the certificate. You don't have to do everything in this course. For example, even if you don't learn TensorFlow and if you don't learn Kubernetes, you can still do two projects and then pass the course. But what you need to learn (what you have to learn) is the deployment sections – without them, you will not be able to finish the course. So chapter 5, definitely. 7 is also helpful – and 7 builds on 5. In my opinion, 7 is actually easier. Once you know the foundation from 5, taking 7 and then applying this is actually easier. You will need to do less work, if you use Bento compared to Flask. You can use Bento for your project, and I do suggest using Bento for the project. Then we will also have a chapter about serverless deployment and then Kubernetes. It's up to you whether you want to take them or not. I do recommend taking them, but use your own judgment to understand if you have the energy to actually do this. I know some people sign up for this course just to do the Kubernetes module, and there's nothing wrong with that. But if module five was difficult for you, maybe Kubernetes would be even more difficult. So maybe you can focus on something a bit simpler. Or… there is no harm in trying, right? I would still suggest trying Kubernetes. You will have quite some time for the Kubernetes homework. According to our schedule, it will actually be scheduled over the holidays, so you will have quite a lot of time to finish it. Let me check. I think for Kubernetes it was last year that it took a bit more time. We actually left the submission form open for quite some time, so the people who needed to catch up did this over the holidays. Don't worry, you will be able to catch up. But the important thing is to focus on the deployment modules. It will help you with the project.
Machine Learning Zoomcamp;2022;Is pandas useful in dealing with a lot of data or do you use other faster libraries like datatable?;"It depends, really. I use Pandas for pretty large data sets. But, it depends on how large your machine is. With pandas, you can always take your data and chunk it into multiple pieces. There is the pandas read_csv iterator – you can iterate over the rows of your CSV file or whatever. Yeah, just use pandas. Usually, in practice, more often than not, we use a database. Yes. For example, for the setup we have at OLX, we do have a database – I would call it a data lake, because at the end it's just a bunch of parquet files in S3. _x000D_
If what I say makes no sense for you, maybe check out our data engineering course, eventually (after this one is over) where we explain what a data lake is, what a data warehouse is, and things like this. _x000D_
At OLX, we have some sort of database (some sort of storage) and we can run some queries on top of this storage. Then, the result is often saved as a CSV file or as a parquet file. Then what I do often, for example, is just fetch this file, download it on the machine where my jobs are executed, and read it with pandas, and then apply the function that I need (like a model that predicts to this pandas data frame) and then save the result somewhere. _x000D_
It's also possible to do it with a database. Actually there is a very nice library that I will show you right now. We will have a workshop called Effective Machine Learning Inside Your Database (September 21st 2022) There are tools that allow you to run machine learning inside your database. If you're watching/reading this past the 21st of September 2022, there will be a recording of this workshop on our channel. This shows how to actually apply machine learning in a database without leaving the database at all. So there are many, many options that you can do here. Again, there is no right or wrong answer – it depends on your case – but pandas is totally fine. _x000D_
There's also the common one – Spark. Spark is also used quite often. Again, maybe after this course, you can check our data engineering course. There, we show how to use Spark in one of the videos there. I am the instructor for the Spark module. In one of the videos there, I show how to apply a machine learning model to a Spark data frame. But maybe not right now – after the course – because right now you already have a lot on your plate, so don't get distracted. It's important to keep focus. If you intend to go through the course one-by-one, maybe it's better to focus on the course. Make a “to watch” list to come back to after finishing the course."
Machine Learning Zoomcamp;2022;What is the difference between Streamlit and Gradio?;For those who don't know, Streamlit and Gradio are frameworks for creating user interfaces for machine learning. The main difference between them is that Gradio is designed specifically for machine learning, while Streamlit is a general purpose user interface library in Python. You can think of them as Flask and Bento ML. With Flask, you can deploy a machine learning model, but you can also deploy much, much more. While in Bento ML you can only deploy machine learning. Here, it's the same – with Streamlit, you can build a machine learning interface for a machine learning model, but you can also build pretty much anything. With Gradio, it's only for machine learning. At least this is my understanding. I have used Streamlit. I have not extensively used Gradio.
Data Engineering Zoomcamp;2023;Can you show how to run DBT from the subfolder of the repo?;"Victoria_x000D_
I think this one was solved this already. [inaudible] I know that was in one of those threads. But yes, this is this is my account. I am here under settings on the projects. You're going to see that I actually have the same account as you'll have. It just the videos are one year old and DBT changed a lot since then. You're going to see that when I go through one of my projects, I have this project SAP directory, where you can write the directory. If you don't write anything, then your project that you’re going to develop, is going to start in here. The same when you're writing, it’s going to start always in the root. _x000D_
This is my repository – it has nothing but week 4. I have this other one here, If instead you have something like the full repository of the data engineering Zoomcamp, then I can put the project’s directory here and my DBT project will start after all of this. That is where it's going to go and look for my code. If you're using the terminal, then it's as simple as locating yourself where your project yml is. This is the same for here. _x000D_
Here is my project yml, under this directory. You locate yourself with either C/D, if you're using Linux or Mac, or dir, if you're using Windows and you execute DBT from there. It's going to go and look through their project yml and then execute from there."
Machine Learning Zoomcamp;2022;What is an excellent midterm, capstone, or any ML project? Modeling lots of data, how features engineering is done, how it is presented, or the accuracy?;"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. _x000D_
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
Data Engineering Zoomcamp;2023;Is there a checklist for the final project to make sure everything is applied?;Yes, there is. If you go to the project folder you will see the criteria. This criteria is like your checklist.
Machine Learning Zoomcamp;2022;How many portfolio projects do we need to have before we can say, â€œNow I can apply for jobs.â€?;Zero. You can start applying for jobs right now. You don't have to have a portfolio for that. Maybe you can use your Zoomcamp projects as a portfolio. That's a very good idea. Maybe one-two should be good, but you can find a job without any projects in your portfolio at all. It also can happen that you don't find a job even with 10 projects in your portfolio. There are different situations – everyone has different backgrounds, everyone has different problems. So I don't think there’s a “one size fits all” answer to that. I'd say one or two is helpful to have.
Machine Learning Zoomcamp;2022;I am a physician and would like to work in data science. What is the best path for me? What would you advise me?;I always confuse the two – is a physician a doctor or somebody who does physics? Because I guess the answer would be completely different. Either way, coming from physics (from STEM) you probably already know how to program. But as a physician, as a doctor, you might not have the same skills, so this is where you need to start. Maybe learn the basics of Python. I think this advice actually applies to anyone who wants to start in data science – get comfortable with the command: things like navigating the file system like CD, LS, CP, MV and commands like that. Get familiar with using Git as well – configuring GitHub, that's pretty useful, too. Pushing code to GitHub, pulling code from GitHub – I think this is a must. After that, you can already take the course (this one). Maybe the chapters about deployment will be too difficult for you, but the rest of the chapters, where it's a more general introduction to machine learning, could be quite good. So just start with that and if it's difficult, we can think about that. But more importantly, you need to improve your Python. But since you're asking it here and it's already week nine, I assume your Python is fine. So just stick to the course and you'll be fine.
Data Engineering Zoomcamp;2023;Pulling raw data from GCS into BigQuery gives data type errors. How do I handle this? Transform data types before loading data to GCS?;"Alexey_x000D_
I think this is related to the question I answered a few questions ago about this wrong column type. In week 5, we actually used Spark to set the schema. In this case, when we process CSV files like that with schema in Spark and then we use it in BigQuery, everything should be fine. Here, the main idea is not using Spark but specifying the schema. If you specify the schema (if you force the schema) then all the files you create will have the same schema and all the columns will have the same format. Then reading this data in BigQuery will be fine because it will not be confused that in one file, one column has one type, in another file, the same column has a different type. Because of that, there are data type errors. But if we force the schema, all the columns will have the same type and it should be fine."
Machine Learning Zoomcamp;2022;What was the y_train variable for the week 3 homework? I got to question 4 to use logistic regression and got an error that (x_train, y_train) were not the same size.;It was this above average thing. That was the y_train variable.
Machine Learning Zoomcamp;2022;However can a beginner contributor add value to a BentoML project? The project is exciting.;"Tim _x000D_
Awesome question. Honestly, just in the last few days, you guys have all added a lot of value. I know you could feel frustrated sometimes when you find bugs – just little things. I think a few people found that “—reload” wasn't working on Windows systems. Just finding things like that and reporting them is actually a huge help to us. That way, we can find all the different scenarios where it doesn't work. That makes the library so much better. So I think filing bug reports helps and if you want to contribute to the codebase itself, I think there are a couple of different easy ways to get started. Certainly, if you've built a project with BentoML, we're always accepting more projects in our example folder. But there's also the tag in GitHub “good-first-issue” as well. Those are all issues that, if you comment on them and ask to contribute to build those issues, you can. A good first time issue is the good thing for your first time contributing. Then “help- wanted” is just a tag anywhere we see a need, we tag it with that. Although it may be a more advanced feature."
Machine Learning Zoomcamp;2022;Since waitress-serve works on both Windows and Linux, isn't it a better idea to use it instead of Gunicorn?;Yeah. If it works for you, use it. I don't mind.
Data Engineering Zoomcamp;2023;Can an Oracle DBA with 10 years of experience transform into a data engineering career?;Yes
Machine Learning Zoomcamp;2022;Should we research papers to stay up to date with recent advances in AI? Most are focused on deep learning, transformers, diffusion models, DALL-E, etc. How?;"Depends. [chuckles] Again, sorry for answering this way. So, what is your end goal? Do you want to be a researcher or do you want to work as a machine learning engineer? Depending on what you answer, the answer to this question will be different. If your answer is “machine learning engineer,” then the answer is you don't stay up to date with recent advances in AI. You don't have to stay up to date, let's say. If you want, just go on Twitter and see what's trending and then read these articles. Or Reddit, r/machinelearning, for example. But if it's too much for you, then just don't stay up to date. Nobody is forcing you to do this. _x000D_
To know the basics well enough and for actual practical applications of machine learning, I recommend attending conferences – something like PyData, for example. I think PyData has chapters everywhere, so you can find a PyData conference in the area where you live, and then just attend it. Or watch PyData conferences online. And it's not just PyData. You can go to DataTalks.Club webinars, workshops, and also learn from them. Of course, these are not the only channels that you can find. YouTube is full of information that you can use for getting experience from people from the industry. This is how you stay up to date for machine learning engineering."
Machine Learning Zoomcamp;2022;Can you revisit the ETA explanation? I do not understand what it means yet.;I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
Machine Learning Zoomcamp;2022;Can you talk about linear regression and regularization?;Yes, I can. There is actually an entire module about that. You probably mean something specific that you didn't understand. Maybe ask about that in Slack.
Machine Learning Zoomcamp;2022;Any tips for networking with people related to ML/data science? How to create an eye-catching profile for other people and companies?;"I think this is actually two questions. How do you network with people? I think we even have a few podcast episodes about that. I like the one with Juan Pablo, From Math Teacher to Analytics Engineer, where he talks about networking – how to meet people at meetups and so on. So maybe check out this podcast. What you can also Google is a set of ice breaking questions. When you approach people at meetups, you can just ask them these ice breaking questions. I'm not sure if I'm very good at this to give you any good advice. But yeah, please check our podcasts. The one with Juan Pablo is the first one that comes to mind. Maybe we should have a special podcast episode that is for networking. _x000D_
Then how to create an eye-catching profile for other people and companies? We also had an episode about that. There are quite a few. I like this one: Standing Out as a Data Scientist with Luke. By the way, we will have another follow up episode with Luke in January, so check it out. Luke says (if I can give you a TLDR for this) you need to pick a niche and do a few projects in this niche. You also need to understand what companies in this niche you are really looking for. This will make you attractive to these companies in this niche."
Data Engineering Zoomcamp;2023;Why does the YouTube page cover a blue duck? Why do you like ducks so much?;"Kalise_x000D_
Our duck is named Marvin. Jeff, do you want to say why he's blue or why we like ducks? [chuckles]_x000D_
Jeff_x000D_
Yeah. “Rubber ducking” is a super common thing when you get stuck. You should do this before you put the question on Slack – you should talk to the rubber duck. Oh! Michael’s got his duck. I got mine. I got Marvin here in the background, let’s bring Marvin over. So just instead of talking to anyone, you can just talk to Marvin – talk to your duck, explain your problem, and as you're explaining it, oftentimes you realize like, “Oh! It doesn't work, because I didn't do that thing that's really obvious!” or whatever. So rubber ducking is a common troubleshooting step for people as they are programming. That's where we got Marvin, our blue duck._x000D_
Alexey_x000D_
And the reason why it's blue? It's just a nice color?_x000D_
Jeff_x000D_
It is a nice color. I feel like I should know the answer to that, but I don't know if there is a reason. [chuckles]_x000D_
Alexey_x000D_
Because why not blue, right?_x000D_
Jeff_x000D_
Why not blue? Yeah. [chuckles] Because then if you use DALL-E or some kind of AI generation to get blue docs, you can get all kinds of weird images._x000D_
Alexey_x000D_
Yeah, I think that's what you do for your preview images, right? For your blog. It's all Stable Diffusion or DALL-E or something. _x000D_
Jeff_x000D_
Yeah, mostly Stable Diffusion."
Machine Learning Zoomcamp;2022;How should we select the best model? How should I choose the parameters for hyperparameter tuning?;You use validation for that and then you see what works best on validation. Then you use that test dataset to make sure you do not overfit. That's how we do it.
Machine Learning Zoomcamp;2022;How do you handle categorical variables with several options in the dictionary vectorizer, since it consumes a lot of memory when training it?;"One thing I would suggest is to check out Office Hours from last year (2021) if you go to the midterm project, there are these Office Hours. I think it's week nine, where I show how to use the count vectorizer for doing one hot encoding. With count vectorizer, you can apply some filtering. For example, you can say that “I'm only interested in categories that appear at least in 100 observations.” Then, instead of looking at all possible values, you look at only the frequent ones. So go through this document. _x000D_
I think there are a lot of different ways you can add filtering here. It could be minimal frequency, it could be something else. It's actually kind of misusing count vectorizer a bit, because count vectorizer is supposed to be used with text features, and not with usual categorical features. But you kind of hack it, in a way, if you say, “Okay, turn these categories into text, and then you train the count vectorizer on them.” So yeah, go through this thing. You can also find the video from that, where I do it live. That's very useful. Again, if you have a huge load of different categories – different values – then the count vectorizer will still require some memory to train. _x000D_
There is an alternative. It's called a hashing vectorizer. This one. It does not require training. So you can say, “Okay, I only want to have only like 10,000 features and not more than that.” And it will actually take a word, and it will compute a hash of this word, or value by category, and it will randomly put in one of the columns of this vectorizer. Not randomly – it will compute hash and then put in one of them. So it will be deterministic, of course. Sorry, I didn't choose the right words. _x000D_
This is a good way to save memory if you have a lot of categories. So, hashing vectorizer – it works in the same way as count vectorizer, except you don't need to do fit. Actually, you can actually just go through this and read it yourself. It explains everything that you need to know when you compare this one versus count vectorizer."
Machine Learning Zoomcamp;2022;How do we choose numbers of folds (for k-fold cross validation)?;"I think some of you asked that question in Slack already. Let me repeat what I answered to that question.  Imagine that you have a very big dataset. For this big dataset, training a model takes a lot of time. You don't want your k to be very large. Because if you have a very large K, it means you will spend a lot, a lot, a lot of time just training this model and then computing this course, and then taking averages. And then because your dataset is big, it will probably not be very different across different folds. Usually, this is the case when your dataset is large. Therefore, in large datasets, it makes sense to use k=2 for k-fold cross validation – so you just do it twice. _x000D_
Often, even sticking to the usual train validation split, like 80/20 or 75/25 makes even more sense, because you can just do it once without training it multiple times, because A) you save time and B) it's very likely that you will not see a very large difference across different folds when your dataset is large. You can experiment with this and see that it's actually true and that this course that you get on different folds are quite different. _x000D_
Then you can just do the usual train, validation set and use that because it will speed up your experiments. For medium-sized datasets, having k=three is fine. Here, you should think in terms of how much time you are willing to spend on doing your cross validation. You don't want to spend a lot of time here. So maybe k=3 should be sufficient. As your dataset size goes down, then maybe you can increase k. For example, for smaller datasets, k=five, or even k=10 in some cases, makes a lot of sense. That's how I typically choose. The smaller the dataset, the larger the k. And for big datasets, I usually don't use k-fold cross validation at all."
Data Engineering Zoomcamp;2023;I tried using parquet files from the source, but these contain a different number of rows than the backup CSV and the result is in the other answers.;Yeah, I will not really check that. You are kind of expected to use the backup CSV. I don't know why the number of rows is different. Please use the backup CS
Data Engineering Zoomcamp;2023;Can I submit the final project past the due date?;No, you cannot.
Machine Learning Zoomcamp;2022;Is it worth doing machine learning in R in 2022 or is it better to learn Python?;Well, I think it's better to learn Python simply because it's more popular. There are still some problems where R is more suited to solve, like time series, some statistical stuff, but in general, Python is pretty versatile. When it comes to deployment, it's much easier. I would focus on Python and then if your work requires you to learn R, then do that. If nobody specifically asks you to learn R, then go with Python. It's not like if you don't know R, people will not hire you. They will still hire you – you will just need to learn R at work. It's not a bad language, it’s a good language. If you need to learn R because the company uses it, just learn it.
Data Engineering Zoomcamp;2023;Many did not know about one point per post for Learning in Public and many lost 12 points since two homework assignments were scored together. Can you please have ways to gain those lost points?;"Alexey_x000D_
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. _x000D_
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. _x000D_
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?_x000D_
Michael_x000D_
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
Data Engineering Zoomcamp;2023;Will there eventually be instructions on how to deploy Prefect with Terraform?;I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.
Data Engineering Zoomcamp;2023;What is the expected hours per week commitment required to complete this course satisfactorily for a beginner and practitioner data engineer?;"Michael  _x000D_
I think that would heavily depend on how much experience you have with Python, SQL, GCP. If I remember correctly, I think I probably did about six hours a week. Also, if you want to have the material really sink in, you'll go through it two or three times. So it really depends on the student and the week. Some will require more time. But I think if you manage your time well, the time commitment can be pretty small. _x000D_
Luis_x000D_
Yeah. Like Michael said, I think it also depends a little bit on the issue of the team that you're talking with. For example, on SQL hours, I am already completely advanced, so it didn't take much time with that. It was really fast. But for Docker, I was completely a beginner, so it was harder. Besides that, there's the leaderboard, and if you want to be on the top of the leaderboard because it matters for you, then it will probably take more than that six hours a week. But I think five to six hours per week is expected._x000D_
Alexey  _x000D_
I saw a similar question asked on Reddit and many people said that they’re putting in up to 10 hours, especially in the first week. The Docker week was the most challenging. Then the Airflow week, again, the problem there was mostly setting up. This is where people spent a lot of time. For example, BigQuery week was relatively easy because it's a managed service – you just go to Google Cloud Platform, and you don't need to set up anything locally. It's easier._x000D_
Do you guys remember what the most difficult weeks for you were? Was it the Docker and Airflow weeks or some others? _x000D_
Luis_x000D_
Oh, definitely. The first week was the most difficult. Definitely. Actually, I must admit that I didn't do your week of Kafka. Sorry. I was completely busy with a lot of work and skipped the streaming week. [chuckles] Yeah, it was too hard for me._x000D_
Alexey_x000D_
But it's actually a good point. Many people were asking me things like, “Hey, I don't want to study DBT. I'm interested in Spark.” Yeah, just go ahead and skip it in. We're not forcing you to study DBT. If all you're interested in is streaming, go ahead and watch the streaming lectures. They're already there. Then at the end, you just use the tech you want in your project and you don’t use the tech you don't want. You have this freedom. You have this flexibility. _x000D_
Luis_x000D_
I don't know if someone asked already, but one thing that is important is that the projects you don't need are all the things of all the weeks. Don't think “Oh, I have to have streaming. I have to have Prefect. I have to have…” No. You don't have to do everything. Just some topics._x000D_
Ankush_x000D_
Just remember, if you want to get on the leaderboard, you might need to do every week."
Data Engineering Zoomcamp;2023;How much hands-on expertise does a data engineer need in DevOps (DataOps)?;"Ankush_x000D_
Are you in a company which has a lot of DevOps (DataOps)? Then none – then you don't need much. But if you are working in a startup, I would say some DevOps knowledge (Terraform, Docker, how to fix Docker issues, how to go inside and see what's going on, logs) would be helpful, in general. Even if you are a backend developer or a software developer, this knowledge would be valuable in any case. If you're working in a very big company where there are thousands of developers with this kind of experience, then maybe you can rely on them more._x000D_
_x000D_
Alexey  _x000D_
I think of companies like Zalando, or for example, OLX where I work, there is already a data platform. If I need to schedule a SQL query, I don't need to do much. I just write the SQL query, commit it to Git and then it just works. I don't need to do any Docker, Terraform, or anything. I'm not a data engineer, but this is how it works."
Machine Learning Zoomcamp;2022;When you split the dataset into two-week content, how do we avoid having repeated data?;I don't think I understand the question. Is the question about what we do when we have duplicates in data and we split it and then there are duplicates? Some of the datasets in week one and then one split and then another one, or…? Oh, okay. Yeah, the way we do it is, we generate random numbers. I don't remember, to be honest, how exactly we do this. Let me quickly take a look. If we go to regression, notebook, and split – I think we shuffle data, not draw random numbers. Yeah, we take a range from zero to n exclusive. So n is not included and then we shuffle it. This way, we make sure that the same number does not end up in the different splits. I hope that answers your question.
Machine Learning Zoomcamp;2022;Do you know why your CI is failing?;"Tim_x000D_
I actually don't. [chuckles] I think there’s some really obscure version of Ubuntu, actually, that is not passing. We tried to regress it on multiple OSs and I think there's an obscure version of Ubuntu in there._x000D_
Alexey_x000D_
[chuckles] I will not go there. You mentioned that the best way to contribute is to just run it and see if it works. Knowing how many different environments students of this course have is already a good contribution because all of us have different versions of Windows, different versions of shells in this OS. With Windows life is always dangerous and full of surprises. [laughs]_x000D_
Tim_x000D_
Well, Mac M1 is kind of similar. But yes, Windows can show problems sometimes. I think the best way that I've found to have a consistent environment is just to create a Ubuntu instance in AWS and then just go from there. And then if my environment completely blows up, I can just stop the machine and then start a new one. [chuckles]"
Machine Learning Zoomcamp;2022;How does the linear regression algorithm know how to estimate the right weight for a particular feature, given that some features are more important than others?;"Yeah, that's an interesting question. This is something we do not cover in detail in this course. Again, maybe (if you want) you can refer to this Andrew Ng course for more theory. Here we don't cover a lot of theory – we focus more on practice. But from the theoretical point of view, linear regression is trying to optimize the so-called “loss function” and it's doing it by using some mathematics. A normal equation that we saw is one of the ways of finding the minimum for this loss function. Basically, the weights we get at the end are the best in terms of minimizing this loss function. The weights that we get, they make sure that this loss function is minimal. I don't know if it makes sense to you or not, but there is some mathematical foundation behind this. There is an explanation of why it happens this way. _x000D_
If you want to find out more about this, maybe you can check about a thing called “gradient descent,” which explains the loss function – we move down by tweaking our weights in such a way that, when we arrive at the end, the weights we have in our model, they are the best ones with respect to this particular loss function. I think we slightly cover loss functions minimization – I think maybe in neural networks. I don't remember. Here, we focus more on practical aspects. You kind of put some trust into the idea that the model gives you a good enough estimate, and then use it from a practical point of view. But if you want to learn more theory, then I think there is no better course than this one – Machine Learning by Andrew Ng. From my point of view, that was one of the best courses I took when it comes to theoretical machine learning."
Data Engineering Zoomcamp;2023;Can you please explain the second project? Does this mean that we'll get a second attempt or it's a second project entirely?;Yeah, you can think of these as two attempts. Not two projects, but two attempts. If for some reason, you cannot take part – you're busy, you have a vacation, something happened, you get sick – and you cannot deliver the first project in time, then you get another chance. This is why we have the second attempt. Or for example, if you fail your project, then you also get another chance. If you don't do the first project and you fail the second attempt, then unfortunately, there is no way to resubmit it. It will be two attempts, but one project.
Machine Learning Zoomcamp;2022;What teams do you interact with at your job as an ML engineer or data scientist? What does a team usually consist of? I mean, according to roles and responsibilities.;Instead of answering this question right now, I will refer to this article called Roles in a Data Team. I wrote it some time ago, but you're basically asking for this information. What the people in the teams are, how they work together and things like this. When it comes to the setup I have at OLX, it's a little bit too complex to explain in the remaining 10 minutes, so maybe I will not do it right now. But please do check out this article and if you have questions, I'll be happy to answer them.
Machine Learning Zoomcamp;2022;I want to apply to a computer vision job together with web pages/web services. Please give advice for achieving this.;Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
Data Engineering Zoomcamp;2023;Why is the Prefect register block used? As I can see the GCS block in Prefect before as well?;"Jeff_x000D_
This is getting at different servers. When we start out in the course, you're using the Prefect Orion server. It has a bunch of blocks ready to go with it – they’re built in and it knows about this. But then there are bunches of modules – we have over 40 different collections now on our integrations. There are all kinds of different things out there. If someone has a piece of software and they want it to integrate, they make a module for it, you can install it with pip, but we need to somehow let the server know that it has some blocks in it that could be used by Prefect. So that's what Prefect block register does, as the name of the module suggests. If you connect to a different server, like Prefect Cloud, for example, or a different workspace there, the server doesn't know that you have those blocks available. It doesn't have anything on that server. Those blocks live on that server, so you need to somehow let it know. That's what registering those blocks can do with the block types."
Data Engineering Zoomcamp;2023;Who will review homework?;We will, or rather a script will review it.
Machine Learning Zoomcamp;2022;What is the difference between CMD and ENTRYPOINT?;Okay, what I will do is I will copy this question and paste it into Google and see what it answers. I don't know, is this good enough? Or do you want to know more about that? I usually go with ENTRYPOINT. In lambda, we use CMD because ENTRYPOINT is already specified in the base Docker image. Then using CMD, you can kind of overwrite, but only partly. CMD is like a part of ENTRYPOINT. As far as I remember, that's the main difference between them. Please look it up. I don't remember too much about it.
Machine Learning Zoomcamp;2022;I want to be a full stack ML engineer. Do I need data engineering Zoomcamp or MLOps Zoomcamp after this?;"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. _x000D_
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. _x000D_
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. _x000D_
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
Machine Learning Zoomcamp;2022;I used a couple of code snippets from online. That's okay, right?;That is definitely okay. As a data scientist, I often Google. I end up on StackOverflow and I see a code snippet – I copy it and use it in my work. I see nothing wrong if you do the same. That's a usual thing. But if you copy the entire project, that's a different situation, right? [chuckles]
Data Engineering Zoomcamp;2023;When will the next courses of DTC be released?;"Alexey_x000D_
I don’t know what you mean by “the next courses”. We have three courses right now: Machine Learning Engineering course (ML Zoomcamp) which is in September. Then we have this one (DE Zoomcamp) in January. And the MLOps Zoomcamp starts in May. We plan to reiterate them every year, for now. These three courses take the entire year to run and it makes it very difficult to think about if we can add another course to the courses we already have – how to find the time and how to do this. Right now, I'm figuring this out –I am often the bottleneck when running these courses, so I'm thinking about how I can just step out of the course and then let it run by itself without me. Then maybe we can focus on other courses. When this is figured out, then maybe there will be new courses from DataTalks.Club. But for now, you can enjoy the courses we already have."
Data Engineering Zoomcamp;2023;Do you have any date for when the new Terraform videos will be available? I already finished week 1 a few days ago.;We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
Machine Learning Zoomcamp;2022;I have a question for you, Tim. If people have questions for you, what's the best way to reach out to you?;"Tim_x000D_
The best way to reach out to me is in the Bento ML Slack. Usually, you can join our Slack group and then you can just directly message me"
Machine Learning Zoomcamp;2022;What would be something that is unforgivable to a data scientist?;"Well, to me, you have to know how to set up a validation framework. You have to validate your data. If you asked me, “What is better – random forest or logistic regression?” I would say, “Okay, just do validation and check it.” If you don't know how to do this, I would suggest learning how to do this. Maybe “unforgivable” is a strong word. For example, if a candidate that I interview doesn't know about validation, then this candidate will probably be rejected. _x000D_
So I think this is one of the most important skills – knowing how to validate your model. Then once you know how to do this, you can answer any question by just testing it. If you have an environment where you can experiment – and you can experiment if you have a validation framework – then you can answer all the questions, like, “What if I increase the learning rate?” You just increase it and see what happens. Because it's all case-dependent, it's all data dependent. You need to know how to experiment and be comfortable with experimenting. I think this is the most important thing for a data scientist."
Machine Learning Zoomcamp;2022;In which cases should you apply logarithm to the target variable?;Maybe go check out module two, where we talked about that. I think during Office Hours, we also covered that. I don't remember, to be honest, what I answered to that, but usually, when you have a long tail that goes to plus infinity (to the right) this is when you want to apply a logarithmic transformation and usually when you don't have negative values in your original data  is when you use it.
Data Engineering Zoomcamp;2023;How does the peer review work?;"Alexey_x000D_
You finish your projects and then after that, you get to review three projects of your peers. There will be a set of criteria. It should be here in the project section, if you search for “Peer review criteria”. You will need to follow this criteria to evaluate. That's how it works, roughly."
Data Engineering Zoomcamp;2023;To the Prefect team, how does the deployment really happen? What is the difference between Prefect Orion and Prefect Cloud during deployment in terms of the local code?;"Jeff_x000D_
With deployment, it’s server again – back to the server. We have a server, when you want to do a pip install Prefect and Prefect Orion start – you have the server running locally. You communicate that with that server through the command line, and then information goes to the server. You can put the deployment information on the server. It's the same thing with Prefect Cloud, except it's hosted. It's our database that's getting that information. In both cases, there's a database backing things. Locally, it's a SQLite database by default. If you want to, you can use Postgres for bigger, higher use operations with multiple people. We have a Postgres database on Google Cloud (GCP) that is backing our stuff on the cloud, too. That's what we're using behind the scenes too. Those are just different servers in different places, as Kalise mentioned before, ours can do things like authorizations so only certain people have access to data, and you can share/collaborate on Prefect Cloud. Basically, the information lives on the server there and when it's time to go and run that, you can schedule it as you've seen from the homework and elsewhere in the course. The agent, somewhere, will be pulling and saying, “Hey, alright. Yeah, I got something ready to run here. There's something that's waiting. I'm going to run it.” And the agent will kick off a process either on your local computer or in some infrastructure you specify, like Docker, and it'll make sure that has the flow code available there. Question 4 on the homework – lots of questions there – we asked you to put the code on GitHub, because GitHub is super popular. You're gonna be able to collaborate with people on GitHub. The code is being found on a GitHub repo, and then the processes are being started in that infrastructure and then it all runs beautifully. You never have any errors and everything works."
Data Engineering Zoomcamp;2023;Any plans for next books or paid courses in DataTalks.Club?;"Alexey_x000D_
Yes. Well, maybe. Nothing concrete. If you have some ideas, again, please share them with us. I know that many people asked for a course in deep learning. I unfortunately don't know deep learning that well to make the course about that. But maybe there are some other areas that you have ideas about – share them with us. Again, for deep learning – I think maybe we will eventually do this. I just need to find people who know it a lot better than me and I will just help with the process. So nothing concrete, but if you have ideas – share them."
Machine Learning Zoomcamp;2022;Can we use BentoML with Colab?;"Tim _x000D_
I think, yes. We have a few examples on our website. I think with Colab, it becomes a little bit more difficult once you're hosting the Bento to access it, if it's running on a Google Colab server. But yeah, you can save it. What I would say is that you can save your model and then export your model to somewhere where you can then build your Bento there._x000D_
Alexey _x000D_
By default, Bento saves in a local file system (in your /home) so you need to have a way to save this model somewhere, let's say on the cloud, S3, whatever – somewhere externally. The workflow here is that you train your model in Colab, you save it, and then you deploy it somewhere. You don't deploy it on a Colab instance._x000D_
Tim_x000D_
Right. When you save your model, I think it'll save it to the Colab instance. Then you have to use BentoML Export to push it to an S3 bucket or something where you can pick it up and deploy your Bento._x000D_
Alexey _x000D_
And there are options for deploying. We already discussed this in the course, when we looked at Elastic Beanstalk. I assume it can work with Bento. I haven't checked. [Tim agrees] But then what we saw this week, we saw how to deploy it with ECS, and then we can also deploy to Kubernetes EKS, we can also deploy to lambda – there are a ton of other options. Everywhere where you can deploy a Docker container (Docker image) you can deploy it there. Right? [Tim agrees]"
Machine Learning Zoomcamp;2022;Why train models anymore? Isn't there a huge range of models available on Hugging Face. Do you know other model hubs online apart from Hugging Face?;I mean, if there is already a model in Hugging Face for your problem, that's good for you. But I guess that's only like 1% of problems that you solve in the industry. Data scientists wouldn't be working at companies if that was the case. [chuckles] We still need to get data and we still need to train these models. And then if you think about it, somebody actually put these models on Hugging Face, right? I suspect that it was done by data scientists. Right? [chuckles] I don't know any other model hub apart from that. Maybe Kaggle. I would say that’s also a good source for models – more for data, but in data, you have a dataset and you can also train a model there. There are many notebooks that show how to do this.
Machine Learning Zoomcamp;2022;Since you mentioned MLflow, how does BentoML integrate with MLflow? Because with MLflow, you do something similar â€“ you finish training your model and then at the end, you say, â€œMLflow.XGBoost.save_modelâ€. Then you save the model to the MLflow registry. With Bento, itâ€™s very similar, right? So how do these two work together?;"Tim_x000D_
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. _x000D_
Alexey_x000D_
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. _x000D_
Tim_x000D_
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. _x000D_
Alexey_x000D_
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. _x000D_
Tim_x000D_
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well._x000D_
Alexey  _x000D_
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? _x000D_
Tim_x000D_
Right. Some places, though, require specialized… _x000D_
Alexey_x000D_
Like Lambda, right? _x000D_
Tim_x000D_
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format._x000D_
Alexey _x000D_
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
Machine Learning Zoomcamp;2022;Have you used logistic regression in your work?;"Very, very often. This is probably the model that I use the most at my work. This is something I use pretty much for every project – for some projects. Not only is it a good first baseline, but also sometimes we just deploy this logistic regression, it works and then no further work is needed and we just leave it there. Sometimes these models are then improved with something like XGBoost or something, but logistic regression is like the workhorse of machine learning. It's used in many, many, many stations. _x000D_
Actually, in some situations, it's not really possible to use anything else except logistic regression. For example, at the previous company where I worked, which was an advertisement company, and there, it was very, very, very, very important to be able to make predictions very fast. Logistic regression is the best model for that. You cannot beat the speed of logistic regression with any other model. Maybe with a decision tree, but it will not be as good as logistic regression. Logistic regression works really well when you have a lot of features and this was the case in the company where I worked. It's really an important and useful model."
Machine Learning Zoomcamp;2022;In deep learning, why is it that in some cases it's not necessary to do preprocessing for images and only rescale?;Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
Data Engineering Zoomcamp;2023;Are all the homeworks/projects individual or do we have to form teams?;Everything you do here is individual. You don't form teams.
Data Engineering Zoomcamp;2023;What are the reasons why we would just use Spark instead of DBT? Do both Spark and DBT do transformations? What are the differences? Is it possible to combine DBT and Spark? Before feeding the data into Spark, can we perform the transformations needed via SQL using BigQuery or DBT rather than Athena?;"Victoria_x000D_
The first thing to note is that you can use DBT with Spark. There's an adapter for Spark that you can use. You probably saw it when you created your project in DBT cloud and then for Core, you need to install a different adapter – instead of DBT BigQuery, you do DBT Spark. This means that the limits that DBT has for the transformation are really the limits of the data platform that you're connecting it to. The question is not whether it's more Spark versus BigQuery, rather than DBT versus Spark. _x000D_
The other question is: Is Spark with DBT or not? That one can also sync with any of these platforms friends. Why we chose to include both. These are alternative ways of transforming and, as a data engineer, you're going to find both setups in a company. There are companies that you may work at that have all transformations that are all Spark, while other companies have all of the transformations in BigQuery. The same – or just Redshift or just Snowflake. _x000D_
Some companies have both BigQuery and Spark, for example. The main difference really will depend on the use case of your data. Spark is usually used for higher loads of data, or a need for streaming as well. That's where normally one would use Spark. It also depends on the data engineering team. And then BigQuery is going to perform better in big data sets, but still is more than Spark in batch processing. I would still recommend reading more about both."
Machine Learning Zoomcamp;2022;What function or library can I use to separate a set of images and training validation and test?;You're talking about images, so the way I would do this – maybe there is a library for that. I would use a thing called glob. I would use this glob function from Python to get all the files in the directory with images. Then you have a list of files and then you can use something like train/test/split from SciKit Learn to split it into three groups. Each of these groups would be train, validation, and test sets. This is how I would do this. It's not a library, so you will still need to use like four lines of code or something like that. Use glob, and then train/test/split from SciKit Learn and then you'll be able to do this. Of course, that might not be enough. You might also need to write a bunch of extra code for putting this into directories. For example, in this clothing dataset, I already put this dataset into three folders – we have train, and then we have classes here. So you might also need to write some code for distributing all your files into different subfolders. But this is also not a lot of code, like 10 lines maybe 12 maybe 20, give or take. I don't know – but it's not too awful. Maybe some of you will figure this out – you can post it in Slack, so then others can also use it.
Machine Learning Zoomcamp;2022;Any tips for understanding from scratch, hypothesis testing and p-values?;"There is a good book about that called OpenIntro Statistics. You can check that one out. There is also a book called Think Stats. That's also a good one. Check these two books out, especially Think Stats. I think it's free. Both of these books are actually free. As you can see, it's available as a PDF. Just go through this book. _x000D_
Think Stats is good because it shows code – there is a lot of code. It just shows you all these things like hypothesis testing, p-value, and everything. To some extent, it's kind of similar to this Zoomcamp. They focus first on a project and then they implement some of the things. Check that one out."
Data Engineering Zoomcamp;2023;How much SQL do I need to know?;Enough to complete the first homework. If you can't complete the first homework, you should probably take a SQL course. Or maybe you can quickly take it now and finish the course. I know students who did that. If it's not too difficult, or not too unfamiliar regarding what we do in the SQL refresher video and for the homework, then you're fine.
Machine Learning Zoomcamp;2022;Do you recommend ML developing solutions directly with cloud? For example, using AWS Sagemaker inbuilt algorithms without using Docker.;"I personally would not, but that’s just me. I usually prefer… I want to understand what's going on there. If it's just a prebuilt thing and I put some data in and then something comes out and I don't really know what's going on there – I don't trust this thing. But that's just me. Again, maybe the important bit here is –you set up your validation framework and whatever blackbox AutoML solution you use gives you a good score on this validation framework, then you're good, right? If you don't worry about how exactly it's done there. Just use it. _x000D_
It’s probably a good thing not to worry about these things, especially if you're a startup and you want to move fast – you just want to get something that is already available for you. You just want to start using it and then see the benefits of using it. Maybe just go for this. But I usually like to have a bit of control of what's exactly inside. That's why, for me, I usually prefer to have a Docker image that I deploy to the cloud (to AWS Sagemaker, for example). It just gives me peace of mind, I guess."
Machine Learning Zoomcamp;2022;Is log transformation suitable for both right- and left-skewed distribution? Is there any other better transformation method available for skewed data?;"Yeah, I think log transformation is suitable for right-skewed distributions. The same we had in the course. Because for the left-skewed, I don't actually know how it will work. You can just apply it and see what happens. I think that's the best approach. Maybe the answer is “it depends”. You can just try it and see what happens. Then based on what you observe, you can decide what to do next. If you see something that remotely reminds you of a bell-shaped curve, then you're on the right track? I hope I answered this question. _x000D_
I don't know about any better transformation methods available for skewed data. I think logarithmic transformation is a pretty standard one when it comes to pricing data, that's why we did include this in the course. If you work with prices, it's very likely that you will have to use this one. But when it comes to others, maybe it's less common. There are things like the Box Cox transformation – there are quite a few of them. But maybe they are not as widespread as this one."
Data Engineering Zoomcamp;2023;In the course, itâ€™s mentioned that it's better to destroy infra using Terraform (the storage buckets and BG_dataset). If I apply them again, will all the data be gone?;I think the idea here is that you can do this, but you don't necessarily have to do it right now. You can do this at the end. Because if you do destroy it now, of course, the buckets and BigQuery datasets will be gone. When you apply it again in the future, it will be recreated. Indeed, the data will be gone. Don't destroy.
Machine Learning Zoomcamp;2022;I would like to understand how we take the data engineering component and connect it to this course. For example, transform Big Query data to train a model.;Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
Machine Learning Zoomcamp;2022;What would you advise to learn between TensorFlow and PyTorch?;I don't know. Since we covered Keras in the course, I'm kind of biased, because my advice would be to learn Keras. But I do admit that PyTorch is becoming more and more popular. Probably, if you look at the community size, you will find a lot more things in PyTorch than in TensorFlow. Also many new things are first implemented in PyTorch and then ported to TensorFlow. For example, Stable Diffusion was first implemented in PyTorch and then ported to TensorFlow. But in the end, it actually doesn't matter. They're quite similar. If you’re now learning TensorFlow or Keras, and then you join a company where they use PyTorch, you will just switch to PyTorch and the other way around. On a higher level, it doesn't really matter, but if you want to go with popularity, then maybe PyTorch is more popular for modeling purposes. But if we talk about model deployment and model serving, I think PyTorch is still getting there. Because of TensorFlow Serving and there’s also a thing called TensorFlow Extended and all that, it's backed by any data, but to me personally, it feels that TensorFlow is more mature when it comes to model deployment. I might be wrong, but this is my feeling.
Data Engineering Zoomcamp;2023;Do you feel any feature is missing in Prefect that is available in other tools like Airflow, Dagster, etc.?;"Jeff_x000D_
No features are missing. Everything's there. Easy. _x000D_
Alexey_x000D_
And more, right? [chuckles]_x000D_
Jeff_x000D_
Yeah, and way more. [chuckles] You know, I think there are always new things we can add. It's kind of nice to have multiple tools that are all working hard to compete with each other and up the game for workflow orchestration. I think the winners are everybody who are the users there. So we will keep doing that. As I mentioned earlier, we have more visualization tools in the works and things happening there. You always want to keep making things easier and easier to use. In this course, you got to see a number of things, but we didn't even touch on things like parallelizing, working with Dask and Ray. We have integrations for things like that. You can always just make it an easier process. There's a number of parts that you have to start for any of these services, whether it's Airflow, Dagster, or us. We want to do things like make that easier and easier. There are lots of things there. We have a free Python version just like everybody else, because we want people to go and use it and get value out of it. And that's great. Then for companies that need to have more enterprise features – you want single sign-on, you want to be able to control things when people leave your organization, all that kind of stuff. We got a company behind it. It's a win-win process for everybody and it allows us to get paid and work on Prefect and do things like teach courses like this. And it helps everyone do it. So I think it's good for all data engineering."
Data Engineering Zoomcamp;2023;Is it possible to extend the week 3 homework deadline?;"Alexey_x000D_
I don't know when this question was asked, but we did extend it."
Machine Learning Zoomcamp;2022;Rule-based modeling versus data modeling: what points should be considered to pick one over the other?;"Again, it really depends on your problem. But you probably want to start with rule-based modeling. The very first approach you can take is to develop a simple rule. Then you can actually learn this rule. Maybe you can train a decision tree on this data and then you can see what kind of rules the tree learned. Then you can just use that as your first model. But then, over time, it's growing. Actually, I think I'm just describing data modeling here. [chuckles] I just realized that training a decision tree and then using it kind of becomes data modeling. _x000D_
But honestly, for some systems, it makes sense to have rule-based modeling, because sometimes you want to have flexibility of overriding some of the model’s decisions. You can even have both. You have a machine learning model, and then you have rules and they kind of coexist. This is actually what we have at OLX for moderation. Moderation is the process of deciding if a listing (this is an online marketplace) and let's say you want to sell your phone. You go to OLX and you want to sell it. You fill in the title, the description, you upload some pictures, and then you hit “publish”. The listing is not published immediately. It goes through the moderation process. We look at this listing, and we conclude that it's safe to publish. There are a few checks. The first check might be “Is it a duplicate of another listing?” If it's yes, it's not published. If it's not a duplicate, then we do another check. Actually, these checks happen in parallel. Another check could be like, “Is there a gun or some other prohibited item?” Or “Is it not safe for work content?” _x000D_
There are a bunch of checks. Most of these checks are machine learning models. But also, a lot of these checks are rule-based models. I will not tell you these rules, but they could be “If something then not published,” for example, or “If something, then show it to moderators.” These rules alone, they're very flexible and moderators can just go to our moderation platform and add these rules manually. This is very helpful, because retraining a model takes some time. You don't always have this time, especially in cases like moderation systems. _x000D_
You want to have the flexibility of adding rules and reacting to things that are happening almost immediately – as they happen. Rules allow for this flexibility. These two things coexist. In summary, you can start with a rule-based model then you do data modeling – and, actually, one does not exclude the other. You can use these rules as features to your models and then you can use both in your production systems."
Data Engineering Zoomcamp;2023;Do Office Hours focus on course chapters or is it more like an Q&A sessions on the videos?;It's both. If you have any questions you want to ask, then you can use Office Hours for that.
Data Engineering Zoomcamp;2023;Will there be a step-by-step solution to question 4 in week 2?;"Jeff_x000D_
Yeah, I got the video recorded already. You just can't see it. We will release it after the deadline."
Machine Learning Zoomcamp;2022;I lost five weeks of ML Zoomcamp, including the midterm project and the last two homework assignments, but I'm catching up. Is it fine if I do both capstone projects?;Yes, that's the main reason we have two capstone projects. This is for those who are catching up to be able to get the certificate. Now there are holidays, like Christmas holidays and then January holidays in some countries – like in Russia, for example, people usually take holidays not during the end of December, but at the beginning of January. No matter in which part of the world you are in, you still have one month to catch up (November) and then start working on the capstone. Actually, I’ve now realized that the capstone starts quite soon. But for capstone 1, you only need the first six weeks or so, and then you can finish this project. Then during the holidays, you can watch Kubernetes, neural networks and so on, if you want. Then you can do capstone 2. That's probably better. But yeah, it's fine. You should do 2 capstones.
Data Engineering Zoomcamp;2023;Do I need to know cloud basics?;It will be helpful, but you can learn this in the course as well.
Data Engineering Zoomcamp;2023;In ETL web to GCS, can we save the data as a parquet file directly to GCS instead of locally first?;"Jeff_x000D_
I think we answered this in Slack and a few other places too. We just want to do a few things. You could just go right in and ingest it as a CSV file, but we wanted to actually change it into a parquet file, do some transformation there and just go through a little example of cleaning up something. But you could go and read something directly in. That's fine. There are tools that are becoming more popular for doing ingestion in particular. Things like Airbyte and Fivetran and other places, especially if you have lots and lots of data in a big organization where you need to have ingests – that can be helpful._x000D_
Alexey_x000D_
In the live chat, there is a comment that says “Getting a Data Engineering Job - Jeff Katz.” This is actually one of our videos. Let me show you how to find it. Go to our YouTube channel and then you just put “Getting a Data Engineering Job - Jeff Katz “ in the search and then this is the link. Check it out. It's also pretty useful."
Machine Learning Zoomcamp;2022;I have a small dataset with 160 items. Any recommendations on train-test split? Cross-validation helps, right?;"Yeah, you definitely need to use cross-validation here. Tree models overfit. That's why you need to use cross-validation – to make sure that this doesn't happen. Usually, when you have such a small dataset, it helps to use very simple models. Trees – when you let them grow indefinitely deep, they do tend to overfit. _x000D_
So maybe you stick to the depth of three, or use something like linear regression or logistic regression with very few features. Because you have a small dataset, you need to be very careful. I guess, that’s the general rule – use simpler models and then use cross-validation."
Data Engineering Zoomcamp;2023;Any Flink materials in this course?;No, we don't cover Flink
Data Engineering Zoomcamp;2023;In which situation would we choose to use Spark or DBT to perform data transformations?;"Alexey_x000D_
I think I tried to answer this question in Slack already. Let me try to recall what I said there. Spark requires an infrastructure to run it. This is more like a downside of Spark. Because with DBT, you delegate all the compute – all the transformation happens in a data warehouse. DBT just orchestrates this transformation. With Spark, it takes the data out of the data warehouse, transforms it in the cluster, and then puts it back. So it's a different thing. In the case of DBT, the data never leaves the warehouse. In the case of Spark, it does leave and it processes it. _x000D_
There are pros and cons to that. For some cases, Spark is just cheaper, especially if it's not a data warehouse, but a data lake that you’re using. With Spark it’s just a lot easier to access the data lake and do all the transformations there on the data lake, and then save the data back to the data lake. On a large scale, when you have a lot of data, it becomes cheaper. With Spark, you also kind of have more control, because it's Python or Java – whatever you use. This is code – you can test this code, you can… it's just easier to manage this code, because it's code. Meanwhile, in DBT – of course, you have tests in DBT, but they are different. They're not like unit tests. So you have more control, more flexibility – for more complex things, for more complex transformations, perhaps Spark is a better option. _x000D_
I don't remember what else I wrote there. I think we should put this in the FAQ somewhere."
Data Engineering Zoomcamp;2023;In real life, each environment has its own warehouse: dev, staging, prod? How to keep in sync? Any articles on a typical workday from feature branch to merge?;"Victoria_x000D_
I don't have a good article where it would explain this that I know. In my experience, this depends a lot on the company and the person setting up. I've worked in companies where we had completely separate accounts for each stage. We would have something like one for development, one completely separate account for something like QA, and one completely different for prod, and then we would have something like a cloning production into our QA environment and our dev environment, for example. _x000D_
But normally, the architectures I always see work with DBT are – you would have one warehouse where you'd have multiple data sets (as it’s called in BigQuery). In others, like Postgres, Redshift, and Snowflake, it's called schemas. Or you'd have one warehouse with separate databases. Then you could use something that all of the data warehouses tend to have, which is some way of doing tasks, where you can copy. For example, doing something like copy clients, where you can move the data from production to dev. _x000D_
Another topic that is super related to this question, in my opinion, is CI/CD. We didn't go through CI. I would recommend this blog as well. Here, what you could also do is all merge. You would have normally open a pull request and you would have in there something like a CI check that would make sure that the code that you're trying to merge works. Then you could run something like a DBT Cloud or merge, like a shove in here or on a pull request. That is also another way where all of these stages would communicate."
Data Engineering Zoomcamp;2023;I was able to run the Terraform part of homework 1 from my laptop. Can I keep doing this?;Yes, you can keep doing this. You don't really need a cloud VM. A cloud VM just makes things simpler for us in the sense that everyone has the same environment. Then, if you have some problems, we can help. But if everything works locally, just keep using your local environment.
Data Engineering Zoomcamp;2023;In the real world, would we create a container for our Terraform setup?;Terraform is not for creating containers. It's for creating infrastructure – different services in the cloud. One of these things could be creating a Kubernetes cluster, for example, creating… I don't remember how these services are called in Google Cloud Platform, but there are some other container orchestrators. Then you deploy your Docker containers there.
Data Engineering Zoomcamp;2023;How many homework assignments will there be?;Six and the first one kind of has two parts. So I guess, seven.
Machine Learning Zoomcamp;2022;How relevant do you think books are when everything changes so fast in the tech world?;"Well, I'll tell you a story about my book. In the book that I wrote Machine Learning Bookcamp, I thought that it would be a good idea to include a chapter about Kubeflow Serving. So I did that. It was actually at the end. I wanted to write a chapter only about Kubernetes, but then I thought, “Oh, it could be a good idea to also talk about Kubeflow Serving.” So I included that and it took a couple of months for the book to go to production, meaning they turned all these documents with chapters into a book. _x000D_
By the time the book was published, Kubeflow Serving no longer existed because it got renamed to K Serve and most of the stuff I was talking about in the book no longer worked. Books on new technologies become pretty… not irrelevant, and maybe not obsolete, but maybe slightly outdated. Therefore, the last part of chapter 10 in my book is outdated now – you can only use it for this old version of Kubeflow Serving. _x000D_
It’s the same with even this course. We did this K Serve model and by the time it finished, there was a new version and some things stopped working there, so we needed to update that. I want to thank Max Payne (I know it's not his real name). He was very helpful preparing things and he left a few notes there. So thanks a lot. Because of these notes, now it's actually up to date. The video might not be, but if you look at the notes, it is. I recently checked it and it's still working. Books do become outdated. _x000D_
But the rest of the stuff I put in the book is actually not outdated. Kubernetes still works. Flask still works. Maybe the version of Python we used (I think it was 3.7 in the book) I wouldn't use now. I would use 3.9. But most of the concepts are fine. So yeah, some things changed, but not so fast. Some things change fast. There are books, for example, Elements of Statistical Learning, that even though most of the content is pretty old, if not everything, it’s still relevant. But because it's theory – theory maybe doesn't change – fundamentals maybe don't change as fast as new technologies."
Data Engineering Zoomcamp;2023;Do I need to publish a link to a public GitHub repo or should we zip and share the file?;Yeah, just the link. You cannot upload a zip file to the form.
Machine Learning Zoomcamp;2022;Can you explain regularization better? I did not really understand it from the course video. Thank you;"I need more information to actually help you here. I don't think I will have enough time to explain it here. What you can do is maybe check out “regularization Andrew Ng” via Google. He is really good at explaining regularization. He's explaining it from a different point of view, but the explanation he gives is really good. So check it out. _x000D_
The problem we had here in this course is – sometimes you have correlated variables, when you have one variable and another variable mean the same thing. When this happens, you cannot invert the matrix. That's why we use regularization here. But it has a much wider scope. And Andrew Ng’s explanation will help you. If you have specific things that you did not understand, please ask in Slack, or maybe next time. But this is such a broad question that I do not know how to best answer and help you."
Data Engineering Zoomcamp;2023;I am really a beginner with IaC (Infrastructure as Code), but I saw a Reddit post in the DevOps sub about Pulumi being better than Terraform. What are your opinions on Pulumi?;"Alexey_x000D_
I have never used Pulumi. Again, we use Terraform because it's just one of the tools. The principles apply, Infrastructures as Code applies. It doesn't matter if you use CloudFormation, Terraform, Pulumi, or something else – the principle is still the same. The syntax is better. Pulumi might be better than Terraform when it comes to syntax. I think everything is better than Terraform when it comes to syntax. I don't know why it's so popular because the syntax, in my opinion, is terrible. But yeah, I never used Pulumi. If you like it, you can use it for your project."
Machine Learning Zoomcamp;2022;For experiments, do we use the Conda environment and when we're ready for model deployment, we create another environment with Pipenv for the web app?;"The way I usually do it is I have one Anaconda environment (I don't create a separate Conda environment for all experiments, I just have one Anaconda) [Terminal image as reference] I usually stay in the base environment. All the libraries I need for development are here, so I use them. And when I need to productionize it, then I create an environment with Pipenv. So this is how I usually do this. It doesn't mean that you have to do this the same way or that what I do is the “right” way of doing this. This is just the way I usually do it. _x000D_
Some of you might say, “But we need to have an environment from the very beginning.” And you will be right in this case. Ideally, for each project you start, you will start it with creating a fresh virtual environment and you install the packages there. Then you have no surprises when you move from the exploration and training phase to the deployment phase – all your dependencies are the same. But I'm lazy and I just use the environment I have for all the POCs – for all the, let's say, exploration projects. Then, when porting them to production, I create environments."
Machine Learning Zoomcamp;2022;In the homework for week 3, we created the above_average with the whole dataset. This is a bad practice, right?;Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
Machine Learning Zoomcamp;2022;Is one-hot encoding recommended instead of dummy-encoding?;"Yes. Well, dummy-encoding is… what's the difference – I don't remember. We don't include one of the columns, right? Initially, when I said that hot-code encoding is recommended instead of diamond coding, I thought you were asking if we should use one-hot encoding from SciKit Learn versus one-hot encoding from Pandas. In Pandas, we have this get_dummies method, which works like one-hot encoding, but the problem with this method is that it will create different… Let's say if you're on your training data you have this variable “make” with only 10 different values, but in validation you have 11 – you will have a different size of your matrix. You will see this covered this week, when we cover one-hot encoding. _x000D_
Actually, using one-hot encoding from SciKit Learn is recommended. The way we implement this is through dictionary vectorizer (DictVectorizer) and this is something that makes it easier to deploy models later. This week, you will see how we implemented one-hot encoding and in week five, you will see how we can use this dictionary vectorizer to turn a request that is coming to our web service, and then turn it into a matrix, and then predict whatever we're predicting."
Machine Learning Zoomcamp;2022;How to choose how/where to deploy a model?;This is maybe a long discussion. For simple projects, lambda is probably the best choice, because you don't need to worry about paying for the time when your servers are up. Kubernetes is usually needed when you have a lot of traffic. For example, when you have 1 million requests per day, then it's time to consider Kubernetes because it could be cheaper than lambda. I guess this is like a rule of thumb – if you have a lot of requests go with Kubernetes, if you don't have a lot of requests, go with lambda. Also, for Kubernetes, you need people who will know Kubernetes well. If you're just a data scientist and in your company, nobody else uses Kubernetes, I don't think it's a good idea to use Kubernetes. Only use Kubernetes if you already have Kubernetes in your company and if there are people who know what they’re doing and know how to operate a Kubernetes cluster. Because as data scientists, as machine learning engineers, that might not be your core responsibility to maintain Kubernetes clusters. You will probably have a lot of other work, so you don't want to give yourself any more work by maintaining the cluster.
Machine Learning Zoomcamp;2022;By the end of the project, is it possible to do a project simulating a real workplace (teams deciding the approach, closely simulating a work environment)?;"I'm happy to support you if you want to organize that. In DataTalks.Club, we have so many initiatives that it will be very difficult for us to actively organize this. But if you want to do this, I can give you all the support you need. I can promote this, I can send it in the mailing list, I can announce this on LinkedIn. So I will be very happy to organize that, but you will need to think of what kind of project it could be. We can actually think together. If you want, you can start putting some of these ideas in a Google document and then we can discuss this and run it. But somebody needs to start this initiative. _x000D_
I, unfortunately, cannot do this – but I will be very happy if we do this together. We can think of some cases, actually. One of the examples somebody shared in these Office Hours was to train a model that could select the best answer from the FAQ. We have a lot of answers here and many people go to Slack to ask a question. Many of these questions are already in this FAQ document. So a good “close to real-life” project could be to build a system that can actually select the best answer from the FAQ for a question in Slack. It could be a nice idea – if you’re interested, let me know and we can see how to organize that. Or maybe you have some other ideas."
Machine Learning Zoomcamp;2022;Does all real time machine learning model deployment follow the same procedure as we did in week 5 or are there other tools and procedures available to deploy it?;"If I look at the course content outline, we are here right now, which is deployment. Then we will have BentoML, which is deployment. Then we have neural networks, which is more about how models work. Then the rest of the content is about deployment. As you see, 3 of the models (plus one optional) are about deployment. There are other tools that you will learn about and how to apply them. The procedure is very similar – you train a model and then deploy it. That's more or less it. _x000D_
What we covered in week 5, are fundamentals – how to organize your virtual environments, what Docker is, how to create a web service, etc. You don't have to use Flask. Many people say that “Flask is no longer cool – FastAPI is cool. We should use FastAPI.” Please, go ahead and use FastAPI if you want, but the process is roughly the same no matter which tool you use. If you want to learn more about that, also check out our MLOps Zoomcamp where we go into more detail about how it should actually be organized. We also talk about different tools we need to use for training and so on. The current course is the foundation and then MLOps builds on top of that and goes more into all the engineering aspects of machine learning."
Machine Learning Zoomcamp;2022;Will there be live events?;This event is a live event. So, yes, there were live events and there will be others, if this is what you meant. If you meant something else, please let me know what exactly you have in mind.
Machine Learning Zoomcamp;2022;For the capstone projects, can we use new models? For example, for object detection.;Yes, by all means, you can. But please make sure you document everything in the readme file. You need to assume that the people who will review your projects do not know anything about object detection, which will probably be the case because we didn't cover it. You will need to describe all of that in the readme. Like: What is object detection? What exactly are you doing? What is the function? What is the metric you use for evaluating? How does it work? Then people will look at your project and understand what's happening there. So you need to put more effort in documentation, but this is actually a good thing because your reviewers will also learn something new. If you feel like you can do that, then by all means, do that.
Machine Learning Zoomcamp;2022;Do you recommend doing projects like those in #project-of-the-week (which seem to be more tutorial and discovery) to build a portfolio?;"Yes, this is a really good way to build your portfolio. By the way, if you don't know about #project-of-the-week, this is the repo. Well, we only had two iterations here. But here, it's more like a study group, where we focus on building and learning things. For example, we did this one about Streamlit – there is a task for day one, there's the task for day two. You are somehow on your own, but still at the same time with others, who are also going through this. This is, let's say, more independent. At the end of this thing, we will not have 30 people all with the same project – we’ll have 30 people with different projects. It's a different thing. It makes it a bit more interesting and unusual. _x000D_
It's definitely a good way to build a portfolio. If you have some ideas on what exactly you want to learn, you can suggest these ideas and we can do this as a #project-of-the-week. So yeah, I do recommend this approach. I think that’s one of the best ways to do projects is when you try to come up with a problem that you want to solve yourself. And you will have to do this for this course too. We will give you some guidelines, these guidelines will be this evaluation matrix that you’ve maybe seen, but you will actually have to find a dataset yourself, you will have to find a problem yourself, and then you will have to implement everything we did in the course yourself. At the end you will have a model that is deployed and could be used. And I think this is really good. This is what you want to have."
Data Engineering Zoomcamp;2023;Do we have reading materials for every week? Itâ€™s nice to read on topics more in-depth.;No. But if some of you find anything interesting, please share the links in Slack
Machine Learning Zoomcamp;2022;If I want to be called a full stack data scientist with machine learning data science and data engineering knowledge, what should I learn after ML Zoomcamp?;Well, I’m glad you asked. We have a data engineering Zoomcamp, which starts in January and we have the MLOps Zoomcamp, which starts in May. The only thing that is missing here to become a full stack data scientist is the business understanding part. Remember, when we talked about the CRISP DM process, the first part was business understanding. This is business domain knowledge and things like that. So the only thing that is missing to become a full stack data scientist is picking up this part. For that, I don't know what the best way to learn it is, apart from just joining a company and then talking to stakeholders (to the users of your model) and trying to understand more and more from them. I do recommend doing this, but let's say if you’re not working yet and you want to be a generalist, taking all these three courses is fine. You don't have to do everything there. For example, in the data engineering course, maybe data warehousing is not as important for data scientists as for data engineers, so maybe you can skip that part. But chances are that, as a data scientist, you will need to work with data warehouses too. So you might as well just watch the whole thing.
Machine Learning Zoomcamp;2022;How about starting a Kaggle competition?;"That's a very good idea. We were actually thinking of doing something like this for December. December will be somewhat of a slow month – Christmas holidays and New Year holidays, we'll have a break. But we were thinking maybe we can start a competition at the beginning of December and finish it at the end of January, with other things. _x000D_
Actually, I even had an idea of using this generated dataset of dinosaurs and dragons. There are dinosaurs and you can also have dragons here – then you can build a model that tells the dinosaurs apart from the dragons. So that's one idea. Maybe it's boring. I don't know. We haven't come up with a better idea yet, so if you have some suggestions of what could be a good dataset of a good problem for this Kaggle competition, please let us know."
Machine Learning Zoomcamp;2022;I have missed three weeks. What should I do? Iâ€™m a beginner and it takes me some time to grasp the lessons. Iâ€™m so confused on what to do right now.;Just take your time. Go through the materials at your own pace. If we talk about the syllabus, you will only need the second, third, fourth – up to the midterm project. This should be sufficient – up to the seventh module. This should be sufficient to finish the midterm project and the capstone project. You can treat the modules after 7 as extra. They are useful – they are very useful. But if you're really short on time, you can just skip them and use the materials from the first seven modules to complete your capstone project. Once you complete your capstone project, once you must submit everything you did, then you can take the modules after 8 at your own pace. That could be one approach. Another approach could be just taking them without caring about the certificate. Just focus on learning and do things at your own pace. Don't rush.
Machine Learning Zoomcamp;2022;Is RMSE (vs MAPE or MAE, etc.) a common metric to use for regression models? Would it be a good metric for optimization in retail store planning forecasts?;RMSE and all these metrics are good. I am not an expert in that. I would suggest going to our YouTube channel, where we recently had a talk just a few weeks ago called Probabilistic Demand - Forecasting at Scale by Hagop Dippel. Check it out. He also talks about metrics there and you will see what exactly to use – what kind of metrics to use to evaluate your models for this specific case.
Machine Learning Zoomcamp;2022;Any recommended folder structure for large and complete ML projects (EDA, model selection, services, deploy, etc.)?;I like a thing called Cookiecutter Data Science. There’s a good directory structure here. You can use it. In the projects I do, the structure is pretty similar. So do check this out. Also Kedro project structure they have a good structure. You can check it out too. Kedro is a library for creating machine learning pipelines. But I don't think there is a common standard. There are good standards like this Cookiecutter one, but I don't think there is a single one that everyone uses.
Data Engineering Zoomcamp;2023;Data processing (transform) is after or before saving into Google BigQuery? Or do it directly to BigQuery?;Both. You can do it both ways and there are pros and cons. It really depends on the use case. From what I understood, it’s usually cheaper to first transform the data, put it into a data lake, and then load it to Google BigQuery. For example, in cases of DBT, this is what you do with DBT. You transform the data that is already in the data warehouse. Thus, both options work.
Machine Learning Zoomcamp;2022;How do you do data cleaning of a survey if all the columns are categorical and most of them have values like ones and zeros (i.e. True and False)?;"It's basically ready. You don't need to do any data cleaning if it's true or false. You just take it as is and put them inside a model. I think. Of course, it depends on things like if there is missing data. Then maybe you need to do something with this missing data. _x000D_
But from what I see here – if it's just ones and zeros – it’s good to go. You can just take this and use it as is. It's actually quite a simple situation from what it seems. Maybe I don't see some complexities there. There could be some, but on the surface it looks rather straightforward."
Machine Learning Zoomcamp;2022;When we have a large dataset, how should we select the features? Should I use correlation? What if the relationship was not linear?;"Yeah. Here, the important thing is to use some validation framework. So set aside some data for testing – for validation – and then use it to select features. There are many things you can actually use for selecting features. If you go to our DataTalks.club YouTube channel, there’s a talk called Feature Selection in Machine Learning with Python. In this talk, Soledad explains what kind of feature selection models there are – algorithms. So please check it out. And I think if you just Google “feature selection,” you will find a lot of articles about this. _x000D_
Correlation is a good first step and we will see this in week three, when we look at feature importance. Sometimes you can just throw away unimportant features and that should be large enough. But the best way here is to use cross-validation or just validation and see if removing the feature doesn’t actually doesn't change the performance of your model."
Machine Learning Zoomcamp;2022;Can you share the machine learning curriculum with us as a guide?;Yeah, of course. This is why we are all here, right? You just go to ML Zoomcamp and you follow that – that will be your curriculum. And I'm sharing it now with you. You're welcome.
Machine Learning Zoomcamp;2022;How does XGBoost or LightGBM improve the last tree and build a new tree and so on?;Yeah, that's what the lecture was about, wasn't it? I don't go into details about that. The theory is quite complicated there, to be honest. But there is a paper – this one. “Gradient boosting machines, a tutorial” by Alexey Natekin. This article talks about derivatives and the functional space. If you're into this kind of stuff, you can check out how exactly it works. But if you're not, don't worry. You can just skip that and focus on improving your validation score.
Machine Learning Zoomcamp;2022;Is Scikit Learn different from what you did in the video for linear regression? For homework 2, would we get different results?;"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. _x000D_
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. _x000D_
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
Machine Learning Zoomcamp;2022;What do we mean when we say that there are assumptions associated with a linear regression model?;"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: _x000D_
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/_x000D_
_x000D_
Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. _x000D_
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. _x000D_
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
Machine Learning Zoomcamp;2022;I'm struggling with pandas. I am getting your lecture videos, but when I try to implement on my own, I don't know which method to use and how to use pandas.;"Welcome to the club. Every new library is difficult. You can replace the statement with “I am struggling with X,” and it will apply to pretty much any other library. The good news is that you can somehow find a solution to this problem and the solution is practice. You get stuck – try to get unstuck by Googling, by understanding what you need, and then try to formulate what the problem you’re facing right now is. What do you want to do with pandas, but don't know how? Then try to formulate it and put it into Google and you will find a solution. This is how you learn libraries. This is how I learned libraries. _x000D_
There is usually documentation, tutorials – try to go through these tutorials – but first, it starts with formulating what the problem you have is and you want to solve, and then trying to find the solution to this problem. Usually the solution is a piece of documentation, or a video, or a tutorial, or a project that somebody did. Then you try to make sense from the information you find and this is how you learn – by practicing. There are probably videos about pandas – tutorials that you can just watch to learn a lot of that. But you still need to put it into practice. If you don't, then you will simply forget what you learned in the courses."
Data Engineering Zoomcamp;2023;Will you help us with coming up with interesting project ideas?;Yes, we will share some ideas with you. In this project folder, we have datasets. You can just go through these datasets and find something is interesting – you can just take it and do it. If you really struggle with ideas, of course, ask in Slack and we will be happy to help you.
Machine Learning Zoomcamp;2022;What would your suggestions be for projects that have impact from the interview and business perspective? 1) Choice 2) Data selection and gathering 3) Technicality;"Any project will work as long as it's not Titanic, or Iris or MNIST – or similar popular datasets. It would be best if you collect the data for this project yourself. That's a really, really, really big plus. Most people just find a CSV file on Kaggle and there is nothing wrong with this. Most candidates that have portfolios find a CSV on Kaggle and then they create a Jupyter notebook where they train a logistic regression model and they call it a day. But if you really want to set yourself apart from these people – again, there is nothing wrong with this, that's a good project already – but if you want to go the extra mile, you can, first of all, deploy your model. Then, show how you deployed this – and this is what we will cover in this course. In this course, we will train a model and then deploy it as well. _x000D_
If you want to go one more extra mile, then consider collecting the dataset yourself. You can just scrape data from somewhere, or you can use crowdsourcing platforms or you can just take pictures yourself. For example, for the book Machine Learning Bookcamp, there’s a chapter about deep learning and image classification. For this chapter I actually collected the data myself – I curated the data myself, but there are pictures that I took myself as well. I actually put clothes on the floor, and then took my phone and took pictures of the clothes. Then I also asked other people to do the same. This is how we collected the dataset for the book. Now we also use this for the course. _x000D_
If you do something like this, it will be insanely amazing. Very few people actually do this – very few people collect the dataset themselves for the portfolio projects. Then, you don’t only do your project end-to-end, because you start with collecting the data, and then you finish with deploying it, but it also shows that you care about this thing. Usually, you will only do this for things that you find interesting or important or something that’s close to you. These types of projects are especially interesting. I know some people who were looking for a flat in Berlin – in Berlin, it's very difficult to rent a flat – so they wrote a scraper that scrapes Berlin flats and then helps to find a flat. I think they had a model for predicting the price and they see how different the price is from prediction and use that to somehow find the flat. _x000D_
Here, you have a reason – a goal, a problem you want to solve – and then you collect the data yourself, you build the model, and then you deploy it. And you actually use it to solve your problem. This is amazing – this is as close to a business perspective as possible."
Data Engineering Zoomcamp;2023;If using a local laptop, how much free storage will be required?;I don't know – Docker can be quite hungry for storage. 50-100 GB should be enough, I guess.
Data Engineering Zoomcamp;2023;How powerful should your local machine be?;"Alexey_x000D_
I think 16 gigs of RAM should be sufficient. But again, this year, we are not doing Airflow. Airflow was the most problematic week when it came to local machines. When we tried to run Airflow in Docker, my computer was basically heating up and was going to fly away on the fence. It was too much for it. In the end, I did most of the stuff in a remote virtual machine. This is what you should do as well – use a remote virtual machine because you have this $300 credit. But 16 gigs should be enough, 8 gigs will not be enough. _x000D_
Michael_x000D_
Just in case they didn't catch that. If you don't have enough RAM or CPU, you can just start from the beginning in a virtual machine in GCP – you've got more than enough credits to make it through the course and be running that machine from the start._x000D_
Alexey_x000D_
That's the best thing. Then if everyone has the same platform, same operating system (which is Ubuntu) it just makes it a lot easier to help you. You just say, “Okay, I'm using this virtual machine. It's Ubuntu. This is the problem I have. And then anyone can reproduce this problem, hopefully. Meanwhile, if you’re running this on Windows or MacOS, then everyone has a slightly different version of Windows and sometimes it's just impossible to produce."
Data Engineering Zoomcamp;2023;Jeff, can you please record the live Prefect workshops?;"Jeff_x000D_
I don't know. When hosting meetups, I'll just draw on [a board]. It's great to have hybrid options for folks. People who can be remote, people can be in-person – it definitely adds complexity in how things are done. There is a lot more with logistics and things. The short answer is no. But if people are interested in some kind of online Prefect-specific course, send me a message in Slack. I'm there. Let me know about your interest level. It sounds like there's some interest._x000D_
Alexey_x000D_
At least six people are interested. _x000D_
Jeff_x000D_
[chuckles] At least six. _x000D_
Alexey_x000D_
Maybe more, but they just didn’t know that they could vote."
Machine Learning Zoomcamp;2022;When creating a histogram, is there any rule of thumb to decide the bin number?;You're probably in week two already, which is when we look at histograms. There, just follow your intuition. The answer is – enough to get an understanding of the shape of the distribution. This answer is pretty vague, I know. But when you start experimenting with this, you will kind of know when it's too many or too few. When it's too few, you basically have a few giant buckets and it's very hard to tell for you what the distribution is. But then, when you get too granular, then maybe it's just too much. Usually, for me, around 30-50 is a good rule of thumb to go with. Again, it depends. Just plot and then let your aesthetic feelings guide you. There is no right or wrong answer, I think. Someone wrote a comment that there are actually some rules for binning called Freedman–Diaconis. That's the first time I’ve heard of this, but you could probably check it out.
Data Engineering Zoomcamp;2023;Is it possible to do the course locally?;Yes, you can do many things locally. You can use Postgres and run all your queries there. It will not be the same as using BigQuery. But still, you will practice SQL, you will practice analytical queries, and that's what you need.
Machine Learning Zoomcamp;2022;I could not meet the registration deadline. Do I have a chance to register later and submit the assignments?;"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. _x000D_
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
Data Engineering Zoomcamp;2023;Where could we see other cohortsâ€™ personal projects to get ideas for our own and see the bigger picture of what we'll achieve at the end of the course?;Go to our date engineering course page. There should be a part with projects. Here, we have a peer review assignment. Here, you can actually see projects from the previous iteration. For example, this one. I think this is one actually from Lois who is one of our teaching assistants. This is his project. You can see what he did with this project. This is how you do this. You go through this, click on the project and you can see, for example, what Ilya (one of our students) did. You can see how they approached it. This is how you can find projects from the previous iteration.
Data Engineering Zoomcamp;2023;Any plan for Prefect to enhance their documentation? The docs didn't show examples of how to create it. I need to read the code of the class instead.;"Jeff_x000D_
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. _x000D_
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. _x000D_
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. _x000D_
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
Machine Learning Zoomcamp;2022;What do you think about plotline (ggplot in Python) for data visualization?;I have not used it, so I don't know.
Machine Learning Zoomcamp;2022;Could I pass the project if I build the modeling part, but not deploy it?;"Okay, well, let's take a look at the criteria. Model training, reproducibility, model deployment, dependency… You can see that four out of nine of the criteria are about deployment, right. This one (reproducibility) is also about that partly. Exporting notebook to a script. Hmm. Most of them are actually about productionizing your model, not about the modeling. If you do that, you will get seven points. I don't think that will be sufficient to pass the project. So you will need to work on this as well. But it doesn't have to be perfect. If you just create a Docker file, use BentoML, and you automatically have some points here and there. Just put in some effort and you pass it. Don’t worry. But if you just submit a Jupyter notebook, and that's it – I don't think it will work. You need to do a little bit more than that._x000D_
I want to add that this is a machine learning engineering course. This is not a course about data science. This is a course about engineering. If you want to be an engineer and receive a certificate showing that you learned the engineering part at the end, you have to learn the deployment part. There is no way around that."
Machine Learning Zoomcamp;2022;You have a dataset with two numerical independent targets. The question is, â€œDo you process it as two different datasets?â€;"For your project, what you can do is just select one of them – you don't have to do two. Another option would be to train two models. Let's say you have one XGBoost model for predicting target one and then you use the same features for predicting target two. Then you have two models. This is a perfectly viable approach. Then another approach would be to use a deep learning model or a neural network. With a neural network, the output that you have could be two numbers – your model will output two numbers, two targets. I guess that's what you referred to when you said “double”. _x000D_
By the way, we don't cover a use case like that in the course. We don't show how to create a model with two predictions. But if you do some research, you will find it. With Keras, it's actually relatively easy to do that. You can just Google or use your favorite search engine to find this. I think both of these options are good. It really depends. I don't know what the best way is. _x000D_
The upside for having a neural network is that you have just one model. But the downside is, let's say you want to improve the accuracy of predicting one value, but not the other – how do you do this without affecting the other prediction? Sometimes you might want to have this separation and you want to have independent models. I would actually go with two models."
Machine Learning Zoomcamp;2022;Do you have any recommendations for where to start with NLP? Project-based, hands-on, not full of theories â€“ like ML Bookcamp or some other camp?;"What I personally like is Kaggle. It's not a course, yes I know. But there, you can see a lot of different kernels, notebooks that you can follow and try to learn. I don't think this answers your question. I'll give two recommendations. The first recommendation is to go to our MLOps Zoomcamp. Then in cohorts 2021, in the midterm project, we had these three office hours. During these office hours, I showed some extra stuff. In office hours 8, I showed how to deal with texts. So, if for your project, you're going to use text, then check this notebook and check out the corresponding video on how you can quickly incorporate text in your machine learning models. This is a relatively simple approach. There is also a good book called “_x000D_
Introduction to Information Retrieval. This is a very nice book. It's also available for free. It's relatively old – from 2006 – but it's a very good book. It's quite practical. It talks about search, but many of these things are applicable to machine learning. For example, “vector space classification” is for text classification and Naive Bayes. So it’s not always relevant for NLP, but some of the things are. For example, “index construction/compression” is more relevant if you want to learn about search. But some of the things like “How do you break down strings into tokens, into words? What are the methods there?” I think it's a good book for that. There is also a thing called Taming Text. But I think the examples are in Java, if I'm not mistaken. So maybe it's a bit old. Kaggle is probably good. If you saw, I recently asked the question in general, “What kind of course do you want me to work on?” Maybe you can write what exactly you want to know about NLP. I know some stuff about classical NLP, information NLP, and search. But I don't know much about “modern NLP” which is what I call all these transformers, GPT-3, BERT – this kind of stuff. All this, I don't know anything about, to be honest, so I won't be able to actually do a course about that. But please go to the thread and write what you have in mind."
Machine Learning Zoomcamp;2022;When should I use a model with grayscale images?;In some domains, maybe the color doesn't matter much – on the shapes matter. If it's that domain, then yes. For example, I think for clothing, it might be the case. Maybe colors don't matter much, because you can have all different colors. But on the other hand, maybe it's actually still important because you want to distinguish the background from the foreground (from the actual item). This is where the color information could be important. If you think it will give you some performance increase in terms of making predictions faster – just use validation to find out if there is any predictive performance drop (drop in accuracy) when you switch to grayscale. If you see that there is no drop, then just stick to that.
Data Engineering Zoomcamp;2023;Do you have plans to expand on information related to cost optimizing data warehouses/cloud services?;"Ankush_x000D_
[corrupted sound] talk about pricing when it comes to BigQuery. I don't think we're gonna compare it with other data warehouse solutions outside Snowflake because Snowflake costing is a bit more complicated. I don't think so. I think we’re going to stick to BigQuery for now. And just remember, the less you pay, the better it is. But the human cost is always more than machines. There is always a bit of a compromise. [chuckle] To answer it simply – no, we're not going to do it."
Machine Learning Zoomcamp;2022;What do recruiters look for when hiring for remote DS jobs?;"That's like a pretty generic question, so the answer will also be generic. They will look at the skills you have, they will look at the match – how the skills you have match what is needed. I guess that's it. This is what recruiters usually look for in candidates. If you're interested, another thing you can check out is our podcasts. _x000D_
We have an interview, Recruiting Data Professionals with Alicja Notowska. Alicja is a recruiter. She was working as a recruiter at Zalando, which is a very large company in Germany and she shared some tips from your point of view of a recruiter and what she looks for when hiring people. So check it out. It’s quite insightful. You will probably find useful things there."
Machine Learning Zoomcamp;2022;I think this question comes from week two, which is about RMSE. Can you give us a feeling of what is a good/acceptable RMSE in an industry project? Or does it depend project to project?;"Indeed, it depends. It really depends on your data. Also, when it comes to RMSE, it depends on your target. If the values of your target are large, then your RMSE will also be large. So it really depends. For classification, usually the metrics we use are 80% (or something along those lines). So they are in percent, and are kind of not absolute, honestly. They don't depend on what you have in your data. But when it comes to RMSE, it depends on the values in your target variable. Here, it can be anything. There is no industry standard. _x000D_
Actually, when it comes to other evaluation metrics that we will study in this week – like accuracy, precision, recall – again, there is no good industry standard for this. In some cases, 90% is bad accuracy and in some cases, it's good accuracy. Precision of 90% is good sometimes, in some cases it’s bad. It really depends on the project, on what kind of data you have, how good your features are, and things like this. I would say that if your RMSE or any other performance metric is too good – for example, we will cover AUC as a metric – if it's 95%, then it's suspicious, usually. 95% or more. If I see a very good number, or RMSE that is close to zero, then it should raise suspicion and you should go and check and investigate and figure out why the performance is so good. Quite often, it's because you have some sort of data problems – data leakage, for example, or things like this. Usually, very good performance is an indicator of something being wrong."
Machine Learning Zoomcamp;2022;What topic would you consider fundamental and relevant for time series analysis and forecasting in real time? Would you teach this or can you give suggestions?;"We will not cover it in this course. There is a good book about this called Time Series Forecasting Principles and Practice. It's a bit oriented on R, but I think you can find some code for Python as well in this book. If you want to learn more about time series and forecasting, this is the book you should check out. Usually things like exponential smoothing are efficient for most cases. We also recently had a webinar about this called Building the Modern Geospatial Data Stack with Ramiro Aznar. _x000D_
We actually had multiple – we had this one, called Probabilistic Demand Forecasting at Scale, which is quite a nice talk. And this one, called Feature Engineering for Time Series Forecasting. These two webinars are quite comprehensive, so check them out. But I would start with the book that I showed you. I think in this question “fundamental” means things like exponential smoothing. I think this is quite an easy approach and it works quite well."
Machine Learning Zoomcamp;2022;Do we have to follow along with the lesson or is the homework completely independent from the lessons?;"The homework is not independent from the lessons. In the lessons, we show you how to do stuff and then the homework is to take the materials from the lessons and apply them independently. So you don't have to follow along with the lessons, but you can. It's up to you. If you feel that you already know this topic, just open the homework and try to do it. _x000D_
If you don't know how to solve it, go check the lessons. I'm also sure that if you already know, let's say, something about deep learning, then you can just find a solution online. Not the solution to the homework, of course, but the solution to the problem you have. Don't try to look for the solution to the homework problem online because that's cheating."
Data Engineering Zoomcamp;2023;Is there any plan to include a block for IBM Cloud Storage?;"Jeff_x000D_
We have not had a request for that, I don't believe. Not any of that I know of in our catalog of requests. I will note that there is interest in IBM Cloud Storage. But the best move here – you could probably take something that's very similar to GCP and add that as a collection, potentially. Our collections, again, are all here and you can look at contributing your own if you'd like, so then we would have that available for other people to use. I encourage you to check that out. You can also just make your own custom block and use that. That’s kind of an easier step. There's a section on custom blocks in the docs. You can check that out here and use those as a guide. That can be something that's used. But we haven't had a lot of requests for that, so it's not something I expected soon."
Data Engineering Zoomcamp;2023;If you have previously signed up with GCP and the free credits expired or used up, can you still use the free tier with the course?;Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
Machine Learning Zoomcamp;2022;Where to inform if there is an error in the evaluation of my homework (classification chapter)? My answer is correct but the system gives 0 points.;You can ask that in Slack. You can send a direct message to me and we'll figure this out.
Machine Learning Zoomcamp;2022;How are you going to validate the Kaggle competitions? Should we fill out a form so they know we are participating?;Yes, I did create a form recently. I should have created it earlier. You will submit your Learning in Public links here. But then you also will submit the name of the leaderboard and the score for Learning in Public. This is how you do this. This is how we will know, otherwise, it's tricky.
Data Engineering Zoomcamp;2023;Are you going to update videos for week 3? Currently, they are using Airflow.;I already removed one of these videos from the playlist and I removed it from the GitHub repo. Right now, all these materials – everything you need orchestration – is in week 2, including the part when you need to do some BigQuery stuff. This was the part in week 3 that was a part about using Airflow and BigQuery. Now it’s in week 2. Everything is already there and you should not see any Airflow stuff. We might have accidentally missed some of this, so if you see some Airflow stuff somewhere, please let us know.
Machine Learning Zoomcamp;2022;What do you suggest after this? Date engineering Zoomcamp or MLOps Zoomcamp?;I can't make this decision for you. I don't know enough of your background to answer this question. It can be both, too. Why not? Probably, the more relevant one would be the MLOps Zoomcamp. Here, I am assuming that you are more interested in ML engineering for this course. And if you're interested more in ML engineering, then it's definitely MLOps Zoomcamp. In the end, it's really up to you. There is no right or wrong answer, so I would suggest doing both. [chuckles] My opinion is a little bit biased here. Also, you only have 24 hours in your day.
Machine Learning Zoomcamp;2022;Is linear regression applicable in real life or is it more of a simple baseline model?;"It's both. It is very, very applicable in real life. Usually, most projects start with a simple baseline – without any machine learning – and then you can use the next baseline, which would be linear regression. Then your model can work using linear regression until you think “Okay, now it's time I need to improve it,” and then maybe you can use something like XGBoost or something like this. So it's very applicable and it’s applicable as a simple baseline model. _x000D_
Sometimes you just don't need a more complex model. Let's say if you have XGBoost, then it becomes more difficult to serve because it's more computationally demanding, while linear regression is just matrix multiplication. It's very simple. It's very performant. It is a good first model that you always should use and you should always start with."
Data Engineering Zoomcamp;2023;Why Web to GCS to BigQuery, and not Web to BigQuery?;"Alexey_x000D_
I guess that's a question about our ETL that we wrote – why we wrote first to GCS and didn’t skip it._x000D_
Jeff_x000D_
I think this comes up at some point when talking about having your data lakes. A lot of times, it's good to put the raw (or fairly close to raw) data into a data lake and then you have the opportunity to go and get it if you need to do something with it. That’s kind of the really raw unprocessed data. And then you can take it from there into BigQuery and use it in BigQuery for people who want to go and do some analytics work with it. There are a lot of different workflows these days. A lot of things are happening more directly with BigQuery. Things going right into data warehouses or “data lake houses” as are often referred to. These are both Google products – GCS and BigQuery – and they're just getting more and more tightly integrated as time goes by. We're able to do more and more and just actually store BigQuery data in Google Cloud Storage, and just kind of access it directly. There are just so many different ways that it happens, but a common workflow has been to first put the fairly raw data into the lake and then do things with it later in the warehouse. That's the flow that we're showing here. Ankush, any comments?_x000D_
Ankush_x000D_
I think, as you said, this is just about different patterns. One pattern is obviously writing directly, one is writing to GCS and then to BigQuery. And obviously, everything has its advantages and disadvantages. But writing to a data lake always gives you a certain backup or an advantage, per se. Because now if you want to move away from BigQuery and use Postgres or use Snowflake, you don't really have to care – you can start from GCS all over again. What happens if BigQuery is not accessible or it's getting expensive for machine learning load, and then you can directly read from GCS other than actually doing it through BigQuery. Right now, our example is very small and it does make sense to directly load it to BigQuery. _x000D_
But what happens if your data size is increasing on a daily basis, or a weekly basis, and you just cannot put that into BigQuery? All your data is very unstructured and you need to spend the time to figure out what kind of data you want to put into BigQuery. So all of these questions really come up when you have a lot of data coming and a big variety of data coming in. Using something like a GCS bucket, where you can just dump your unstructured raw data format, and then later on, maybe six months down the line, you figure out, “Hey, this data is really important. I want to put it into analytics. Let me pull this into BigQuery with some work.” And I think that's a great pattern to start with. I would always suggest looking into this pattern this way, but obviously, there are also companies and some data sources, where it does make sense to directly put into your data warehouse or BigQuery."
Machine Learning Zoomcamp;2022;Can you explain the difference between setting regularization without limit on W0 and with? Andrew Ng shows a formula without the limit, but in the second week, we made with one.;"Yeah, I honestly think that there is not much of a difference between this. In one case, if we don't put limits on the bias term, we say that we don't want to penalize it – it can be as large as possible or not. But in the case of Andrew Ng (in his machine learning lectures) they say “Okay, bias term can be anything. We only want to control weights for things.” Practically, I don't think there is a significant difference, but it becomes a little bit more difficult to implement it. _x000D_
When you add these to diagonal lines, for our case implementation is super easy. You just take this identity matrix and multiply it by something. But then in case we don't want to penalize for the bias term, it becomes a little bit more involved. Since I didn't notice any practical difference in these two approaches, I decided not to spend time explaining that it's also possible to do it this way. I hope that answers the question."
Machine Learning Zoomcamp;2022;Is it possible to submit a competition entry as a capstone project? I'd love to enter the competition, but I'm afraid I won't have time for both.;It is possible to use the competition as your capstone project. I actually would like to encourage that. So please do that. There’s one thing, though. In the competition, you only train a model – you do not deploy the model. That is something you will need to do extra for the capstone project. You can spend a bit of time taking part in the competition, then you will have a model, and then you can just do the rest of the stuff like deployment of that model for your capstone project. That is totally fine. Also, the competition will last for two months. I wanted to open it from the first of December, but somehow I didn't see how to keep it closed and open it on the first of December on Kaggle, so it got opened earlier. The official date would be the first of December and then from that, it's two months. So maybe during these two months you will be able to find a bit of time. Please, go ahead and use it as your capstone project.
Machine Learning Zoomcamp;2022;Can we mix the type of transformations (different transformations for different features)?;I think so? I need a bit more context to answer this one. But yeah, you can mix transformations in any way you want.
Machine Learning Zoomcamp;2022;How to approach tabular regression sales data when data, for example, for January to August, 150-200 units, suddenly jumps to 400 units in September.;"We're not covering time series data here. I'm not really an expert in that. Maybe I can give you a quick answer from what I would do in this case. Here, you probably want to have the same number of samples (units) for each month. I think in Pandas, you can do this with the resample method. This resample would make sure that you have the same amount of records for each of the dates, for example – for each of the months, days and so on. I think I would go in this direction. _x000D_
When it comes to actually predicting, it's a totally different story. The usual approach is like what we cover here for linear regression – they don't necessarily work here. They work with some adjustments. For that, I will, again, invite you to check our channel. In our channel, we covered these topics recently. There was the podcast called Feature Engineering for Time Series Forecasting from Kishan and then Probabilistic Demand Forecasting at Scale. These two talks could be interesting for you if you're into Time Series."
Machine Learning Zoomcamp;2022;What are LSTM, RNN, and transformers?;I'll show you a trick. You can thank me later. You take this question. You go to your favorite search engine, you put this question to search and you see the answers. You're welcome. [laughs] But honestly, I don't know much about these things. I know a little bit, but I might tell you something that does not make any sense, so I will not even attempt to do that. I'm not really an expert in NLP. Secondly, with Google, you can find a lot of good resources. Maybe I will show you one good resource which is CS224N. This is natural language processing with deep learning, which talks about all these things. Word vectors, recurrent neural networks are RNNs. So go through this thing if you're interested in learning more about that.
Data Engineering Zoomcamp;2023;Can I use GitHub Codespaces to make the Prefect Docker deployments?;I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
Machine Learning Zoomcamp;2022;Is it possible to use the same data for the next project but investigate it with a deep learning framework?;I would prefer that you don't do this and that you take another dataset instead. If you take the same EDA (exploratory data analysis) and you repeat it in a new project, it's kind of plagiarism. If you don't do this – if you do everything completely from scratch – it's kind of okay. But maybe you want to have a variety of things for your portfolio? So maybe try to get a different dataset, just to get exposure to different problems. It's not a super strict requirement – as long as you're not plagiarizing or self-plagiarizing, you're good.
Machine Learning Zoomcamp;2022;I want to work in ML engineering â€“ ideally, freelance. Do you think picking recommender systems as a topic to work on in my portfolio is a good strategy?;Yeah, I think it is. I don't see a reason why it's not.
Machine Learning Zoomcamp;2022;In your experience, Tim, do you think data scientists usually take care of model deployment or is it somebody else?;"Tim_x000D_
This is a really great question and I don't have a good answer for it, I think, because the industry is evolving so quickly. I've definitely seen data scientists being in charge of deploying models, probably where that data scientist is one of the only engineering resources, or the engineering resources that are there are too busy. If you're a data scientist and if you build a model, the way that you scale the business value of that model is having that model deployed as an API. If somebody has to do it and there are no resources anywhere else, then yes – the data scientists can do that and we try to make that as easy as possible. In larger teams, I think, typically not. You'll probably have at least some type of DevOps or ML engineer that will help you deploy your model. But knowing how the model deployment pipeline works, I think, is pretty important. That way, you can work better with your engineers. I think it's always nice to know at least the high and medium level of how it works and how to do it end-to-end in fairly simple scenarios. But if it becomes more complicated, I think it does take somebody with a little bit more specialized knowledge."
Machine Learning Zoomcamp;2022;How and where to apply for remote DS jobs for freshers?;Yeah, I don't know. LinkedIn? There is also Angel.co, which I think is a good one too. Check that out. Maybe Glassdoor? The usual places where you look for a job.
Data Engineering Zoomcamp;2023;What if you have no Docker experience?;That's why we discussed Docker in the first week. We actually meant this week as a kind of Docker refresher. In the end, many people didn't have any Docker experience. They took it and it was maybe a bumpy ride, but it was fine. Many people did not have Docker experience and they successfully finished the first week. But it was probably harder than other weeks.
Machine Learning Zoomcamp;2022;Can you suggest some theoretical machine learning course that is inclined with this course?;I don't know what inclined here means. But I think I already gave a recommendation – I think these two machine learning by Andrew Ng and this other one are quite orthogonal. They focus on different things.
Machine Learning Zoomcamp;2022;What team are you supporting in the Football World Cup?;For me, as a Russian citizen, it was actually surprising to learn that the World Cup started, because I simply didn't know that until last week, when people all of a sudden started watching football. I see that Ukraine isn't playing there – they have other problems right now, I suspect. I would root for them if they played. I guess Poland. I know that they won a match recently against Saudi Arabia – so, good job. I didn't watch it though. But if you asked me which team I would root for, then maybe Poland.
Machine Learning Zoomcamp;2022;Can we take a break, please?;"Yes, of course, by all means – take a break. This course is not meant to squeeze all the life juice out of you. You can take the course at your own pace, too – remember that. Please take things easy and don't worry about missing homework. The world will not stop spinning if you miss a homework assignment. Please be mindful of your own capacity and don't overwork yourself. Also, I should mention again, if you are a little bit behind and you cannot make it to the midterm project, it's not a big deal. It's fine. _x000D_
You can catch up later. December and January, (December mostly) will be pretty slow. So you can catch up with everything – all the content – by the end of December and then do a project in parallel to that. Then you will have all of January for another capstone project. Remember, you only need two projects. If you do these two projects as Capstone 1 and Capstone 2, then you will be fine."
Data Engineering Zoomcamp;2023;Is Prefect sponsoring the course in favor of showing off their tech?;"Alexey_x000D_
I don't think I understand the question. But I'll tell you a story. Last year, when we ran our Data Engineering Zoomcamp, we used Airflow. Airflow was quite problematic. In Airflow, it was very difficult to set up locally. We thought it would be easy. It was in Docker – it had a Docker compose file. We thought, “All you need to run is just Docker Compose Hub, and everything will work fine.” It was very naive of us to assume that. It gave many people a lot of problems. Meanwhile, in MLOps Zoomcamp, we used Prefect from the start. We didn't consider using Airflow. And it was a lot smarter. That's why we thought that for teaching the course, Prefect will be easier to get started, because it's a more lightweight thing. _x000D_
Prefect is a nice tool. It's a really cool tool. I'm a big fan of Prefect. But also Airflow is probably more popular. That's why we don't remove Airflow from the videos altogether. They are optional and we still encourage you to try them, too. But if you first walked through the Prefect video, I think it will be a lot easier for you to start with Airflow. It's more lightweight, the code is less verbose there, and it's easier to get started. Once you get up to speed with Prefect, then everything will be easier._x000D_
Somebody shared a report in the data engineering course channel. The report was from the AI infrastructure Alliance, I think. They also explain why people like Prefect more than Airflow, so maybe find it."
Data Engineering Zoomcamp;2023;Are the videos on the GitHub page from last year relevant for us?;Yes, they are. Most of the videos are still the same. There is no point in re-recording them
Machine Learning Zoomcamp;2022;In the weekly projects that we implemented, we did not use scaling techniques like Standard Scaler. Isn't it important to use it before building the model?;"Alexey_x000D_
It is quite important. I wouldn't say it's a must for all the projects. For example, if you use something like a tree-based model, then you shouldn't worry about scaling. I think I shared links to our Office Hours from the previous year. If you go to cohorts, and the previous year, and then you go to midterm project – in week ten, we show you how to use SciKit Learn pipelines. This is an amazing concept. So far, what we've been doing is Dictionary Vectorizer (dict_vectorizer) and then it was followed by a model. In the case of Bento, we needed to save this Dictionary Vectorizer separately, as an artifact (as a binary) and then we needed to load this and then we needed to apply the model. But with a pipeline, you can actually train a pipeline and just use BentoML to save the pipeline. It can be saved, right Tim? You can just take a pipeline and do BentoML.sklean.save_model and then the entire pipeline will be saved. Right? [Tim agrees] This is a cool thing. I don't have an example, but you can include this feature scaler here in your pipeline. There should be an example somewhere. I think we did that last year. There is an example without the pipeline. It should be here – classification, and then notebooks scaling this one. In this notebook, we show how to scale your features with standard scaling, or min/max scaler. If you check Office Hours #3 from last year I spoke more about that. It is an important topic. We intentionally did not include this because we cannot include everything, but this is an important topic. I recommend that you check it out. And using pipelines is cool because you will not need to save all this Scaler, Dictionary Vectorizer – all these things separately. Let's say with Bento, you can just take one pipeline, save it with Bento, and then you will have one binary on model. It makes things easier._x000D_
Tim _x000D_
And if you do have any kind of custom Tokenizer, or Scaler, or Vectorizer – you can always add it to that “custom objects” field._x000D_
Alexey_x000D_
We talked about MLflow. It's very annoying, to be honest. With MLflow, I need to choose to save this in a file. Then I have a Pickle file and I tell MLflow, “Okay, there is an artifact that is already in this file that I want to upload together with the model.” So I upload it together with the model and then when I want to use it, I need to download this from the registry, then I need again to use Pickle to load this into the memory. There is too much overhead. With Bento, I really like how easy it is to save extra stuff._x000D_
Tim_x000D_
Right. In our first version of BentoML, we were just saving models but then we had so many users saying “Well, how do I save my Tokenizer along with the model?” A lot of the time, the Tokenizer, the Vectorizer, is sort of one-to-one with the model. You may have a version of that as well. So it's nice to be able to version it with the model."
Machine Learning Zoomcamp;2022;What to do when the data is too large and comes in real time. Any pointers?;"I believe we have a course that is just about that and that course is called Data Engineering Zoomcamp. So if you want to know what to do with cases when the data is large, check out our Data Engineering course – Data Engineering Zoomcamp. Maybe I’ll answer this quickly. So what to do when the data is too large? There are multiple things you can do. First, you can sample your data – meaning that you just take a part of this data and discard the rest, and you train your model on that. That is fine. Then you can go distributed –you can use distributed computing for accessing all the data, which is more complex. You should have a good reason for doing this because, oftentimes, training a model on a sample is sufficient. But if you know that you will get better accuracy, better performance, if you train on the entire dataset, then you should consider distributed computing – things like Spark and so on that we cover in the data engineering course. _x000D_
If data comes in real time – I don't really know what you mean here. But again, if data comes in real time and you want to score it in real time, then this is something we talk about in the data engineering course. We cover it in the streaming lecture and also in our MLOps Zoomcamp. In our MLOps Zoomcamp, we cover streaming, which shows a case when there is a stream of data, and how we apply the model to this stream. This is a course I recommend taking after the ML Zoomcamp – the one we're doing now. So you can finish this course and then go to MLOps Zoomcamp. When it comes to web service – I don't know if I'm answering your question right now, but we will cover this in deployment. This is for applying the model to your data, for predictions. If your training data comes in S3, in real time, then you just need to collect this data, save it somewhere, and then train. This is where Data Engineering Zoomcamp comes in."
Data Engineering Zoomcamp;2023;To get the certificate, is it only required to submit the final project?;Yes, you don't need to do weekly homework assignments for the certificate.
Data Engineering Zoomcamp;2023;Does creating resources (like we have created Google Storage Bucket and BigQuery using Terraform) have any charges?;I think that once you start using them. You also pay a little bit for requests – every time you make a request, you pay a little bit. That’s to my knowledge. I don't really know how it works in GCP. I usually use AWS. In AWS, every time you send a request, you have to pay a little bit. But I think you will get charged money, like fractions of cents, when you start using it.
Data Engineering Zoomcamp;2023;Google considers MLOps as a part of data engineering. What do you think about this?;"Victoria_x000D_
I think that goes a little bit to the fact that almost none of us except Ankush are data engineers. We're all capable, or we think we're capable, of teaching something that is relevant to data engineers. It's the case where every company has their own world, or roles, and they set it differently. Ankush, you could even share that, at one company you work with DBT, and then at another company that was being done by someone else, right?_x000D_
Ankush_x000D_
Yeah, exactly. Completely, completely different there. Data engineering in its own self has become a very gray area, honestly. There is also a lot of DataOps there, there is a lot of MLOps there, ML engineering is also considered somehow data engineering. The only thing left is data science and I'm pretty sure that will also become data engineering sooner or later. [chuckles] But this is very specific to Google or very specific to a company you're working for. _x000D_
I’ve worked in a company where data engineers do mostly SQL and DBT. There are companies where data engineers are supposed to do MLOps, DataOps, and data engineering. And obviously, others which just focus on data engineering and classify them as streaming data engineers and batch data engineers. It's really very specific to the company you're talking about."
Data Engineering Zoomcamp;2023;Can I change the order and study streaming after the basics and setup? Will it be more difficult than following all the course parts in order?;This question is tricky, because I haven't yet published the homework for streaming. I think it's in progress right now. You can study streaming right now, but you will need to do your homework and submit the homework later, when it's released. But let's say you don't feel like studying data warehousing. Then just don't. I think you should, but let's say that you don't want to study DBT and you just want to do streaming. Feel free to skip DBT Batch and focus on streaming. That's fine. You don't have to do some models that you don't think are relevant for you.
Machine Learning Zoomcamp;2022;If you were interviewing a candidate who has done this course, what would be some of the important questions that you want him/her to give a good understanding of?;"I structure the interviews that I conduct around projects. Typically, a technical interview that I conduct is usually one hour long. The first five minutes is just a bit of introduction, where I say a few words about myself and then you also say a few words about yourself. _x000D_
Then, in the next 25 minutes, we talk about your past project – your past experience. Here, I would ask you to tell me a few words about yourself, select a project, and then ask you to talk me through this project. How did it start? Who was working on this project? What is the main problem that this project was meant to solve? What is the main goal? _x000D_
Then we would go into technical details – which kind of model you selected, why, and so on. Here, I wouldn't ask specifically theoretical questions just out of nowhere. All these questions would be based on your experience – on everything you say. Basically, if you said that, “For this project, I used XGBoost,” then I would ask “Why XGBoost? Why not something else? Was it better? What kind of other models did you try? Why this particular choice? How exactly did you evaluate your model? This is when all the topics from this course would start appearing? “What is the difference between XGBoost and Random Forest? Why did you decide to settle on this particular model? Then, based on the materials of this course, you can answer these questions. _x000D_
Also, I spend a bit of time asking about your deployment experience, which is also something we cover in this course. This would be the first 25 minutes. The next 20 minutes is about live coding. I ask a problem and then we use Python to solve this problem. And then the last 10 minutes is the time for the candidate to ask questions. _x000D_
Maybe I’ll just add a bit of a shameless plug. If you go to my GitHub profile, and then look at the pinned repositories – the first one is data science interviews – you will find a lot of interview questions. For many of these questions, you will find answers in this course, but also in this resource. For example, here’s “What is gradient boosting trees?” and so on. This is quite a useful thing. Also, I was talking about coding for the second half of the interviews I conduct – I might ask some things from here. I typically don't ask SQL. I know many companies do ask this. But I would ask something from this part for the coding part of the interview._x000D_
In the course, we focus a lot on projects. There is a lot of emphasis on doing projects in this course. Perhaps this is what you should pay attention to. Then, when you have interviews after finishing this course, you will probably also talk about the projects you did. This is when all these things might start coming up. Sometimes the questions are trivia questions. By “trivia” I mean that somebody just comes across this list of questions (on GitHub), and they go “Let me flip a coin and ask this question.” Then they just ask this question, “How do we check if a variable follows a normal distribution?” And then you need to answer that. _x000D_
Maybe for this particular question, by the way, you will not find an answer in the course. Or at least partially – maybe we talk a bit about bell-shaped curves. Then again, it's not the end of the world if you don't know the answer to some of the trivia questions. I think in the end what matters is your projects, your portfolio, and not if you know some encyclopedic material or not. So focus on projects."
Data Engineering Zoomcamp;2023;How do you see the future roadmap of Prefect? Is it publicly available somewhere?;"Jeff_x000D_
That's a good question. We don't have a publicly available feature roadmap, probably just because we iterate really fast on things. Every week, you'll see there's about a new release. In this course, and I'll point this out here again for folks, we pinned everything. We pinned our versions in all the work to try to make sure that when people come here in a year, hopefully everything still works well. There’s a much higher probability of that. It’s a good idea to pin your versions a lot of times for your packages, but there's a lot of iteration taking place. We have new views in the last couple of weeks. We have other new features. So if you want to see what's happening and what's the latest, just don't pin it – just do an upgrade – just do a pip install of Su_prefect. Same thing with other packages. Then you’ll see what's happening. We do have release announcements and release notes every week when we release at least once a week."
Data Engineering Zoomcamp;2023;When will the Terraform videos be released?;"Alexey_x000D_
Yeah, there are some issues with the new Terraform videos. I don't know. We will probably finish them before the end of this course. I'm not going to promise anything right now. Let's see. Now the first week is over, so it's kind of late anyways. We'll try."
Machine Learning Zoomcamp;2022;How are you using Ubuntu with Windows and why are you using Windows? I am thinking of dual boot Windows with Pop!OS.;"I have no idea what Pop!OS is. The reason I am using Windows is because I like it. I have been using Ubuntu since 2010, I think. I used another Linux OS even earlier. But I switched to Windows recently (last year) when recording this course, actually (ML Zoomcamp course) because I needed to edit videos and editing videos on Linux is super annoying – I don't know if you have ever attempted to do this. Ubuntu or any Linux is just not the operational system people use for editing videos or any other thing – pictures, also forget about this. So I thought “Okay, how about I try windows?” I gave it a try and a funny thing happened. _x000D_
There is this thing called WSL – Windows Subsystem for Linux – which works exactly like my Ubuntu. And I thought, “If I can have Ubuntu on my Windows, then why do I need Ubuntu?” [chuckles] You see what I mean? On Windows, I could edit videos, I could do many, many other things. So some of the software I use right now, for example, Loom for recording videos, or I use a tool called Krisp for noise reduction, and many, many other tools. They simply don't work on Linux, because the developers don't care about Linux users – they care about Windows users and they care about MacOS users. _x000D_
Now I get the best of both worlds. I can use tools that work on Windows, but don't work on Linux and I can still use Linux and do all the development stuff I've been doing on Ubuntu since 2010. And it's been quite great. I really enjoy Windows. I have Windows terminal, which I can use to either do things in Windows or in Ubuntu. And I can always just SSH to a remote computer and have my Ubuntu terminal there. I am not coming back to Ubuntu anytime soon, at least “full” Ubuntu, let’s say."
Machine Learning Zoomcamp;2022;Can I use a dummy variable result? That is, I have two answers for a variable, but I see that one does not help me much so I only use the most relevant one.;Again, set up your cross validation and then test it on your validation. If your validation says it's fine to drop it, just drop it.
Machine Learning Zoomcamp;2022;I'm just curious, the target audience for BentoML â€“ is it data scientists or is it machine learning engineers?;"Tim_x000D_
It's kind of both. We try to make the deployment of your ML model as easy as possible, whether that's for the data scientists or the engineers. I think we have a lot more detailed features for the engineering and Ops side that maybe a data scientist may not care about immediately. I think it's both. I would say it's half and half._x000D_
Alexey _x000D_
There’s an interesting thing. This course is based on a book and when I was writing this book, I had a software engineer who wanted to switch to machine learning engineering in mind. Then we ran the course, and in the forum there was this field poll, where I asked everyone who signed up for the course to specify what they're currently doing. Funny thing was, most of the people (most of the students) were data scientists. It was a surprise for me. I was expecting software engineers to join this because this was the target audience, but the reality was different. I guess maybe for you, something similar happened, right?_x000D_
Tim _x000D_
Yeah, I think so. I'm guessing we see probably more engineers than your ML Zoomcamp does. An engineer like me is not going to know much about training. And I think that's one of the questions down below. I think it's good to know the concepts of training and that's why I participate in ML Zoomcamps like this. But as you scale the team, I think you learn to specialize a little bit more. It's just like a small company versus a big company – if you're a small company you wear a lot of hats and then the bigger you get, the more people you get to hire, and you wear fewer and fewer hats. That's just the nature of being either a small or a big company, it just depends on what you like._x000D_
Alexey _x000D_
Maybe I'll also add to this question. Even if you have a team where you have ML engineers, backend engineers, other engineers, data engineers – there could still be cases when they're busy with something else because they have other priorities. In our case, it was a big monolith and they were very busy decoupling it and creating microservices because it was very important. But I was annoying them with deploying the model and then they simply did not have time for that. Sometimes it may happen that even though you have people on the team, they cannot necessarily jump on this problem right away. Being able to deploy a model is helpful. It doesn't have to be super resilient, super available, super reliable, but if it's deployed (if it can be used) then it's already a good thing. And tools like Bento help a lot with that. They help data scientists to just do a good enough version that doesn't break and when engineers become available, then they can take care of tuning it._x000D_
Tim _x000D_
Right. Even when the team gets big enough, engineers will hand the deployment over to DevOps as well. As you get bigger, you get more and more separation and specialization."
Data Engineering Zoomcamp;2023;How do we decide what to put in a Docker container and what doesn't need one?;I don't know – scripts? It would be nice to have an example. Usually, your longer jobs would require a container. It also depends on where you will run these jobs later. A typical scenario, for example, is using something like Kubernetes or AWS batch for executing these jobs. These environments require a Docker container. There, basically the rule is – if you want to deploy anything there, you have to put it in a Docker container. Another alternative could be, for example, if you use Prefect, there are two options. One thing you can do is just do all the work in Prefect, so Prefect will execute this thing. Or another option is to use Prefect as a simple orchestrator and delegate the execution to something like, again, Kubernetes, AWS Batch, Spark, and so on. In this case, if you use Prefect and the agents have all the required libraries, you can just use Prefect for executing things. You don't necessarily need to put these things in a Docker container. But if you're going to use Prefect only for orchestrating and the actual jobs will run in some external environment where a container is expected, then yeah, you Dockerize everything. If you have some examples in mind that you want to talk about, let's do this in Slack.
Machine Learning Zoomcamp;2022;I just entered this Zoomcamp today and I wanted to ask if there is a problem that I'm late. Would submitting the homework late be a problem for you?;"In summary, “Yes, you can join late. Not a problem. If you care about the certificate, it will not impact your chances of receiving one. All we care about for that are projects. We will have three projects. If you complete two out of the three projects, then you will receive a certificate.” _x000D_
For this part “Would submitting the homework late be a problem?” It will be a problem in the sense that you will not be able to do this. Once we close the form, the form does not accept any submissions, so you will simply not be able to do this. There is little point in this, because once we close the form, we also publish the answers. Theoretically, after this, you can just look the answers up in the solutions and then put it in the form. Then it kind of defeats the purpose. _x000D_
Submitting homework assignments after the due date will not be possible. It doesn't mean you will not be able to get the certificate. Let's say if you're joining this course in November, then you can binge watch everything until the end (I know some people did this last year) and then you just focus on projects for the remaining time and then you get the certificate at the end anyway."
Data Engineering Zoomcamp;2023;Do we need to know the basics of Java as well, not only Python?;No. From what I understood, from what Ankush said, you don't need to know Java.
Machine Learning Zoomcamp;2022;When it comes to a simple decision tree versus logistic regression, how can one decide which model to apply? Is there something in the data that tells us that one would work better?;Yes, there is a thing called the validation dataset that you can use to guide your decision. That's the best thing you can do. Another thing you should take into consideration is time. Let's say that we have a lot of categorical variables, then fitting a decision will take more time than a logistic regression. Then applying it will also take more time. So this is something you should also consider in your experiments. But in the end, you probably want to use the validation set to guide you.
Machine Learning Zoomcamp;2022;How should I choose the number of layers and networks within a layer?;I guess, use validation.
Data Engineering Zoomcamp;2023;If I use Airflow, will it be able to complete homework for week 2 and the following weeks?;Yes, more or less. Some parts will require Prefect, but I think you will still find a way of getting the answers without Prefect. I believe so. So technically, yes – it's possible. But then again, we do not officially support Airflow in this iteration. Other students who use Airflow for homework for week 2 can help you and most likely will help you. We also have some frequently asked questions from the previous iteration. Do this if you know what you're doing. If you're just learning right now and you don't feel very adventurous, then stick to Prefect.
Machine Learning Zoomcamp;2022;I would like to create a portfolio of projects to get experience and showcase my skills. Can you recommend this strategy to make up/find projects?;"Yeah, I can. First of all, you’ll have to do projects here for this course too. You can treat them as real projects – something that you will show to potential hiring managers. When you do this, keep this in mind – think of it as though you're not creating this project for this course. You're not creating this project for your peers to evaluate. Instead, think of this as if you're creating this for a hiring manager – one who will be looking at your project and trying to evaluate it. Keep this in mind when you work on this. Add as much documentation as possible, as much context as possible. _x000D_
The hiring manager might not be aware of the course and they might not know some things, so give as much context as they need to figure out what's going on. I don't think this is really the answer to the question you asked. I think it will definitely showcase your experience and skills if you write very good documentation for your projects. Concerning strategy – just find a dataset. Go to Kaggle, where you will see a news feed. For example, here’s a Jupiter Network Global AI challenge. You just open it up and look at what people do here. You will see some data._x000D_
[image for reference]_x000D_
You can see what people work on and see what it’s about. In this example it’s “Predict hourly sales for cash registers across a retail chain.” This looks like a cool project. So how about you get this dataset and work on this? If you run out of ideas, just open Kaggle – you will see what people are working on and you can work on the same thing. It’s as easy as that. You can also spend a bit more time trying to figure out what you like and then find a dataset about that. As I said last time, the best thing is trying to think what is important for you, and then getting a dataset for that – collect the dataset yourself. _x000D_
If you have problems with that, just go to Kaggle and pick whatever people are working on. Here are some other projects [scrolls through Kaggle] Hourly energy consumption – looks pretty interesting. Data science job salaries – also pretty interesting. Any of these things can be used for a project. Again, Kaggle is not the only place where you can find data. Just today, we released a dump of our Slack into a special channel – I think it's called “slack dump”. There was an announcement in the general chat. You can also check it out, maybe do some analysis of that, and see what you can find there."
Machine Learning Zoomcamp;2022;Will we see deep networks with tabular data instead of image classification or is it not very recommended?;Yeah, you can do this. The sky's the limit. Again, use a validation dataset to find out and then remember about the trade-offs. The trade-offs are speed, I guess, and the complexity of your model. You need to take that into account and then use the validation framework that you set up to make the best decision. So use data to make the decisions.
Machine Learning Zoomcamp;2022;Where can I find resources/materials for this program?;I'm glad you asked. MLZoomcamp.com. All the materials are here and you will find them there.
Machine Learning Zoomcamp;2022;Alexey, you should train some moderators to help you on Slack. Share the load â€“ otherwise in the long run you might lose the joy of doing this.;I don't particularly enjoy moderating Slack, to be honest. [chuckles] So if you want to share the load and help me, please reach out. I will definitely share the joy of moderating Slack with you. That's a good suggestion. Thank you.
Machine Learning Zoomcamp;2022;Would gradient descent give completely different weights than the normal equation?;No. They shouldn't. They should give roughly the same weights. At the end, the minimum of your loss function is the same, but gradient descent and normal equation arrive at the solution differently. And then at the end, the solution should be the same.
Data Engineering Zoomcamp;2023;What is the difference between Prefect and other ETL tools, like DBT and Matillion?;"Alexey_x000D_
I wouldn’t call DBT necessarily... It is kind of an ETL tool, but more like a transform tool. It doesn't do orchestration. Matillion – I have no idea what that is. What would be interesting is to compare Prefect to things like Airflow, Dagter, Flyte and other similar things._x000D_
Jeff_x000D_
Yeah, I think people will learn about DBT here in week 4. You're gonna get to see how DBT works and get more experienced with that. Things like Airflow are potentially interesting to compare to. Kalise, you probably give that spiel more often than I do, if you want to do it._x000D_
Kalise_x000D_
Yeah. Concerning Prefect, our founder was actually a main contributor to Airflow. It was really the vision of Prefect as an improvement of Airflow on workflow orchestration. Having data passing between tasks and being Pythonic as a first-class citizen feature is really important. I really suggest the article, Why Not Airflow? It was written quite a few years ago, but it actually touches on a lot of points of what Prefect does versus Airflow as well. In terms of just a lot of ETL tools, workflow orchestration can do more than just ETL as a whole. If all you are really doing is ETL, or transformations, or something very particular, definitely use the tool that's right for you._x000D_
Alexey_x000D_
I think that there was a blog post from your colleague recently, from Anna, about something similar. Was there?"
Data Engineering Zoomcamp;2023;How can I add Python ingest data to the compose file as a service for everything to work together by only typing â€œcompose upâ€?;"Alexey_x000D_
I guess you refer to week 1, right? You do it in the same way as you do other things. There is one nuance,  one thing that you need to keep in mind, is that your service needs to wait till other things are ready. Perhaps you will need to add a bit of code there for that. If anyone knows a good example, please let us know and share it in Slack."
Machine Learning Zoomcamp;2022;I see people referring to â€œApplied Linear Statistical Modelsâ€ (Is it a book? I don't know this one) that says collinearity does not affect predictions. Is it true? Only if not severe? Depends on the model?;"Yeah, it is true. If not severe? Yes, it does depend on the model. For example, for linear regression that we implemented ourselves in week 2, you saw that it does affect it. Our model simply could not work unless we added regularization. But when we added regularization, we fixed this. Collinearity was no longer an issue. So it is true and not true at the same time – it depends on the model. _x000D_
For tree-based models that we are covering right now in this week's materials, collinearity can also affect how we train our team models. If one feature and another feature are exact replicas of the same, then the tree would randomly select one of them. At the end, the model will probably be the same, more or less, but there are nuances. Sometimes it doesn't play well. _x000D_
For example, for random forest, when we select only a part of the feature set, the column that is a duplicate of another column – this feature simply will have more chances to get in a random set, which will affect the quality of the model, but probably not significantly. So I wouldn't say it's a big problem, because we have regularization, because we have models like trees that aren't really affected that strongly. But again, I don't think it's a good idea to have collinear features that correlate. Simply because – why do you need them? Your model will be simpler if you don't include them, so don't include them."
Machine Learning Zoomcamp;2022;What's the difference between Docker and Kubernetes?;Docker allows you to run a Docker image – a set of prepared instructions – this is what you put inside a Docker file. Then like when you do Docker build, Docker takes the Docker file and then creates a Docker image and then you can run this image. You can run this image with Docker locally and that's fine. But what Kubernetes does is it allows you to run many different Docker images. Kubernetes is a container orchestration platform. It allows you to take all these Docker images that you’ve built on your local machine with Docker and it allows them to execute somewhere in a cluster. It doesn't have to be on your computer. It can automatically provision more machines, if you're running out of machines and all that. It's not 100% correct what I will say, but you can think about this as “Kubernetes orchestrates Docker images.” You have some images and then you just put them in Kubernetes, and Kubernetes figures out on which machine to run it, how many resources to give it and then it just runs (orchestrates) all these things. So you can think about this as Docker is for local use and Kubernetes is for deployment in the cloud.
Data Engineering Zoomcamp;2023;Will there eventually be instructions on how to deploy Prefect with Terraform (or will this be necessary)?;"Michael_x000D_
It's not necessary. We do have recipes for using Terraform with Prefect with different things. We talked about the Prefect recipes repo, but I'll put a link to it here. _x000D_
Jeff_x000D_
This is in a docs and we have a GitHub repo where we have lots of different recipes. Some of them here explain things like, say you want to run an agent – how can you do that? There are so many different cloud environments. You can put things on Kubernetes, you can put things on something like a more serverless platform like Fargate. For example, on the right there, if you were wanting to use AWS with Prefect and ECS (elastic container service) then with Terraform – we have some recipes for that. So there's just so many different permutations of possible ways to do things. A lot of these are helpful ways to go and look at options, but it depends on what you're using in the real world (in a use case). I don't think we have anything specific for this course that requires us doing this, so we don't have anything further for it._x000D_
Alexey_x000D_
I guess, the usual setup that I saw (not necessarily Prefect-related) you typically use Terraform for setting up infrastructure or things like a Kubernetes cluster and configure it there. But then there is a separate repo with configuration for Kubernetes that is not maintained/managed through Terraform but through something else. For that, you don't really need to Terraform – and then Prefect could live in this Kubernetes cluster._x000D_
Jeff _x000D_
Yeah, we do have some guys working with Kubernetes in here, for sure. I see some of them here. We do have a Helm chart that we can use. If you're into the Kubernetes world, there are lots of things there. It's a whole rabbit hole._x000D_
Alexey_x000D_
Here are the Prefect Helm charts._x000D_
Kalise_x000D_
Also, there are a lot of YouTube videos we have that can help if you're trying to use some of these resources – how to go about deploying the Helm chart with Kubernetes and stuff, like step-by-step tutorials as well. Generally, if you just go to the Prefect YouTube channel, you'll see a lot of those tutorials. There should be some playlists there that have various things._x000D_
Alexey_x000D_
Is this a duck in the cover or what?_x000D_
Jeff_x000D_
That is. It's the top of a duck. Blue ducks!"
Data Engineering Zoomcamp;2023;Is it possible to use Tableau for week 4 instead of Google Data Studio?;Yes, you can do this, especially for your project. For your project, you can choose any technology you want. It could be Tableau instead of what we've covered.
Data Engineering Zoomcamp;2023;Can you please explain why the Parquet format is better than CSV?;"Jeff_x000D_
Yeah, I can talk about this a little bit. I actually was just reading about this yesterday. I had a book in front of me and I was like, “Oh, I want to read it right from the book. Maybe I'll copy/paste it when I go grab it.” I like the Fundamentals of Data Engineering book by Reis and Housley that just came out recently. I recommend that. It is a great book that gives you a lot of background in data engineering. It has a good description of Parquet, in particular, and why it's useful. It can be compressed a lot more than a CSV file. CSV is just kind of this not very standardized way of doing things and they take up a lot of space, even if you do compress them. Parquet is a columnar format and it's very useful for more efficiently storing data, and then you can compress it, oftentimes with Snappy compression and make it even smaller. So it's a fast way to read and write data and not take up tons of space. There are other file storage methods that are even more efficient for reading fast and writing fast. But it's very good for a balance of ways to store things and it has some metadata about the columns in it that goes along with it. That's just really useful for a lot of things like putting it into a database, for example. So it's a winner. I feel like it's pretty much the standard way, if you're just going to store something in a file persisted to disk and just gonna be sitting on a disk. Parquet has become the most popular way to do that with data. But other people can feel free to jump in and add on or disagree._x000D_
Alexey_x000D_
Which chapter was it? Sorry. _x000D_
Jeff_x000D_
It's in the appendix at the end._x000D_
Michael_x000D_
I would also add that with Parquet, you can also do partitioning to decide where the files are written to. When you read the files back in, it's not going to read all of them like you would with a CSV. Also, Apache Delta Lake is built with Parquet files. It just uses extra JSON files to track the changes, so you can just take object storage like S3 or a GCP bucket, and instead of making a full database, it just writes the Parquet files, but then you can interact with them like it's a database, which I think is just really powerful._x000D_
Jeff_x000D_
Parquet is cool. Use Parquet. [Michael agrees]_x000D_
Alexey_x000D_
We had one issue though, with Parquet. Michael, you should know about that, because we came across this issue again recently. You have to be careful with the schemas there, because once you say that a particular column is flawed, and then if in another Parquet file, it's not flawed, but it's integer – then you might run into problems. So you have to be careful with the schema. Meanwhile, in CSV, it's just text and then you kind of convert to the proper type as you read, not as you write._x000D_
Michael_x000D_
Yes, so I would say that you need to be very explicit with your schema definitions when saving your files. If anyone is interested, the NYC data – I know they updated the Parquet and there are many files with many issues in there, which is great to practice your data wrangling (if you want some punishment [chuckles])."
Data Engineering Zoomcamp;2023;Could the course be a little flexible by adapting other clouds besides GCP?;For the course material, we cannot re-record it for other clouds. If you know what you are doing, by all means – use other clouds. For your projects, you can do whatever you want.
Machine Learning Zoomcamp;2022;As a junior data scientist, what do you consider to be the most important skill or skills (top 3 three)?;I mentioned cross-validation. Another thing I ask when I interview people is about projects. Usually, I ask, “Why did you do this project?” If I get a good answer to this question then it's a very good sign. Here, what I mean to say is – explaining why you did certain things is really helpful compared to a validation framework. So communication is the skill I mean when I say “explaining things. And I guess coding in general is the third one.
Data Engineering Zoomcamp;2023;How to get a data engineering role for non-IT people with a two year gap? Please suggest any project.;"Alexey_x000D_
Well, I can suggest a project for this course. At the end, after you complete this course, after you complete all the modules, as a part of this course, you will have to work on the project. This is how you will get a certificate at the end – if you pass the project. This is a good project to include in your portfolio. _x000D_
Michael_x000D_
I don't know if I have much more to add other than being active in online communities. This course is a great place to start. There are a lot of large cities that have meetup groups, which are always great. One of the good side effects of the whole COVID pandemic is that a lot of those are remote now. If you Google “San Francisco Big Data meetup,” meet people, talk with them, get their insights. I think that'd be a good way to approach that._x000D_
Jeff_x000D_
Plus one just for getting involved in communities, whether it's conferences, online or in-person is great. Volunteer, if you can, at those. That's often a great way to help out and work with people. Go to meetups. I run a meetup in DC on data science. If anyone's in the DC area, there are data engineering meetups in town too. If there isn’t one somewhere, and if there's a few people around, I encourage you to start one. That's something you can do, regardless of your level of experience._x000D_
30:06  _x000D_
Alexey_x000D_
Then I'm going to do another shameless plug. We have a podcast and the topic of career changing comes up pretty often there. One interesting one is from Juan Pablo. Here he's actually not talking about… well, it's kind of related to analytics engineering. He was a math teacher, which doesn't really qualify as an IT person. He was also working as an Uber driver. In the podcast, he tells his story of how he actually did this. He talks exactly about that. He talks about going to meetups. Instead of not talking about this, just go and check it out. That's not the only relevant podcast episode. You can just go through everything we have and see if anything catches your interest. I'm sure you'll find a lot of interesting stuff."
Data Engineering Zoomcamp;2023;How many percent of people successfully completed the course last year?;Percentage-wise, it's hard because a lot of people signed up, but very few finished. I think we had slightly less than a hundred graduates. But this course is tough time-wise as well. It's okay to take your time for some of the weeks. You might be a bit behind, but maybe what you should do is focus on learning and not on getting a certificate, especially if you have work, if you have family, if you have other commitments. In the feedback form, some people said that it was difficult for them to actually finish the course in addition to other commitments. But what some of them did was take the course and finished it at their own pace and thus still successfully completed the course. But they are not part of the statistics that I shared.
Machine Learning Zoomcamp;2022;You have consistently used .predict_proba instead of .predict for the models from SciKit Learn. Is it because you wanted to use auc-roc?;Yes, also because of that. But usually, typically, I'm more interested in probabilities than in hard scores. For example, this model that we just studied – the risk score. Instead of just saying, “Okay, this user (client) is going to default,” it's much, much more useful to know, “What's the probability that this user is going to default? How risky is this customer?” We have a number that’s not just binary – risky or not risky. There are multiple gradations of risk, let's say. It could be completely not risky, somewhat not risky, moderate, somewhat risky, and risky. It could be five gradations or something like this. Then it's more granular for people who make the final decision and decide if they should give the credit or not.
Machine Learning Zoomcamp;2022;I am used to train_test_split() from SciKit Learn but in our example, it was split/train/test/val. Could you explain the difference between these two approaches?;I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
Data Engineering Zoomcamp;2023;Can we move the homework deadline to January 30th instead of 26th? I had a hard time working on these over just one weekend and need another weekend to finish.;As of today, January 26th, 2023 – it's likely that we will extend it until the end of the week. But this doesn't mean you should put this off until the last day. If you do this, it's likely you will be late and we're not going to extend it a second time. Try to make it happen earlier. If you can't, we’ll see.
Machine Learning Zoomcamp;2022;What to use: MLflow, Kubeflow, SeldonCore? No preliminary requirements defined.;To me, these tools focus on completely different areas. You can use all of them, actually. MLflow is about experiment tracking. Kubeflow is about creating model pipelines (if you’re talking about Kubeflow, because there is also a thing called KServe, which used to be a part of Kubeflow but now this is a framework for serving machine learning models. It's very similar to some SeldonCore). That you used for serving models. All three of these tools can actually play well together. What I would suggest you take, if you want to learn more about this and if you're interested in this – there’s another course that we have that you can take after this one, which is the MLOps Zoomcamp, where we talk about MLflow (in module two). Then we talk about allocation and machine learning pipelines. We don't use Kubeflow pipelines here, we use Prefect, but the concepts are pretty similar. We also talk about model deployment, which is kind of similar to SeldonCore and KServe. Also remember that we have a module about KServe in this course. It’s actually an optional module. You don't have to take it and it's not graded if you take it, because it's pretty advanced. I think it's slightly outdated, too. But recently, I was going through this module and all the things that didn't work – I fixed. So it's actually up-to-date, but it might be a bit outdated. You can check this one out too. SeldonCore is pretty similar. The main difference between KServe and BentoML is that for KServe and for SeldonCore, you have to have Kubernetes. Bento works without Kubernetes, but for this, you have to have Kubernetes. So you need to know Kubernetes at least a little bit. That's why we cover Kubernetes and then we cover KServe in addition to that. You can also deploy models with MLflow, but it's not the main focus. I wouldn't use MLflow for model deployment. To me, it's more of an experiment tracking and model registry tool. If you want to learn more about that, please check our MLOps course.
Machine Learning Zoomcamp;2022;Are there passing points for the project (how many points you need to get in order to pass the project)?;"Yes, indeed there is. You need to get something like 50% in order to pass the project. I don’t remember the exact score we used last year. This is how we did it last year – we looked at the distribution of the scores. We didn’t want to mark many projects as failed, so we looked at the distribution and we made sure that most of you passed the project. I think the passing score was pretty low. Most of the students who did the project – who did put some effort there in doing the project and who met at least half of the criteria or most of the criteria but partly – they passed the project. It wasn't a big problem, so don't worry about that too much. _x000D_
As long as you put in some effort, then you'll pass it. The people who didn't pass last year submitted empty projects, or projects that were copied from somewhere, or projects where they just added some readme file and maybe copied some Kaggle kernel and that was it. So nothing was done in addition to that. There were people who passed, but they just didn't put any effort into attempting to do the project. If you put in some genuine effort, don't worry, you will pass the project."
Data Engineering Zoomcamp;2023;How should we provide you with the homework assignment results?;I will show you. In the Data Engineer Zoomcamp, we have the cohorts folder, and then we have the 2023 folder, where you can see the homework. Actually, there are two homeworks. I think we need to change that. I'll just set a reminder for myself. This is the first homework, and then there is another one here. There are two homework assignments for week one. One is SQL. You go through these questions, and then you submit the answers using this form. And then you do the same for that TerraForm homework. There is a form, you submit the answer here. It's just one question. This is how you do this. After the deadline, we grade the homeworks and you will see the leaderboard.
Machine Learning Zoomcamp;2022;How many portfolio projects apart from the course are needed for getting a job?;"Again, you’ll probably hate me soon for saying this, but the answer is “it depends”. Maybe it's zero, maybe it's one, maybe it's two. You never know. You just need to start interviewing and in parallel to that, get projects done. Maybe you’ll get lucky and get hired from the first interview. Probably not, but you will already start learning what companies need. Then, at the same time, you try to implement this and you see, “Okay. This is what companies care about. Let me use some of the technologies they want to see that I’ve used.” You can look at the job descriptions to figure out what is important. You can talk to them when you have interviews and you can ask them, “Hey, what kind of technologies should I use for my portfolio projects to be a good fit for this position?” for example. _x000D_
It never hurts to ask. And keep doing this. Maybe you will get a job on the third project, maybe to be on project zero, when you haven't even started doing this. I think two or three should be enough, but yeah. When I got my first job in data science, I had been working back then as a freelancer already in data science. I had some projects from past clients that I showed. But in that interview for the job I got, most of the time was spent talking about my Master’s thesis, which was about processing Wikipedia data. I was working with mathematics there and the interviewer for that job was really interested in this. So we spent most of the time talking about that project. So maybe a good answer to this question would be to have one project that is relevant for the company you interview, and then you will just spend most of the time of the interview discussing this project."
Data Engineering Zoomcamp;2023;What is the relationship between Prefect agent and Prefect deployment? What is Prefect Profile? Could you explain it in more detail?;"Jeff_x000D_
The Prefect agent is what kicks off your workflows. It says here, “Go use the infrastructure as specified in the deployment.” So it's constantly there, it's pulling for work. The deployment is this concept that has all the information that is needed in order to actually run a workflow. It will have information such as a way to find your code on your own infrastructure, so your flow code has to be available somehow. Again, it's not stored on Prefects servers if using Prefect Cloud, it's not stored in Orion – it is stored on your infrastructure. _x000D_
And then in the deployment, you’re also going to have your storage, you're gonna have your infrastructure, and you're going to have just some other metadata that could be needed there. But the deployment gets put into a work queue or now a “worker pool,” as we're calling them. And the agent pulls that queue, looking for work. So when there's a scheduled flow run, then an agent picks that up and says, “Alright, now go run the code that’s specified in the storage in this infrastructure that’s specified.” _x000D_
The Prefect Profile is a nice way to just flip back and forth between different workspaces, if you're working locally. With a Prefect server, that's great if you're hosting that. Then you can also use the profile to switch to the cloud workspace. And if you have multiple different cloud workspaces, you can specify those with a Prefect Profile. I'm just gonna go to the docs here and search for profiles, which is under Settings. This is where you want to go. You can check out information about Prefect Profiles, and how to use those, where to switch back and forth. There’s profile files down there, so a number of things. The two most common things I use are Prefect Profile LS Prefect, in order to list the profiles and Prefect Profile use in the name of the profile I want to use to switch back and forth between the profiles."
Data Engineering Zoomcamp;2023;How long is the course duration?;It's like 10 weeks or something like that. If you count the second attempt of the project, it will be like 13 weeks. Quite long, but not so long as, let's say our machine learning engineering course.
Data Engineering Zoomcamp;2023;Depending on the passing certification after the project, can we expect some opportunities (jobs)?;We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
Machine Learning Zoomcamp;2022;How can we thank you for teaching us or how can we help your project to develop further?;"Well, you can just write thank you – that's already good. Also, I want to thank you for helping each other – that's also great. Please keep doing this. This is really awesome. Then there are also many other things you can help in the community. For example, I need some help with preparing for podcasts. Or… There are many things. Maybe if you're interested in volunteering for some things, please write in Slack. _x000D_
If you just want to sponsor in GitHub, there is also a way to support me with like $5 per month. If you go to my profile, there should be a link, Support me on GitHub. You can decide if you want to support me monthly, or just with one-time donations. So that's an option as well. _x000D_
Another thing – I don't know where you work and what your company is doing, but if you work at a company that might be interested in hiring interns, maybe you can help us get in touch. Last year, we had a great experience with a company called Delphi. We collaborated (partnered) with them and they took two graduates from the Machine Learning Zoomcamp as interns. They're really happy with the result. The students are also happy, of course. So if you can talk to your employer and suggest considering this, that would also be awesome. Or if you have some other ideas, please share them."
Data Engineering Zoomcamp;2023;Can Prefect be deployed to any Kubernetes platform like GKE or EKS?;"Jeff_x000D_
Yes, absolutely. You can certainly deploy Prefect to those platforms. We also have a Helm chart that you can use. So you can check that out if you'd like. We do have people who run Prefect in lots of different workspaces, so it’s definitely possible."
Data Engineering Zoomcamp;2023;Following week 3 videos, but whenever I try creating a table, I get an error related to data types (for example, â€œpassenger count does not match double typeâ€). Any tips?;"Alexey_x000D_
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
Data Engineering Zoomcamp;2023;Are the tools used for the course free (such as GCP) or do we need to use any premium stuff?;No, all the tools are free. That's actually why we use GCP here, because they can give free credits.
Machine Learning Zoomcamp;2022;I am working on a Kaggle challenge with a topic that is interesting for me. Is it possible to use this work for upcoming projects?;Yes, it is definitely possible. By all means, please use it. You will have to add extra stuff, of course, like you will need to cover the deployment part. But it's completely up to you where you get data from. If it's a Kaggle competition, it's a Kaggle competition. But since this is your code, you’re not stealing it from anywhere, or rather “copying” not stealing. If you don't, I suppose, use this code for any other course, then it's totally fine.
Machine Learning Zoomcamp;2022;Any production model that is really important for industry?;I don't know what this question means. Everything we study here in this course is important for industry.
Machine Learning Zoomcamp;2022;Data Analyst projects answer business problems or provide business insights. Pair your learning with Business Analysis.;Yeah. Thanks a lot for adding that. Pair your learning with business analysis. That's always a good suggestion. If you’re already working and have some colleagues with experience that you want to get, just talk to these colleagues. That's always a great idea.
Machine Learning Zoomcamp;2022;Alexey says â€œNot surprised that the XGBoost model is the best for tabular data.â€ What is the alternative type of data if it's not tabular?;Image data, text data, time series (to some extent) although it is tabular. So images and text are mostly the ones that come to mind when I talk about non-tabular data. In module eight, we will see what to do with images and with NLP, you can check out a lot of resources on the internet on how to use neural networks, or how to use other things. Because if we use traditional methods, like the count vectorizer that I showed you in the Office Hours, it will still take non-tabular data and turn it into tabular data. But with neural networks, it does a bit more than just that. It's more complicated – more advanced. You can find a lot of examples on the internet.
Machine Learning Zoomcamp;2022;Is the attempt more important than the accuracy/recall of the model? The dataset I'm using is proving difficult to get a good recall value.;Yeah, I think this is what I just said. Yes, it is more important. Some datasets are just inherently difficult to deal with, like click prediction, fraud prediction – with this kind of data, you usually cannot have a very good score. If you have a very good score there, then something is probably wrong. Maybe you are overfitting or you have some sort of leakage or whatever. If I see a model for click prediction that is super accurate, it will be very suspicious.
Data Engineering Zoomcamp;2023;Will there be any mention of CI/CD?;Mention, yes, but we will not actually cover it. If you're interested in CI/CD, we have another course which is the MLOps course where we'll cover it. The course will start again in May. This is the “best practices” one. In best practices, we have things like best coding practices, as well as infrastructures, code, and CI/CD. You can check it out. Here, the focus is on one of the things we cover in the course. So just taking these modules by themselves alone, I'm not sure how useful that is. But if you take the course, it should be useful. However, we don't cover CI/CD in this course (DE Zoomcamp).
Data Engineering Zoomcamp;2023;I have used Python for scripting before but not full-on development. Is it a good Idea to pursue this course?;Yes, definitely. Many people who did Python took the course and succeeded at the end.
Data Engineering Zoomcamp;2023;The retries of tasks can be notified? Or do we have to do this logic manually?;"Alexey_x000D_
I guess this is related to Prefect, right? Please ask this in Slack. Jeff is monitoring Slack, so he will answer that."
Machine Learning Zoomcamp;2022;In homework three, it does not look like we are using the test dataset at all to answer the given question. Please advise if I missed something.;Yeah, if it doesn't look like we're using it – we are not using it. Maybe we just want to set it aside and forget about it.
Machine Learning Zoomcamp;2022;Maybe we will see time series in some module? Or could you recommend a book for this topic?;This comes up pretty often. First of all, I will recommend going to our YouTube channel where you will see this talk from Kishan. Actually, in this talk, I already saw a book I was about to recommend. This book is called Forecasting Principles and Practice. This is the book you want to read if you want to learn about time series. It's in R, but that's fine. If you're doing time series, you probably want to stick to R anyway. But many things work in Python as well – for example, this exponential smoothing. You can implement it in Python yourself. This is like the easiest, in my opinion, the most simple method for doing time series forecasting. You can implement this and it's actually quite fun to implement and tweak it to see how it works. So if you have some time and want to learn more about time series, try to implement this exponential smoothing.
Machine Learning Zoomcamp;2022;The exploratory data analysis should not be performed before the data split? In case we need to handle null values, for example.;"This negation makes it a bit difficult for me to understand the question, “exploratory data analysis should not be performed before the split.” Technically, again, coming back to what I said, you set aside your test data and you pretend it's not there. You just forget about it. The only reason why you want to use your test dataset is to test your model, which happens once in a blue moon – very infrequently. Then you can set this aside, you pretend it was never there – you don't know about its existence – and then you can do whatever you want with the remaining data. You can do exploratory data analysis on the train and validation combined, you can do exploratory data analysis only on train, you can do pretty much everything. _x000D_
Typically, I think the right approach would be: you set aside data for test, you forget about it, then you take what is left, you split it, you do exploratory data analysis on train. This would be the right textbook approach, I would say. Then if you need to handle null values, you handle null values. You explore your full train dataset and you see, indeed, there are some null values. This means that for your preprocessing step, you need to do the same thing for the test dataset. Basically, everything you do to prepare your data for your full train – you need to reproduce it to repeat it on test as well."
Data Engineering Zoomcamp;2023;How does BigQuery compare to something like Snowflake?;"Victoria_x000D_
I think it's also worth putting it as, it’s great, but why is it chosen as a cloud data platform? I saw that a lot of people were also talking about AWS and things like that. There’s even a channel of people that want to use Snowflake instead of GCP. The main reason why we chose GCP is because it has a very generous free tier that's $300. In the case of Snowflake, for example, it’s limited to one month. In the case of GCP it’s until you finish the money – you can use the backups and a lot of people use the virtual machines, and you can use BigQuery. It will cover all of that and you won’t spend anything from your money, which is the idea of the course. That's why we chose BigQuery. _x000D_
But the main goal that we have here is that you can apply the knowledge of a cloud data warehouse to any kind of data warehouse. The same with Prefect, you can also apply it to Airflow if you start working at a company that uses Airflow. Because at the end of the day, you should know how a cloud data warehouse and data warehousing works. In the case of specifics of BigQuery to something Snowflake – both our cloud data warehouses from my experience. BigQuery has a different way of connecting – it goes through an API – whereas in Snowflake you use the more well-known SQL. They both have their flavors or SQL. _x000D_
Big Query also has differences, for example, they call databases “projects,” they call schemas “datasets,” and things like that. So it changes terminology. They also have a different approach on cost. Something like, select, start, limit 10, for a BigQuery table could cost you a lot of money. But they tell you before that, how many bytes they'll scan and things like that. It’s just the smaller things, I would. Ankush, you're also very familiar with BigQuery. I've always used Snowflake or Redshift more, or Microsoft._x000D_
Ankush_x000D_
I think the biggest reason for choosing BigQuery was that it just comes with Google Cloud Platform and it's free to use. That's one thing. I think, overall, Snowflake is generally more expensive than BigQuery, at least in my experience. But, of course, they’re super similar. If you look at Big Query optimizations, it's the same concept in Snowflake. _x000D_
The concepts, as Victoria said, are applicable to both the data warehouse solutions. It's just a flavor that we chose just because it's easy to have something like an inbuilt data warehouse where you actually start something on your own. This was one of the big reasons for choosing BigQuery._x000D_
Alexey_x000D_
You're free to use AWS if you want, as well. I think that is also a channel where people want to use AWS for the course. We, unfortunately, will not be able to give you a lot of support there. Because with DBT, for example, I don't know how well it can connect to Redshift or Athena or whatever._x000D_
Victoria_x000D_
Somebody is saying something in the chat. I think I was misunderstood. It says $300 is not a small amount. That's the amount of credits you get for free. You will not pay $300. The idea is that we chose GCP so you don't pay anything. You don't pay anything! [chuckles] _x000D_
Alexey_x000D_
Yes, that's the main advantage. What you need is a Google account – you use it for registering at Google Cloud Platform. They will ask you for a credit card, but only to verify that you're a real human. This card is not used for a different Google Cloud Platform account. They will not charge you anything. To me, Google is trustworthy. I think you can trust it with your credit card. I mean, so far they didn’t violate the trust I have in them. It's a good deal, basically. You should take it. _x000D_
With AWS, most of the content we talk about – most of the things you will learn here about Google Cloud Platform – are easily transferable to AWS, like all these virtual machines, object storage, Spark. All these things work fine. In AWS, the buttons you need to click are different, or the Terraform script you need to write is different. But at the end, most of the concepts are still the same. You just need to map from Google Cloud Platform concept to AWS concept. But with AWS, you need to pay something."
Data Engineering Zoomcamp;2023;Because of the deduplication step, the numbers of the rows arenâ€™t consistent.;"Alexey_x000D_
For this case, if you go to our data engineering page, and then go to the homework, you will see this note: “If the answer does not match exactly, then select the closest option.” With that, if you have an answer, but it's somewhat not the same one, you just go with the closest one and then you'll get it right."
Data Engineering Zoomcamp;2023;If I select the Kaggle dataset, can I download it locally for the project?;"Alexey_x000D_
You can. Why not?"
Machine Learning Zoomcamp;2022;Conda vs Pipenv? We started with Conda (W1). Is it recommended to install Pipenv inside Conda? Or use Conda for â€œdevâ€ and Pipenv separately when deploying?;"I use Pipenv inside Conda. If I do $ which pipenv – it will be Pipenv inside Anaconda. [Terminal image as reference] There is nothing wrong with this. I use Anaconda as a general purpose Python distribution with the packages I need and then I install everything I need personally on top of that, like XGBoost, Pipenv, TensorFlow – everything that Anaconda does not have by default. What is good about Anaconda is that it's separate from the system Python I have. _x000D_
There is a system Python on this computer, even though it's Windows – it's not really a system Python. But on Linux, there would be the system Python, which you don't want to touch. That's why I like Anaconda, because you put it inside your home directory and then in this home directory, you can do whatever you want. And if something happens, you can just remove your Anaconda and then your system Python will stay untouched. That's the good thing about Anaconda."
Machine Learning Zoomcamp;2022;Can you teach us more about using roc_auc_score for feature importance?;"Yes. The idea here is – for numerical features, you can think of a numerical feature as a score. The output of your model is a score and a feature is also a score. It's just a number between some minimum value and some maximum value. Then you can follow through the same procedure as we did for the classifier (for the output of the classifier) but for the score. In our case, the output for logistic regression is always a number between zero and one. But for the ROC core, it actually doesn't matter what the scale of your variable is. It can be something between zero and one, it can be something between -100 and 100, it can be something between -1000 and 1000, or between 0 and 100 – it does not matter. _x000D_
What you can do there is just take this feature and go through the same procedure as we did in the lectures. Try to plot it with different thresholds and then see what happens. You can also have an ROC plot for the feature, not for the output of the model, and then based on that, you can see what the area under this ROC curve is. That will give you an idea of how well this feature separates positive examples from negative examples. This is what this ROC curve actually does. That's roughly the idea behind using the ROC curve for feature importance. You kind of pretend that this is the output of your model – this is a score – and then you do everything we did in the lectures to understand how well this feature separates positive and negative examples."
Data Engineering Zoomcamp;2023;This is for Prefect HQ. It would be better if you place a link to the documentation on the Blocks page. Any plans?;"Jeff_x000D_
There is a link here, as I shared in Slack. If you click on the Blocks page, anywhere in the UI here, there’s Prefect docs right here on the bottom left, if you're logged into Prefect Cloud."
Machine Learning Zoomcamp;2022;Are the MLOps concepts important to know for a data scientist job?;"I guess? I think yes, because data scientists do not live in isolation from the rest of the company – the rest of the team. The article I referred to actually describes how people work in a team. MLOps is not just about tools, it's also about processes – how exactly your work should be organized in such a way that you can easily maintain it, scale it, and so on. _x000D_
This is why data scientists should know some things about this. I guess that answers your question? There are also tools for experiment tracking, so data scientists definitely need to use them. I think I'm just trying to say “Yes, it is important.”"
Machine Learning Zoomcamp;2022;Adding to the DS and DE jobs in Germany â€“ do you need to know German?;I am ashamed to admit that I don't speak German that well. I wouldn't be able to use German for work. Plus I've been in Germany for quite a long time, too. That's why I'm kind of ashamed to admit that. [chuckles] But you can live quite well in Berlin. Here, I’m talking about Berlin – the rest of Germany is different – but in Berlin you can get around without speaking a word of German. I do speak some German but I'm not fluent enough to actually use German at work. Maybe if I had to use German at work I would become more fluent, but so far, most jobs don't require German.
Data Engineering Zoomcamp;2023;Are there any advantages or extra features when using Prefect Cloud versus the open source version, except that it's hosted?;"Kalise_x000D_
Yeah. Prefect Cloud is open source and there's an entirely free version of it. It is obviously hosted so you don't have to manage the time of updating your UI and everything with new Prefect versions, if you use Prefect Cloud. It also has the ability that you can have collaborators. With the free version, you also get two other collaborators that you can invite to. This means sharing the blocks and reusing blocks within the different flows that you're building with your collaborators. Then there are also additional features such as not just notifications, but automation, which means being able to trigger an action when something happens, as well. For example, say your flow fails, or maybe your agent – the work queue is unhealthy –instead of just getting a notification, you could actually pause the deployment. You could trigger an action with that automation, as well._x000D_
Jeff_x000D_
I’ll just add that there's a fifth homework question. People can use Slack and just in case you haven't been there yet, sometimes Slack is pretty limiting. It’s like, “Oh, too many people are hitting us with notifications for things.” You might have to make your own Slack workspace. There are instructions in the FAQ on that, so just check that out. Originally, I set it up with email, because email is nice and easy enough to deal with Slack. But it's just a cloud feature. So if you want that, you have to have an account on Prefect Cloud, which is free. That's our email server that's running that in the background. There are ways that you can set things up yourself, but it just gets pretty involved. A lot of things like that are just kind of easier with the cloud system._x000D_
Alexey_x000D_
And I guess platform engineers are happy when they don't need to maintain stuff. _x000D_
Jeff_x000D_
[chuckles] Yes. You don't have to employ people to do things. _x000D_
Alexey_x000D_
Yeah. I mean, even if you do have to employ them, they're still happy that they don't need to worry about maintaining yet another server."
Machine Learning Zoomcamp;2022;Which part of data science is trending now in terms of salary (computer vision, NLP and others)?;"I don't think there is a significant difference between different sub-parts of data science. Usually companies that pay higher salaries are American companies. Let's say if you don't live in the United States – in Europe, in Asia, or in some other parts of the world that are not in the States – if you get hired by an American company, they will pay you more simply because companies are usually richer in the States compared to the rest of the world. _x000D_
It's not a rule, of course. If a company is from Switzerland, they probably will pay similar money. But apart from that, I don't think there is any significant difference between different sub-parts of data science. Just pick what you like and focus on this, then you have a good salary. Again, I’m not saying that you have to work for an American company, but if you're after a high salary, then maybe consider American companies that are hiring remotely."
Machine Learning Zoomcamp;2022;Would k-fold cross validation be a good idea to check our validation framework?;Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
Data Engineering Zoomcamp;2023;Do you need an orchestrator on streaming pipelines?;"Alexey_x000D_
Typically you do not, because in the streaming pipeline, the consumers that you have in streams are reactive. Once there is a message in a queue or in a topic, the consumers consume this message and do something. Then they often put the result to another stream and then there is another consumer that reads from this stream and produces something. The execution is triggered automatically because the consumers are reactive. That's why you don't really need an orchestrator. They just wait for their piece of work to work on (to consume), they consume and then they put the results somewhere. Meanwhile, in batch processing, there should be a way to execute a thing, like a job after a job, or a task after a task, in a particular sequence. That's why we need an orchestrator there. There, it's not reactive. Something needs to execute things in order."
Data Engineering Zoomcamp;2023;Can you share with us some real-work data cleaning strategies? Is there a good practices checklist regarding what to look out for when youâ€™re at the â€œtransformâ€ step?;"Alexey_x000D_
Well, let me try to see. Google “data cleaning checklist”. What do we have here? Yeah, maybe you can go through this thing. Seems quite good. There are also tools like, for example, Soda, or Great Expectations. Tools like that. There are quite a few of them. They look at data quality and they have an existing set of checks that you can just use. I think the checks that they have by default, or the checks they have in the libraries, make a good checklist."
Machine Learning Zoomcamp;2022;Is CUR decomposition a good way to achieve important feature selection?;I am afraid this is the first time I hear about this. [Alexey Googles CUR feature decomposition] No, I don't really know what it is. Sorry. I can't answer this question.
Data Engineering Zoomcamp;2023;Just to confirm, the best way to follow the content is through the GitHub repo named â€œWeek X.â€;Yes.
Machine Learning Zoomcamp;2022;How to get out of tutorial hell?;"You will get out of tutorial hell when you start doing the project in this course, because it will be just a set of guidelines – this matrix – but you will have to do the rest yourself. You will have to find the problem yourself, you will have to do exploratory data analysis yourself, you will have to do data preparation yourself, you will have to do everything yourself – there will be no homework with the exact steps that you need to do to have a project. And this is how you get out of tutorial hell. _x000D_
Let's not think about this particular course, but in any setting – you want to find the problem that you want to solve. Once you have the problem, then try to think “What is the best way to solve this? What is the shortest way to solve it?” Or, at least, “What is the next step I need to take to solve it?” And then try to work your way through solving this problem. So focus on the problem. If you just do tutorials, you're not solving problems – you're just doing tutorials. So focus on the problem."
Machine Learning Zoomcamp;2022;Can you give a general overview of what skills a junior data analyst should know. Job postings are confusing, as companies seem to have different ideas.;I mean, I can give you my idea on what data analysts do, but it doesn't mean that everyone follows this idea. You should probably actually start with job postings, and then think, “Okay, these are the companies I want to work for. What kind of skills do they require?” And then go from there. That would probably be more useful than just my opinion. That's, “in my humble opinion”. But I'll try to answer it anyway. Data analysts should know SQL very well. That's their main tool – SQL. By SQL, I don't mean just select star from a table, you should also know “joins,” you should know “group by,” “having statement,” “Window functions”. If you know window functions, that's great. Every time I need to use a window function, I need to Google it, but analysts usually know them much better than data scientists, because they need to do SQL a lot more often. Another tool that analysts need is Python or R. I think Python makes more sense these days. But again, it depends on the company. In some companies, the analysts use R. There, it's the usual. Let's take Python –you need to know tools like pandas, some visualization libraries like Seaborn, matplotlib. I don't remember the name of the interactive library, but if you Google, you will find it (In the comments, people suggest Plotly, which is an interactive plotting tool in Python. It’s quite cool. I’ve seen some data analysts at OLX use it). So doing simple analytics in Python and then plotting the results is useful. What I also see our data analysts at OLX do (we call them product analysts) is also use Tableau a lot – Tableau or any other dashboarding tool. Again, it depends on the company. Many data analysts spent quite a lot of time building dashboards in Tableau. Sometimes, it can also be an ad hoc analysis in Excel, or first in SQL and then putting data in Excel and then saving it over. SQL, the most important thing, then Python and data analytics tools in Python such as pandas, and then dashboarding. That probably covers like 90% of the work. But also, it's important for data analysts to be able to communicate. I hope that's helpful. It's a bit off-topic for this course and I am not an analyst. I work with analysts quite often, but I'm not an analyst, so please treat this with a grain of salt and maybe talk to real data analysts about that.
Data Engineering Zoomcamp;2023;In Terraform, we have to use some credentials. What is the best way to store credentials safely? Can you create some videos about basic information security approaches?;Well, I'm not really a professional in this area. What typically happens for example, in the Google Cloud Platform, everyone keeps the file in a home directory. This is not version controlled. You basically don't commit this file. This is one approach. I think this is a pretty common one. I mostly use AWS. In AWS, there are ways to authenticate, which does not require putting any credentials in your Git. Once you authenticate, you can just apply Terraform stuff. That's how it's usually done. I think it's not the best answer, honestly. Again, I'm not a professional in this. If any one of you knows what the best approach is, maybe write it in Slack and we can possibly create a video about that. Maybe you can write a blog post, if you want, about that. But yeah, I don't know.
Machine Learning Zoomcamp;2022;Week 5 question: need a recipe to check items connected my predict.py file run in VSCode says no module flask referring to code â€œfrom flask import Flask. Please help.;I'm trying to parse this question. If your VSCode says “no module flask” then what you need to do is look up how you connect your Visual Studio Code to your Python Interpreter. Probably something like this. What you end up doing at the end is – select an interpreter, the one in your virtual environment, and then this interpreter knows about flask and all these things.
Data Engineering Zoomcamp;2023;If we consider professional certification for getting into the industry, which one would you recommend?;I would not recommend any certificates. Just focus on projects.
Data Engineering Zoomcamp;2023;I don't quite understand which playlist I should use. Could you repeat in more detail?;Okay, I can. All the course videos you will need are here, Data Engineering Zoomcamp. And all the live videos, all the homework, everything that is specifically related to this cohort, will be in the Data Engineering Zoomcamp 2023. But you don't actually need to use a playlist because all the videos are linked here. But if it's more convenient for you to use a playlist, then this is the playlist to use. And this one is just materials for live streams, homeworks, and so on.
Machine Learning Zoomcamp;2022;Will you teach NLP? I want to learn and implement NLP for market news analysis. Another use case I want to try is understanding data (tabular, image, audio).;Not as a part of this course. We will not cover NLP here. Maybe if there is a lot of interest in NLP, yes. I am actually thinking about what kind of course I should make, so if you have any suggestions, please reach out. I want to start thinking about the next course, so maybe by the end of this year we'll have some sort of outline. If there is a lot of interest in NLP, it could be NLP. But I don't know NLP that well myself. I know the basics, so how deep do we want to go there? So please tell me what you're interested in regarding NLP, and then we can see if this will happen at some point.
Machine Learning Zoomcamp;2022;I chose to work with a fraudulent transaction dataset. I see that there are a lot of similar projects online. Iâ€™m wondering if this will affect my project.;No, it should not. Just keep working on this project. It should not affect anything. Of course, if you decide to copy the project that somebody did, then it will, of course, affect it. If people who evaluate your project will accidentally find this out, you will receive zero points and you will have to rely on the other two projects to pass the course. You will not be disqualified for the course, but for the project, you will. So don't do that. Apart from that, I don't see any problems. As long as it's not a Titanic or Iris dataset – it's not one of these popular datasets, that's fine.
Machine Learning Zoomcamp;2022;Which area of Data Science/Engineering/Machine Learning/Deep Learning is best for finding a remote job?;Pretty much any of these areas. I don't know which one is best. What does “best” mean here? Remote jobs exist for any of these areas. Just pick up what you like, build a portfolio in that area, and then start looking for a job?
Machine Learning Zoomcamp;2022;Why do we take full_train to identify the values for one-hot encoding. It could be that a value is only present in the test dataset. Why not use the full dataset?;"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. _x000D_
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
Machine Learning Zoomcamp;2022;How should I choose the number of pixels to train the image model?;You mean the input size, I assume. If it's a pre-trained model, it's easy, because you just take whatever is available there. You also take into account speed and accuracy requirements – and you will probably have them – you will have some idea of how good your model is or how good your model should be. If that's okay for the model, for example, if you want to make it faster, is it okay to sacrifice some of the accuracy? So you need to take all the business requirements into account to answer your question. The answer, again, is “It depends.”
Machine Learning Zoomcamp;2022;Are the project criteria for the capstone projects 1 and 2 the same as the midterm project?;Correct, they are the same.
Data Engineering Zoomcamp;2023;Thereâ€™s lots of discussion these days around analytics engineering. What are your thoughts about this exciting area?;Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
Machine Learning Zoomcamp;2022;Does this course cover end-to-end projects for better understanding (put data in a database, apply ML to it, convert them to PyScript from Jupyter)?;"We don't touch databases here. It's “semi-end-to-end,” if you will. [chuckles] If we think about the process for data projects – first, we have “business understanding” and “data understanding”. This course focuses more on this part:_x000D_
_x000D_
Partly data preparation, and partly evaluation – maybe more deployment. There’s a focus on modeling and deployment. I would say that the focus is more on Data Preparation, Modeling and Deployment._x000D_
_x000D_
For example, Data Understanding and Data Preparation would be data engineering._x000D_
_x000D_
And then Evaluation and Deployment would be MLOps._x000D_
_x000D_
And then business understanding is more related to product management. There is also a thing called AI product management or ML product management, where they talk more about how exactly the process should look like and so on."
Data Engineering Zoomcamp;2023;Can you talk more about the final project? What should we be thinking about now to prepare us?;"Alexey_x000D_
The first thing about the dataset – what kind of dataset do you want to use? Or what kind of problem do you want to solve? Once you figure this out, then you're basically ready to start working on a project. Then in the project, you need to decide if you want to do streaming or batch. For batch, it's using things like Prefect, Spark, or DBT. For streaming, it’s using the materials from the last lecture (week 2). Once you decide that, you will just implement this and you will find all the information you need here in the week 7 project repo. Just go through this and if you have any questions left, let us know. Keep in mind that these are the criteria that other people (your peers) will use when evaluating your project. Perhaps you can already think about that and how you want to implement your project in such a way that you maximize the score you get from these criteria."
Data Engineering Zoomcamp;2023;I don't have credit cards, how will I create a GCP account for free?;You will not be able to do this because they require a credit card.
Data Engineering Zoomcamp;2023;Do I need to complete both projects to get the certificate or just one?;Just one. It's either the first attempt or the second. You should not try both.
Machine Learning Zoomcamp;2022;If somebody wants to contribute to BentoML and help with SnapML support or whatever it is they want to help with, they just go there and say, â€œOkay, I want to take this feature.â€? What does the process look like? Maybe they should first join the Slack community you have and then say, â€œOkay, I want to contribute. This looks like a good issue. Should I take this?â€;"Tim_x000D_
I think the way that it works is that you can always join the Slack community and ask. That's usually the best way to get in touch with one of us. Joining the community is always helpful. Anybody there can point you in the right direction. Then, depending on what you want to contribute, we'll kind of walk you through how to structure it. A lot of the time, people will need certain features that we don't have and we'll go ahead and fork the repository and open PRs themselves. That's typically the fastest way to contribute. If you need something, then you build it yourself and then open the PR, and then we'll review it and probably have a few comments and then commit it _x000D_
Alexey_x000D_
A word of caution – sometimes just opening a PR without talking to any of the maintainers first may lead to frustration and a lack of understanding of why a PR was not accepted, like “Hey! I spent two weeks of my time contributing and they don't want to accept my project. I'm not going to contribute to open source anymore in my life.” This has happened to me. That was frustrating. What I learned from that experience is that, first, it's best to talk to the maintainers and ask how exactly they want this feature to be implemented. The way you might want to implement may not be the same way the maintainers see this feature being implemented. So it's worth discussing first. _x000D_
Tim_x000D_
That's definitely true, Alexey. I think most of the features out there, we've probably heard of, and we already have some thoughts on how we want to do it. Also, it's likely that we've seen a lot more edge cases as well. That's why it's probably a good idea to run your approach past us before you start coding it. That’s if you want to contribute. If you don't want to contribute and you just want to solve the problem for yourself, you can do it however you want."
Data Engineering Zoomcamp;2023;Can you say something about the required computing power to be successful in the course?;Yeah. Last year, we needed at least 16 GB of RAM to run Airflow. Here, we don't use Airflow, so probably 8 GB of RAM should be enough. But I will still go with a VM – I would still take 16 GB of RAM. It will just be better. Let's say you have a laptop with 8 GB of RAM and you already have Chrome running there and VSL and other things, there will really be no space for running stuff.
Data Engineering Zoomcamp;2023;How to fully utilize GCP credits? It's much more than this course will require. Also, guided end-to-end mini projects will be really helpful.;"Alexey_x000D_
I'm not sure what this question means “how to fully utilize”. Well, come up with projects and utilize them. It's really up to you. I don't know how we can help you here. If you want to have some brainstorming sessions, just go to Slack and start a discussion there. I'm pretty sure we and the students will be very happy to take part in these discussions, contribute, and come up with some ideas. _x000D_
As for the end-to-end guided mini projects… yeah, sure. If you have some ideas, let us know. But then it also feels like another course. The projects we will have here are not super guided. They are more like, “This is the set of criteria you need to satisfy. Go and work on them.” It's kind of guided, but most of the time, you still do stuff on your own. But this is when you really learn. When you do a guided project, maybe you learn less than when you do a thing on your own. But again, in both these cases, let's start the discussion in Slack and see where it brings us because both these things are really interesting."
Data Engineering Zoomcamp;2023;In this course, we use GCP. In the future, what is the best cloud to invest time in learning?;"Ankush_x000D_
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch._x000D_
Alexey_x000D_
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that._x000D_
Ankush_x000D_
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? _x000D_
Alexey_x000D_
Well… I'll have to use Azure. [chuckles]_x000D_
Ankush_x000D_
So we’ll need to migrate? [laughs]"
Data Engineering Zoomcamp;2023;I need advice on how to build credibility as a data engineer. Iâ€™m thinking of freelancing, open source and Git projects.;Yep, projects are the best way. Just start working. That's already good enough. You can also think about giving talks and so on.
Data Engineering Zoomcamp;2023;Can you please explain the Python Black setup in Visual Studio Code? Also, can you explain good Python coding standards as you write docstrings and type strings?;"Jeff_x000D_
I can try. I like Black a lot. If you just Google “Python Black” and go to the GitHub readme, it’s become super popular in the last couple of years for automatically formatting your code. It is really nice. Mine is set up in Visual Studio Code, so when I hit “save,” it automatically formats. As it says, it’s an uncompromising code formatter. I think it says here about asking Henry Ford for color back in the day. Is that right? It's colored cars or color telephones from Alexander Graham Bell or somebody, and it's like, “They can have any color they want as long as it's black.” He wasn't going to compromise. You don't have to think about it now. It's easy. Just go right ahead and get your gear code formatted automatically. So it’s things like two lines after function or before – that kind of stuff just happens automatically with it. Getting it set up in VS Code can be a little tricky sometimes. But there are guides to that. Googling is what I do for that, usually._x000D_
Alexey_x000D_
I just wanted to do a shameless plug, because we have another course called MLOps Zoomcamp. By the way, there is also a Prefect model there. One of the things there is best practices. In best practices, we have this video called Code quality: linting and formatting. It does not show how to integrate Black with VS Code, I think. I don't remember if we do this. But if you want to learn more about testing, and Black, and other things like pre-commit hooks, make files, and so on – you can check this out._x000D_
Jeff_x000D_
Yeah, it's not too tricky to set up. It's like pip install a package in your environment and then you have to give it a path maybe or set one setting in VS code. If all goes well, cross your fingers, it should just work after you reload it. Not always the case. If you have trouble, a lot of people use it, so there are a lot of good resources online. Black is great. This is an awesome set of some resources here on best practices. Type hints are just getting more and more popular. They're very helpful, so that people know what kind – especially with autocomplete and little pop-up type – things like that in Visual Studio Code and other code editors. You can see what kind of argument type you should put in and then Prefect uses that information as well, in our flows, for example, to make sure that if it's a block in the UI or a parameter in the UI – it'll be smart. It'll be like, “Oh, is this a number? Okay.” It will give you options to put in numbers. “Is this a different kind of form field?” It'll have different options. It also can then do some validation to make sure that people actually put in something that conforms to that type-in. Python is slowly getting more and more smart about how it handles typing and newer versions keep adding more functionality. Type hints are nice to use. It takes a little bit of writing, but it makes your docstring shorter. The last thing that was asked about here was docstrings. It's great to have in every function to tell people what it's about. It's something that maybe you don't always do if you're in a hurry, but you should do it, especially if other people are going to read the code. Code is read like 20 times more often than it's written, or however you translate that – some stat. So do it. It's so helpful for you in the future and it's helpful for other people in the future, who are going to read your code to see, “What were you thinking? What is the purpose of this function?” Keep your function small, explain it in your doc string – it's good stuff. Then it shows up in your code editor, if you're lucky (if you have a good code editor). That's all to say about that._x000D_
Alexey_x000D_
Do you know any resources where people can learn about setting up? Or learning more about these things, like good Python coding standards? What I showed is obviously a good resource, but it does not cover all these things that this question asks about._x000D_
Jeff_x000D_
It's a good question. I do have a link to Google Style, or there are a couple different styles of docstrings. It seems like they're a little bit much these days, maybe. But there are links for different ways to do type hinting. I do have a few things if I look around for them. I don't have them at my fingertips right now. But Michael looks like maybe he's got one he shared there._x000D_
Michael_x000D_
Yes, this one is a little bit older, but it is great. It goes into using virtual environments, Poetry… there's a lot to unpack, but I think that's still pretty much the standard best practice at the moment."
Data Engineering Zoomcamp;2023;Any open source ELT tools?;"Alexey_x000D_
Let me just copy this and paste it to Google and you'll find a way to answer your question. Is actually asking for the ELT tools. Not helpful, I guess. But yeah, Airbyte is definitely one of them. There are others too."
Machine Learning Zoomcamp;2022;How can scaling be implemented, several servers? But also with several GPUs? Batch inference?;"When it comes to scaling, for example, when we talk about AWS lambda, you don't need to worry about that. When lambda gets a lot of traffic, a lot of requests, and one lambda cannot handle them, it automatically starts another lambda function in parallel – another instance of a lambda function. So you don't need to worry about any of that, lambda is doing its own thing in the background. But as a result, what you get is it scales – it can handle more traffic. If you want to maybe have control over this, if you want to scale it yourself up and down, then in Kubernetes, there is the Auto Scaling group. You will see in the Kubernetes homework how to actually do this. Also in module five, we talked about Elastic Beanstalk, it can also automatically scale. When it sees a lot of requests coming in, it creates another instance and there is basically a load balancer. I think it's called Elastic Load Balancer, which forwards the request to multiple instances. When it comes to GPUs – in the course, we do not show how to use GPUs with Kubernetes. But if you Google it, you will see how to add nodes with GPUs to your Kubernetes cluster (to eks) and then, based on that, you can scale – you can create more machines with GPUs. If you need that, of course._x000D_
As for batch inference, what we talked about here is what I call “online inference”. In this course, we only covered online inference, and by “online,” I mean we create a web service and it's like an API. You get the request, the service processes this request and replies with a prediction – a response. This is what I call an online web service. There is another one which is called offline and this is what batch inference is, usually. There is also what we saw in Bento. In bento, there is a different kind of batching – it's still online, it just takes all the requests, puts them together in a batch and then it's still a web service. There is another way of serving, which I usually call “batch serving,” or “offline serving”. This is when we have all the data in our, let's say, S3 location or somewhere and then we just load this data with something like pandas, we apply the model, and we save predictions. That's also an option if your use case allows it. If you want to learn more about that, we actually discuss this topic in more detail in the MLOps Zoomcamp. There is a module about deployment, where we talk about three ways of deploying: web service, the one we use in this course (ML Zoomcamp), then there’s streaming, when you have a stream of data and you react to events in that stream. Then the third one is batching. So if you're interested in learning more about deployment, definitely check out MLOps Zoomcamp."
Machine Learning Zoomcamp;2021;For categorical target set, where the distribution is imbalanced (for example, 90/10) what approach should be used?;"Alexey_x000D_
Should we use something non-standard there or can we just go with the usual things we learned in the course?_x000D_
Hamed_x000D_
You just need to test different strategies. Something I noticed – if you have so many parse subclasses in your categorical [inaudible], you should be careful about using one-hot encoding. You might say you can use ordinal encoding, if your data in nature had some order. It will be useful. In my particular data, I couldn't have domain knowledge. I didn't know what the subclasses were, so I couldn't decide which strategy I should choose. But if you have the domain knowledge, that’s the key here, I think."
Machine Learning Zoomcamp;2021;Is there anything that we are not allowed to use? For example, if you want to use hyper-parameter tuning, are we allowed to use GridSearchCV, RandomizedSearchCV?;"No, I don't think there is anything you cannot use. I just want to ask you to document it, if there is something you use that is not a part of the course. For somebody who will be reviewing your project, it will be new information. So just give them some context and explain, “I used GridSearchCV because this is easier and this is what you can do.” Then you describe it, and you give your peers a chance to learn from what you’re doing. This way, they will not be lost and will be able to grade your homework. If you do that, you can use whatever you want. _x000D_
From the materials in the course, like if you use logistic regression, or rich regression, or XGBoost, or random forest, then you don't need to really go into details when documenting because I would expect that people understand that without much explanation. Maybe it's also a good idea if somebody, let's say, wants to use not XGBoost, but LightGBM, or CatBoost, which are different implementations of gradient boosting – you're free to use them in your project as well. You can use FastAPI instead of Flask, for example. You can use Poetry instead of Pipenv. You can use Conda instead of Pipenv. You're basically free to explore to use what you want and play with different tools, just be sure to document that."
Data Engineering Zoomcamp;2022;I have been catching up and have been doing homeworks but not tuning in. Will I be able to turn in the final project?;"Alexey_x000D_
Yes, you will be. You can submit the project. As we promised at the very beginning, we will give certificates based on project completion, not based on the homework. So if you're catching up right now, and you didn't do all the homework, it's fine. If you are caught up and you know what to do, you can take part in a project."
Data Engineering Zoomcamp;2022;Could you please explain what code we should load to GitHub?;"Alexey_x000D_
I think the question refers to the homework form where we asked you to submit your code. Here, you can just create a repo in GitHub, create a folder there, homework 1, put your SQL files there, and just leave a link to this file when you submit. It can be anything else. If you are more of a GitLab kind of person, you can create a report in GitLab. But it should be something publicly accessible. Put your SQL queries there and submit it with the form."
Machine Learning Zoomcamp;2021;Is it just me or does the model have really bad accuracy and generates random images? I tested and identified cats as dogs.;"Dmitry_x000D_
It's fine, because this is the showcase purpose. We said to optimize the number of books, for example, the validation steps. Also, the architecture can be a bit changed. For sure, we can create a much better model._x000D_
Alexey_x000D_
For this model, I guess, if we wanted to build a cat vs dog classifier that works well, we would use a pre-trained neural network and fine tune it, right?_x000D_
Dmitry_x000D_
Yeah. That’s one of the options._x000D_
Alexey_x000D_
Or just get a lot of different pictures of cats and dogs and train from scratch. Are there any other options?_x000D_
Dmitry_x000D_
Other options would be  to try to tweak the parameters and all those things._x000D_
Alexey_x000D_
So you think you can train a model from scratch – the one we had – without using a pre-trained neural network and then have a decent accuracy?_x000D_
Dmitry_x000D_
Yeah, but it's also the question of what “decent” is. _x000D_
Alexey_x000D_
At least 80%, for example._x000D_
Dmitry_x000D_
Around 80, I think yes. But we need to remember that the pretrained will always have the benefit._x000D_
Alexey_x000D_
If we think about this – right now, this model that we have, has an accuracy of 65% on validation, which is just slightly better than a random guess. Well, it's not slightly better, but it's 10-15% better. There are chances that, you pick a few random images and in both of these cases, it will be incorrect just because it's 65%. Right? _x000D_
Dmitry_x000D_
Yeah, but we didn't do any…_x000D_
Alexey_x000D_
Yeah, I'm just saying why this can happen. This could be the reason – 65% is not the best accuracy in the world."
Data Engineering Zoomcamp;2022;Are we distant from what a data engineerâ€™s day-to-day work looks like? What are the main differences?;"Alexey_x000D_
I guess the question is asking about the difference between what we are doing here in the course and the actual work of a data engineer._x000D_
Victoria_x000D_
More problems. [chuckles] More troubleshooting, probably._x000D_
Ankush_x000D_
I think the course is made in a way to set you up for becoming a data engineer or tackling all the problems of a data engineer. Mostly it will revolve around the technologies that we are talking about._x000D_
Alexey_x000D_
So what are the main differences, except for more troubleshooting? I imagined that there are analysts who come to you or other people with ad hoc queries, saying, “Hey, where is this data? Where is this table?” And then you have to help them. What are the other main differences?_x000D_
Ankush_x000D_
I would say complexity. Definitely._x000D_
Alexey_x000D_
Yeah, we have a relatively simple case, right? We only need to do one join. Well, two joins. We have a bunch of tables, but we don't need to join these big tables with each other. We only do a join with the location table, which is pretty small. But in practice, we often have two big tables that we need to join._x000D_
Victoria_x000D_
Yeah, we don't have complex business logic. Plus, you don't really do the setup all the time. You set up BigQuery once and then you maintain it, but you probably won’t be dealing with the service account that much._x000D_
Alexey_x000D_
Ideally, there may be a team who deals with that. For example, at our work, we have a team who manages Airflow, so I don't need to worry about this. All I do is just write DAGs, commit them, and that's it. I never had to set up Airflow locally and worry about these things because it's managed. I think it’s the same with the data warehouse and other things._x000D_
Ankush_x000D_
I think one more thing that everybody with data does is basically communicate outwards what the meaning of different columns is or what this ratio is versus that ratio. All that. I have to personally do a lot of that. What's your experience like?_x000D_
Victoria_x000D_
I would say the same, especially for business stakeholders. But DBT also has this data catalog part. So in the data team that helps quite a lot. Also we look at and work on the selection of an actual data catalog. So hopefully, there’s not too many questions._x000D_
Alexey_x000D_
In our case, for these questions we will have a catalog team. We have an internal catalog tool and they get all these questions, not data engineers."
Data Engineering Zoomcamp;2022;How is the data engineering market in Berlin? How can someone land a DE job in Berlin?;"Victoria_x000D_
I think there's a lot. It's a little bit hard to get a data engineer, and that's also why I would assume there are a lot of openings, and you will have a lot of options. How can someone land a job? If you go to meetups or have contacts, that's probably a good way. Or just apply._x000D_
Sejal_x000D_
I agree. Actually, there are plenty of data engineering openings. When I was a data engineer in my previous role, I used to get invitations for job interviews almost every week from recruiters. There's an abundance for this role in the market. Definitely try it out._x000D_
Alexey_x000D_
Since you changed your title to ML engineer, do you now get fewer or more invitations? You don't get anything? _x000D_
Sejal_x000D_
Oddly. [chuckles] _x000D_
Alexey_x000D_
[chuckles] Okay, so we see that being a data engineer is actually better, in terms of market demand._x000D_
Sejal_x000D_
Yeah, I think engineering skills are way more in demand than data science and machine learning skills are._x000D_
Alexey_x000D_
Yeah, that's interesting. What happened to “the sexiest job of the 21st century”? Nobody wants to hire them now. [chuckles] We had an interesting discussion in one of the podcasts here with Ellen. Ellen was a data scientist and she became a data engineer. She is from Berlin and I think she also talks about how to get a job as a data engineer in Berlin specifically. I think the recommendation was to talk to consulting companies. _x000D_
They have some sort of programs for coaching juniors, because the business model is to hire as many juniors as possible because they're cheap, and then you sell them to their client for a lot of money. So you need to have training programs so that the juniors are effective. Her recommendation was to try to find such a consultancy company and learn there. I hope it's an accurate summary of the episodes. Maybe I'm misinterpreting something._x000D_
Victoria_x000D_
Yeah, I think if you come from a non-data background, and you're starting with the course and all that, just bear in mind that you'll have to apply a lot and there will probably be a lot of rejections. At least that's how it was for me at the very beginning when I moved to Berlin. Because everything was kind of new and all that, I didn't have a job at the time, because I just moved to Berlin and then looked for one. _x000D_
Try to get as many interviews as possible, and then do the challenges and all that. That will train you a lot and will also help you know where the benchmark is, kind of. But yeah, there are a lot of offers, so just start applying. Start applying, start talking to other people, probably to other data engineers, as well. That's also good. About the consultancy, I don't know. I've never used anything like that._x000D_
Alexey_x000D_
I worked at an outsourcing company, but it was before I moved to Berlin. But I can confirm that in the company where I worked, they did have quite a good training process. They would have a coach that was assigned to me who would help me with everything. There was also some sort of bootcamp program. Then after that, they would try to sell me to a client and they would coach me on how to pass an interview with a client. Once a client likes me, I just start working. This is quite common, I think, in outsourcing and consulting companies. For me, that was a very long time ago, so I don't know how that actually works now. [chuckles]_x000D_
Sejal_x000D_
That's actually an interesting model. It's good that companies are actually putting that much effort into coaching the employees to be honed to certain projects. That's nice._x000D_
Alexey_x000D_
Yeah, but the thing is, the business model is buying somebody for cheap and selling them for a lot of money and then taking the margin. So, unfortunately, sometimes (at least this was in my case) the salary was pretty low. I remember that they didn't have much left after paying for my flat. So be careful. [chuckles] But at least that was a good experience. After one year, I could sell this experience for “market standards,” let's say. So depending on how young you are, how much you want to get this experience, I think sometimes it's worth actually agreeing to a lower salary in exchange for experience. Then you get this experience, and then your value on the market becomes higher. _x000D_
Another shameless plug, but this is something we actually talked about with Juan Pablo. He suggested looking for small gigs that don't pay a lot, but this is experience you can sell afterwards. This is somewhat similar to what we mentioned, even though he's more into analytics than data engineering, but I think the tips that he shared when he was looking for a job – I think they're pretty universal. There was also a funny story that he was actually driving an Uber to be able to survive while he was trying to study things in order to switch. That's a cool one as well."
Machine Learning Zoomcamp;2021;We used tf.keras.layers.GlobalAveragePooling2D in our model. What is the difference in using GlobalAveragePooling1D, 2D, and 3D in pooling layers?;"Let's say we have a two dimensional thing (2D) and we want to turn it into 1D. For this case, we need to use Pooling2D. I'm not sure right now and I want to quickly check it. This is the reason. Remember, when I was showing you how I usually go about defining the model? I build it sort of layer by layer and every time I add one more layer, I do a model that predicts to see what the output is. And then based on that, I see what kind of pooling layer, for example, I need. Let's say if we want to turn 2D into 1D, I think we need to use 2D pooling. I'm not exactly sure whether it's 2D or 1D. I think 1D pooling is needed when we have something one dimensional and we want to turn it into just one value, then we use 1D pooling. And then 3D pooling is when we have a three dimensional thing and we want to turn it into a one-dimensional thing. [image 2] These things always confuse me, to be honest. _x000D_
That's why I follow this step by step and then I just try different poolings. Then I want to make sure that, if I convert an image into a vector presentation, I have something one-dimensional. Usually the size is the number of images times something. So it's more like a 2D array. For each image, I have a one-dimensional vector. Then based on that, I try different poolings. Sometimes, you can also just flatten. Flatten takes whatever D – let's say you have KD – and you want to turn it into 1D, you use flatten. There are many different options. I think it's clear what the difference is. I might not remember exactly when to use 1D, 2D, and so on, but the difference is what kind of input they take and what kind of output they produce. If it's just a cube, then it's 2D, if it's a hypercube with three dimensions, that it's something else."
Machine Learning Zoomcamp;2021;If you pickle load an object, are all the methods associated with that object also loaded?;Kind of. Actually, pickle expects that you have the code that, let's say, if your pickle and object are of a particular class. Let's say this class could be part of the package Scikit Learn linear (the class is logistic regression). When pickle loads an object, it expects that this class is present in your Python. It loads all these methods from that code – from that module. It expects that the code will be there and it just basically loads the data and the behavior is stored in the code. So you have to have the code.
Machine Learning Zoomcamp;2021;How's ranking different from the regression model?;"I think in one of the videos, I talked about the three different subtypes of supervised learning, integration, classification, and tracking. About ranking – [image for reference] for regression, Let’s say you have a car and then we extract your feature matrix from the car. You apply the formula (the g function that you trained) and you get a prediction, like price. You do this for one object. You get one car and you do a prediction for one car. _x000D_
When it comes to ranking, you don't have one object. Of course, you can have multiple cars and apply this to multiple cars. But the core difference with ranking is – in ranking, let's say you have some results from Google. So you have some query and you have some results from Google. There could be, let's say, results from 0 to 99. And you need to apply a function to each of the elements. This is a group. And you need to apply this model that you have (the model g) to each element of this group. Then it produces a ranked list. Let's say, you apply g(x0), and you apply g(x99) this x99 is the row here – the results of the query. Then what you do is rerank the output – you rerank all these results using this function. _x000D_
What I'm trying to say is, here, you look at the group and you try to see how good the ranking is within the group. While in the case of simple regression, you have more standalone objects, sort of. But with ranking, it always has to be a group. Of course, when it comes to ranking, this g can also be a regression or it can also be classification. But then you always need to think about the other elements in the group. I hope that answers the question. _x000D_
This is not something we will go into detail about – not in this course, for sure. This is just for you to know that it's a little bit different. Maybe this is something you want to do for your project or explore for the article. This is totally fine."
Machine Learning Zoomcamp;2021;How many questions should be correct for each session in order to get the certificate?;Zero. You don't actually have to submit homework to get a certificate. For the certificate, you need to finish two projects – the midterm project and the capstone project. This is what you need to do to get the certificate. The rest is optional. It will give you some points. If something happens, and you don't find time to complete one homework assignment, the world won’t stop. You can just catch up and perhaps do another homework assignment. You will just not get the virtual points. But for the certificate, all you need to do is finish two projects.
Data Engineering Zoomcamp;2022;When Victoria said that we must use one of the two approaches for the project, what exactly are you talking about? Batch vs steam?;"Victoria_x000D_
For example, there are some weeks that kind of overlap one another. Maybe not overlap, but you could use DBT or you could use Spark. This is what we mean by one of two approaches. You can choose what to use out of everything you’ve learned._x000D_
Alexey_x000D_
Also for the pipeline, indeed, batch versus stream is also an option. How exactly your data ends up in your data warehouse before you do… Somehow you need to put the data in the data warehouse and you can do it with batch or with steam. After the data is there, you can do some other stuff on top of that to transform the data. For example, you can use DBT for that or something else. Indeed, there are many ways you can juggle it. Or maybe you can have both – that is also an option. Probably you should just go with one._x000D_
Victoria_x000D_
I mean, there are some things where there's not much of an option. For example, with BigQuery you need to have Google Cloud Storage or things like that, unless you're using the local version, I guess."
Machine Learning Zoomcamp;2021;Regarding regularization â€“ would it not be better to analyze the data model and remove redundant features?;"Alexey_x000D_
What do you think, Dmitry?_x000D_
Dmitry_x000D_
Thinking about that, I think it's referring to one of the questions before. Usually, when you have a real task, you need to do everything together. This should be a combined process and you shouldn't go step by step. Basically, one of the purposes of the EDA step is to give you the bridge to the feature selection process – thus helping you to understand what feature you will use or not use in your model. After you decide that, you need to decide whether it's okay or not okay to go with regularization. So basically, step by step._x000D_
Alexey_x000D_
It also doesn't hurt to try regularization as well, even though it seems like there is no redundancy in your features – but, all of a sudden, regularization helps. So you just need to test it._x000D_
Dmitry_x000D_
Or it doesn’t help._x000D_
Alexey_x000D_
Yes. So you don't know. There is no way to find out except to try it."
Machine Learning Zoomcamp;2021;We have from November 1 to November 8 to review other projects?;This is correct. Yes.
Data Engineering Zoomcamp;2022;For DBT, you organized the transformation as stage->processing->presentations. Is this architecture typically used or are there more layers?;"Victoria_x000D_
I would say yes, this is the go-to, and not just DBT. Yes, DBT enforces this. If you go and read their viewpoint and all of this, they'll explain that they also do the models with this kind of structure. But as I mentioned in the modern concepts, this is a concept that comes from the 80s that Kimball defined. With the kitchen analogy, I find that very, very useful. This is normally how you go. Of course, the more complex it gets, you may need more steps, and that's okay. But always try to have this separated – What is the presentation? What is the source? You will need to do some typecasting like we do, for example, maybe do some duplication. And then in the middle, you may have more models that will go in that data pipeline, let’s say, in that flow. But this is more or less how it would look like. _x000D_
Alexey_x000D_
What is the kitchen analogy? _x000D_
Victoria_x000D_
The kitchen analogy is that the way you model your data is similar to how restaurants model their food. You have a warehouse where you just have raw food. This is where you have your source data. Not everyone is allowed to do that. In fact, it's actually dangerous. That's why there are security measures about who can actually go to the warehouse, what can be stored, and how it can be stored and things like that. Then you have the kitchen. Only the people that can cook this raw food and make it into food that they're going to serve are allowed in there. This would be the data engineers and the data analysts, maybe analytics engineers. This is what you would be doing in the data warehouse, trying to process that raw data. And then at the end, you have the part of the restaurant where people eat – the dining hall. This is the presentation layer of your data warehouse. This is how fact tables will look like, or your data marts, if you're building data marts. These are the business stakeholders. Everyone is allowed to get that foot, let’s say. There are no restrictions, but it's only already presented. You're not presenting them raw._x000D_
Alexey_x000D_
I think we have this tableau self-service tool, where we can go and do all this Cubes and all that. This is the presentation layer, right?_x000D_
Victoria_x000D_
Yeah. Technically, in BI tools that are self-serve, you could have everything. You could expose everything. Right now, we have the external tables, then we create our own tables in the [inaudible] and then we create the section model and then we create the factories. I could have everything in there. I could even have the ones that my development exposed to the self-service, the BI tool, like Tableau or Looker, whatever. But what this analogy says is that you should use the presentation layer only, and not the other parts. Because if you present the raw data, then people wouldn't know how to use it. It's going to be like, “Oh, I didn't find these phones, or these other ones. Maybe it's because there was no transformation,” and things like that._x000D_
Alexey_x000D_
Nice analogy."
Data Engineering Zoomcamp;2022;When and why do you think the data engineering position started? We had ETL tools a long time ago but can we say that DE started there?;"Victoria_x000D_
I would say it’s probably similar to what happened with data analytics engineering. Suddenly the whole stack became more complex and you couldn't just make do with a few scripts and [unintelligible] and that's probably where it started, I would assume. But I don't know the whole story of the data engineer role._x000D_
Alexey_x000D_
Maybe I will give you a very, maybe not correct answer, but one from a data scientist’s perspective. Data science started more than 10 years ago. Maybe people expected too much from data science. They thought they would just hire a bunch of people with PhDs in mathematics and they would just do some magic and then the data would just be clean and the models would be perfect and the companies would earn a lot of money. _x000D_
They did this, things didn't work, and they thought, “Okay, what is the problem?” It turned out that it required a lot of engineering – all these data pipelines need to be there. I think this was around the time when data science was getting some traction, people realized that it's also important, and I think it was also together with analytics – maybe in the beginning of 2010s (somewhere around there)._x000D_
Victoria_x000D_
Do you want the right answer? 2011. In Airbnb and Facebook, apparently, they started to do it and it's indeed related to all of the things that we were talking about – the complexity of the ETL process, then they suddenly needed a role that was more oriented to just building these pipelines to transport and transform that data._x000D_
Alexey_x000D_
I imagine that a company hires an analyst and the analyst says, “Okay, I don't know what to do with this. Somebody needs to come and set up this infra.” Then, because analysts couldn't do this, data scientists couldn't do this – somebody needed to do this. So ETL developers kind of rebranded into data engineers, I guess. Nice history Lesson."
Data Engineering Zoomcamp;2022;In the GitHub page for homework 2, you mentioned to ingest NY taxi data for 2019-2021. In the homework 2 description, however, it's 2019-2020. Should it be until 2021 or 2020?;"Alexey_x000D_
In week 3, for yellow taxi data we use 2018-2020. And for frequently/high use, I think it was 2019. For zones, it's just one file. Victoria, for week 4, we use just one month, right?_x000D_
Victoria_x000D_
Whatever you load, basically, in week 3. So it would be as I understand the one for 2019-2020, right? Since we're going to be building the model, it would be independent – we'll just use whatever is in the table. If they didn't get to do the homework or something like that, if at least they have one month, it should be enough for them to run the models._x000D_
Alexey_x000D_
I'll try to update the description so it's less confusing. For yellow taxis, it’s 2019-2020 and for for-hire vehicles, it should be 2019. We don't use green taxis for week 3."
Machine Learning Zoomcamp;2021;Is it possible to get email confirmation when submitting homework?;"Alexey_x000D_
It is possible, but only if you have a Gmail account. In Google Forms, this option does exist, but you have to have a Gmail account. Many people who take part in this course have iCloud, or Yandex, or Yahoo, or some other email provider that’s not Gmail. That's why it seems it's not really possible to do that because it requires you to be logged in to your Google and then it automatically captures your email and then sends the result. This is what I know. Maybe I'm wrong. If you know how to make it in a way that everyone, not just Gmail users, can get a confirmation and it doesn't force others to use Gmail for that, let me know."
Machine Learning Zoomcamp;2021;What does it mean, â€œFeel free to submit PRs with links to your notes.â€;"Let's go to the notes. We have this course repo here. Here is the course, let's take, say, linear regression. We have the notes here – and this is where it says “Add notes from video (PRs are welcome)”. What I mean here is that – let's say you watch a video here and you took some notes. And you want to share these links with these notes with your fellow learners. You can do that by taking these notes, putting them somewhere, such as on Notion, or Medium, or GitHub, or whatever you prefer – some online page. You can put them there and just create a pull request with a link to your notes. Then somebody who maybe doesn't like watching videos can just go through your notes and read them. _x000D_
That's the idea. Even better, instead of adding links, you can add notes directly here, so people do not need to go to some external site – they have everything here in the repo. That’s the idea. I don't have time to make notes myself. That's why I added these notes. If you have time, if you want to contribute, and if you have some notes, you can contribute your notes and help everyone with that."
Data Engineering Zoomcamp;2022;For the project on week 7, are we free to choose the dataset or is it provided by DataTalks.Club?;"Alexey_x000D_
Yes, you're free to choose any dataset. A few people already reached out to me asking if what they had was a good dataset. And I think to everyone who asked me that, I said, “Yes, this is a good dataset.” There was one thread that was a question about images – whether it's a good idea to use images with this project. I don't think it's a good idea to use images, because there will be quite a lot of things you will need to do in addition to what we cover in the course, like image processing, and trying to extract some information from images. That is outside of the scope of the course and if this is the first time you’re working with images, it could be quite difficult. I don't think it's realistic that you'll be able to do a data engineering project and then this thing on top of that in two weeks. You might end up spending too much time on that. So maybe try to select a less ambitious dataset. If it's a table, a CSV file, or parquet file, then good. If it's images, it is probably better not to use it. If it's text – with text, you also need to do quite some work to process it. Maybe if it's just natural language, it’s also not a good idea to use it. For example, this common crawl dataset, there is a lot of just text, which probably could be not a great dataset, depending on what exactly you want to do. If you're not sure, maybe share your idea – share what you want to do – and then we can tell you if it's doable within a couple of weeks or not. I also want to share a DataTalks.Club’s Slack dump. There is also quite a lot of structured information that could be used. It's also grouped by days – for each day, there is a JSON file. You can build a nice pipeline for that. I'll probably make a dump and share it in the channel and you can see if it's something you want to use or not."
Data Engineering Zoomcamp;2022;In my project, I used a dataset for batch processing. To transform data that comes in batch I used DBT. Is it â€œbadâ€ not to have a Spark part in my project?;"Victoria_x000D_
No, it's not bad. Normally, I wouldn't say you will have both when you're working. It will depend a lot on the use cases and all that. Actually, we thought through the project in a way that you could use the technology that you want to use. Is it okay? It's okay if you just use DBT. Also it would have been okay if you would have only used Spark and not DBT. In the future, if you want to, you can always do the Spark part. It's kind of like the equivalent to the transformation that you've done in DBT. You can even use a Spark with DBT as well, if you want to get into that. I think that's okay._x000D_
Alexey_x000D_
I think it's quite common that you do some pre-processing while the data is still in the data lake. The raw data you pre-process with Spark, then you put this back to data lake again, but in a more processed/prepared format, and then you load this to a data warehouse and then in the data warehouse, you do find the final grouping, aggregations, and all these things in order to prepare the data for reports, dashboards, and so on – you do that with DBT. This is my understanding of how companies often do that._x000D_
Victoria_x000D_
Yes. Or I would say that they maybe use some for some kind of data Spark, and then some kind of data DBT or the combination of both and then some data goes directly to DBT and some just directly to Spark. So that would always depend on, as always, on the use case or the company._x000D_
Alexey_x000D_
This reminds me of a talk we had recently with Rahul. In this video with Rahul, he talks about modern data warehouse design. He mentions EMR – they use Spark for processing streaming things. They consume from Kafka and then here, they have DBT. Here, they have these generic ETLs. In principle, you can have something like Spark here, then he puts this to staging data, then you have DBT that gets data from the staging area to the domain area. So check the video if you haven't. Judging from the number of views, most of you probably already watched it._x000D_
Victoria_x000D_
[chuckles] I haven't watched it yet. _x000D_
Alexey_x000D_
Yeah, this is a good one. _x000D_
Victoria_x000D_
Yeah, I wanted to. I downloaded it for my client._x000D_
Alexey_x000D_
[chuckles] Cool. Rahul shared this link on his LinkedIn and a lot of people saw it. This post was quite popular, so that's why a lot of people saw this video. It's a cool video. Check it out."
Machine Learning Zoomcamp;2021;Why didn't we use Streamlit instead of Flask?;No reason. I just think Flask is the most popular framework for doing this. It's relatively simple. That's the main reason.
Machine Learning Zoomcamp;2021;How to select an approach for cross validation? (k-folds, leave p out, stratified, etc.);"So far, for week two and for week three, we will follow the simple approach. So the simple approach is what I described. We just take an entire dataset, and we split it into three parts: train, validation, and test. So this is what we'll use for week two and week three. In week four, we will talk about cross validation – I will not explain what that is right now. But what I want to say is that this approach to validation is sufficient for many cases. I think it's pretty safe to say that in many, many, many applications, just doing this split once is enough. _x000D_
Cross validation, of course, is nicer and we will learn more about this in week four. Right now, I will not go into detail and try to explain it. Wait for the video. So “how to select an approach for cross validation?” I would say that there are multiple options: k-fold… I'm not sure many people will understand that because we haven't covered that yet. But basically, if your dataset is big, then you don't need k-fold. If your dataset is smaller, then use k-fold. For “leave p out” – I've never used it personally."
Machine Learning Zoomcamp;2021;Is there any reason, from a technical perspective, why you would rather choose Gunicorn compared to other server managers, like Uvicorn, for example?;To be honest, no. I haven't used Uvicorn. I have used Gunicorn. I don't know if it matters. I haven't actually checked benchmarks. We use Gunicorn. We also use others – I don't remember which ones. There is no reason, basically.
Data Engineering Zoomcamp;2022;How much time do we need to dedicate every week to successfully complete the course?;"Ankush_x000D_
I think dedicating around two to three hours would be enough. Going through the videos will be less, but you might want to do the homework. And you might want to experiment on your own a bit with the technologies that you are not familiar with. That might take a bit more time. But on average, two to three hours per week should be enough to finish the course."
Data Engineering Zoomcamp;2022;What is your opinion on learning Scala for Spark?;"Alexey_x000D_
Yeah, you can learn Scala for Spark. It will be useful. I think we've had a discussion about this and Ankush said it’s definitely going to be useful. I remember one case when, in my previous company, we needed to rewrite something from Python to Scala and it improved the speed of this step significantly – just rewriting it. So knowing Scala is helpful, but I think you can get away with just Python for most of the stuff. It depends on how deep you want to go into Spark, and then maybe you can learn this. My personal opinion is that there are better ways to learn than Scala. [chuckles] Tastes differ. Victoria, do you have any suggestions or ideas? Should they learn Scala or something else?_x000D_
Victoria_x000D_
No, I think that's more of a data engineering focus, definitely. But I'm with you – if you learn Python and stuff. I also think if you come from a computer science background, it just depends, right? If you already know how to program and all of that, don't be scared if you had to pick up Scala, because you'll probably be able to do it anyway. Of course, like everything, you'll have to learn something, but don't be scared that you will not be able to know it or something or that people won't hire you because of that._x000D_
Alexey_x000D_
Yeah. To get hired, Python is sufficient. Also, talking about Scala – as I understood, for Kafka, packages in the Java world in JVM, for both Java, Scala and other JVM languages – are more advanced than Python. For some things, for example, for streaming, there are also libraries like Flink – they have Python integration, but still the Java API and the Scala API are better. For some use cases, you might end up learning Scala or Java. I would say – don't learn it until you need it. But if you are into programming languages, you can pick it up._x000D_
Victoria_x000D_
I think it's always better to know a few things well – like Python and SQL and stuff like that – and then pick the other things up. As opposed to knowing a bunch of things but just not very well. That's my strategy, I would say."
Data Engineering Zoomcamp;2022;I've learned many amazing things about data engineering from the Zoomcamp. May I ask, what are some areas and topics that have been left out of the scope that are also important?;"Alexey_x000D_
Do you have any ideas, Sejal?_x000D_
Sejal_x000D_
Yeah, I think we updated... Maybe you should move that section to the main readme page. You know that part where there are some additional things that we could do in our setup, such as writing unit tests, and the CI/CD framework, and so on and so forth? I think it's in the “project” section, but maybe we should move it to the parent branch. But those are some of the things that you can cover a bit further, I would say, to make a complete packaged environment for yourself, including the test cases and automatic deployment pipelines._x000D_
Alexey_x000D_
Yeah, I think it's a good idea to move it out of the project. Like “next steps,” right?_x000D_
Sejal_x000D_
Exactly. This is just something, based on that question, see how many things can be automated out of this. There might be some cases where you can run scheduled pipelines in Airflow (or whichever tool you're using) in order to generate more frequent reports (maybe weekly reports) for results that you need on a daily basis. Let's say you have a forecasting pipeline where you're estimating the stock prices a week from now, maybe you can have a scheduled cron job based on that. That's something that, with a dashboard, can reflect those predictions or any real-time stock market prices also on a weekly basis or on a daily basis for you. So that is also an app that you can create for yourself. Just an idea._x000D_
Alexey_x000D_
I'm thinking that there are a few things we didn't cover. For example, we didn't cover serverless, which is quite an interesting concept also for stream processing. Let's say you have a stream, and then you have a lambda function (or whatever, I don't remember what they are called in Google). So you have a function that you use for consuming from a stream, and then doing something, and then putting it to another stream. You can build quite complex pipelines from these streams without writing the consumer code yourself. You just write a lambda function that is applied to a stream and does something, and then internally, it handles everything you need to handle (to consume) and so on. This is quite a cool concept – serverless. Another thing we didn't cover that could be useful for data engineers is Kubernetes. Or a similar thing, for example, using AWS Batch on Amazon or using Kubernetes Jobs, or something like that. That also could be quite useful._x000D_
Sejal _x000D_
Yeah, +1 for Kubernetes. I think most companies are using it, preferring to use Kubernetes as the choice of cloud cluster instead of AWS. Having settled hosted solutions on Kubernetes is definitely a plus, so I definitely recommend learning it._x000D_
Alexey_x000D_
There is another thing I would like to recommend learning about, which is something we didn't cover at all. With serverless, the thing I mentioned – we kind of covered that a little bit in the streaming week. With Kubernetes, we kind of covered it a little bit in the batch week. We at least mentioned that you can use a Kubernetes job for batch jobs, or serverless for streaming. What we didn't cover at all is the concept of data monitoring, data quality checks, and things like this. There are a few nice tools that do that. For example, tomorrow, in DataTalks.Club, will have a webinar (a workshop) about data monitoring with whylogs. Whylogs is a tool for doing data monitoring. There are other nice tools, for example, maybe you've heard about Great Expectations, or Soda SQL – there are quite a few of them. This is something we didn't cover at all. Well, maybe a little bit in the DBT Week. We covered testing a bit. But I think this deserves more attention. This is something that many companies are looking into, so definitely worth checking out. Victoria, maybe you also have some ideas?_x000D_
Victoria_x000D_
Definitely data quality, I agree on that one. With DBT, you can do a lot. There are a bunch of packages as well. But, of course, you have to have an implementation. Other than that, observability. I'm not sure if you mentioned that. I guess you did as well. [Alexey mentions monitoring and observability] Also very good tools. _x000D_
Alexey_x000D_
Something we also didn't cover specifically, but mentioned a few times, are tools like Fivetran and so on. I think they're gaining popularity. Or Airbyte, which is an open source alternative for Fivetran. This is also worth checking out. But as you see, there are many, many, many things that didn't make it into the course that look quite useful. Then you might think, “Okay, now I need to learn five, six more topics.” Maybe you shouldn't just try to learn as much as possible. If your goal is to, let's say, work as a data engineer, you can already start applying to data engineering positions and then see what they ask you – what kind of things do they need from you that you don't know yet? There are thousands of things that you can potentially learn, and it's very difficult to select what the next one should be. So maybe you base your decision on whatever you hear from potential employers."
Data Engineering Zoomcamp;2022;If I have Kafka running on Virtual Machine 1 and Spark streaming on Virtual Machine 2, will I just use the external IP to connect the cluster or are there any other settings?;"Alexey_x000D_
I don't think I really understand the question. _x000D_
Ankush_x000D_
If you're in the same VPC, does it matter?_x000D_
Alexey_x000D_
But the cluster – is it a Spark cluster or which cluster? A Kafka cluster?_x000D_
Ankush_x000D_
If they're all running in the same VPC, they should be able to access each other's IP, right? _x000D_
Alexey_x000D_
Yeah. Oh, okay, externally IP. I guess if you're running inside the same network, then you can use just an internal IP. Or if it's inside Kubernetes, then you can just refer to them by names of deployments. I don't know. _x000D_
Ankush_x000D_
I also don't know. It's really specific to that particular use case. You need to give us more. Maybe on Slack you can explain it a bit more – Where are you running this? What kind of machines? And what exactly are  you exposing in terms of external IP?_x000D_
Alexey_x000D_
But usually, you have a Kafka cluster (some Kafka machines) and then you have a Spark cluster. And then the Spark cluster connects to Kafka and reads data from that. _x000D_
Ankush_x000D_
It just depends which VPC you are running in. If you're running in different VPCs, then you need to expose it separately. Then you need to expose the IP and all that. But if you are running in the same VPC, it should not matter. You should be able to access it internally. But I might be wrong. I'm no no DevOps. [chuckles]_x000D_
Alexey_x000D_
Usually, I take these things for granted, and maybe this is not a good thing. They work and I am very grateful to the data team that makes these tools work, but I often don't ask myself, “Okay, how is it actually configured?” Because it just works until it doesn't – when it doesn't, then I can go to the support channel and ask, “Hey, can you please fix it?” _x000D_
Ankush_x000D_
I think that's also not the data team, that's also maybe a DevOps team embedded inside the data team. Because you're working at OLX, which is a pretty big company. Maybe you have many smaller teams inside big teams? But I guess it's about a DevOps topic."
Machine Learning Zoomcamp;2021;Will you grade homework?;"Yes, I will. I haven't written the script for that yet. What I actually want to do is – many people asked me how they can see the results and I made a mistake at the beginning when I was letting people register. I didn't add a checkbox saying that it's okay to use some of the data (like first name and last name) in public. So I cannot really have a public leaderboard, where everyone can just go and check their score, simply because I didn't ask for permission to do that. What I want to do instead is to write a web service where there’ll be an email field. So you go there, you put your email, and you get the scores. This is only an idea. I hope I'll be able to implement this. _x000D_
Hopefully, it shouldn't be too difficult and you will be able to get your scores this way. But yes, I will grade the homework. I will store the scores of this course, because at the end, if you remember from the first video I talked about the public leaderboard at the end. Of course, for the 100 people on the leaderboard, I will ask them for their permission to publish their names. But before that, I'll need to figure out how exactly to communicate the results of each separate homework. For me, grading is actually not difficult. I want to write a script for doing that."
Machine Learning Zoomcamp;2021;Change the vector into a PIL image, maybe?;"Alexey_x000D_
I am not sure what this is. Maybe this is an answer to the thing we just talked about regarding filters. Well, you can use matplotlib for doing that. You take a filter and then…_x000D_
Dmitry_x000D_
I think PIL is a more interesting option. It's more to the images, I guess._x000D_
Alexey_x000D_
Yeah, but for PIL, it's more like if you want to save it as a JPEG, for example. But if you just want to plot it, to see how it looks, matplotlib is probably sufficient. In the homework, you also use matplotlib. You didn't use PIL for that. _x000D_
48:08 [0 upvotes]_x000D_
Can we use dogs vs cats as the main capstone project with better performance?_x000D_
Dmitry_x000D_
I guess, maybe the other datasets?_x000D_
Alexey_x000D_
Yeah. Are there other datasets?_x000D_
Dmitry_x000D_
I mean, there are a lot of open datasets, I guess._x000D_
Alexey_x000D_
With cats and dogs. Yeah._x000D_
Dmitry_x000D_
I mean, I think cats and dogs another time wouldn't be very interesting._x000D_
Alexey_x000D_
Yeah. But if you add more data there, that should be interesting._x000D_
Dmitry_x000D_
Maybe cats versus dogs versus something else?_x000D_
Alexey_x000D_
Who did you do? Wombats? Cats versus wombats. [chuckles] I think in the image net, there must be wombats, right? _x000D_
Dmitry_x000D_
Yeah, there should be._x000D_
Alexey_x000D_
So you can just take a part of the image net and get wombats from there? _x000D_
Dmitry_x000D_
Have fun._x000D_
Alexey_x000D_
[laughs] Or maybe wombats versus… What is similar to wombats? Because wombats and cats are quite different, right? They have different environments. So it's an easy task. _x000D_
Dmitry_x000D_
Maybe wildcats._x000D_
Alexey_x000D_
Yeah, I saw a wildcat once in a zoo. It looked like a normal cat. So I suspect that maybe they just put a normal cat there and said it's a wildcat. [chuckles]_x000D_
Dmitry_x000D_
Better marketing. [chuckles]"
Machine Learning Zoomcamp;2021;For the base.model, the shape was 32x5x5x2048 before average pooling. I just need to know why 5x5. Did we specify something somewhere?;"Alexey_x000D_
This is a question in the lecture. I use exception and the output of exception, when we don't include top, is this 5x5x2048. This is a long, long, long tube – a three-dimensional thing. Dmitry, do you know why it's 5x5? I think this is because they have these convolutional layers 1x1 – quite a few of them. At the end, they have 2000-something filters, and this is the result of applying these filters. So we still have a lot of feature maps. But I'm not completely sure. Do you know what's going on here, Dmitry? What happens in exception on the last layer?_x000D_
Dmitry_x000D_
No, I think it's because of the filters._x000D_
Alexey_x000D_
Yeah, so this is just a bunch of…_x000D_
Dmitry_x000D_
Convolutional nature._x000D_
Alexey_x000D_
Yeah. This is not something we specified. This is coming from the pretrained network."
Machine Learning Zoomcamp;2021;Why should the model be set as binary? Are there other options?;"Yeah, I think there are other options. I think I shared a link in Slack, you can check there. There are options. You can, for example, save your model as a JSON file, you can save it as the PMML standard (predictive model markup language) which is a different way of exporting models. I think it also works with Scikit Learn. There is a library for converting Scikit Learn models into this format. I find it easiest to just do pickle or you can also use joblib, which is essentially almost the same thing as just pickle. _x000D_
There are many advantages – the model becomes smaller. There are, of course, disadvantages that you become dependent on a particular version of the code. This is why Scikit Learn gives you warnings when you try to load a model that was saved with one version and you want to load it from a different version. That gives you warnings. Also, if you think about logistic regression, it’s just a bunch of coefficients and then the bias term. So what you can do is just export it yourself into a JSON file and then load it yourself. You can just write some code for that."
Data Engineering Zoomcamp;2022;A minor side note: any reason why you display Slido questions sorted by popular and not by recent?;"Alexey_x000D_
I guess this way, if there is a question that many people want to see answered, we'll cover it first. To me, it makes more sense._x000D_
Victoria_x000D_
Yeah, that makes sense. But I guess some of the questions were questions relating to topics that we’re answering and they get to the last row because you get to see them. That's maybe where it comes from. But it makes sense to do it the popular way because they get to vote."
Machine Learning Zoomcamp;2021;Do you use KDenlive for editing your videos?;As a matter of fact, I do. Soon I will be editing three or four right now. I don't remember. I think four. So this is what I use for editing videos. Frankly, I didn't find anything else on Linux, but it works surprisingly well.
Machine Learning Zoomcamp;2021;How do we know if we can solve some particular problems by using machine learning?;"That's an interesting question. The tricky part here is – in machine learning, you have this formula that you saw many times g(x)≈y. First of all, we need to have this thing here y. This is our target. If we can already think “What is the target we're predicting?” Let's say we want to understand what is in a picture. We have a picture and we want to build a model for classifying dogs and cats. In this case, our target would be dogs and cats – which is present in an image? So we need to think about this target y. This target is very important. If we know what we want to predict, then potentially it can be solved with machine learning. _x000D_
Basically, the answer to this question is that you need to think about the answer “What is the target variable that I'm trying to predict?” And “What features do I have?” (the latter is represented by x in g(x)≈y). You need to have both. You need to have a target y – this is what you want to predict and you need to have features x. If you can express your problem in these terms, as a feature matrix and as a target, then it can be solved with machine learning. If you cannot, then you’ll probably need to think more about this."
Machine Learning Zoomcamp;2021;Do we do EDA on the initial data frame or on df_full_train after splitting? What is the correct strategy?;So the correct strategy would be not to look at the testing dataset at all? Usually, let's say when I do EDA at work, say I use a SQL query to get the data on my computer, to do some initial analysis. In this SQL query, I only select the rows that I need for the training. I don't look at test at all. It's generally a good idea not to look at test at all because when you do this it’s called data snooping. This is bad because it might influence your decision and you don't want to do this. You don't want to influence your decision. Because maybe you look at it and think, “Okay, if I do these things, it will become better.” But maybe this is not the real pattern and when you see a new dataset, this pattern that you accidentally observed yourself is not true. So try to avoid snooping as much as possible. Use testing only to test your models. Avoid looking at tests at all costs, basically.
Data Engineering Zoomcamp;2022;I've been really enjoying learning throughout this course and I would like to continue after. Would you suggest formal education or just self-learning?;"Sejal_x000D_
My answer would be opinionated, but I have generally learned not via formal education, but just by trying different projects out, trying open source projects out on GitHub, or trying to do little things I can automate at work. Even if you're a data analyst or a data scientist, you can take on some automation, just opportunities wherever you see where you can engineer some pipelines. Then just proceed from there and see how you can help your current use cases out in the company or in self-learning as well. Just try things out on your own based on whatever tech stack, for example, that you want to learn or a particular core area that you want to learn. For example, a recommendations engine, or data engineering pipelines within the recommendations domain, or a search engine and so on, so forth. Just some thoughts on from the top of my head but maybe Ankush and Alexey you can add to that._x000D_
Ankush_x000D_
From what I can say, it basically depends on your end goal. With a formal education, you get the opportunity to meet people, you get the opportunity to learn from PhDs or from professors who are really proficient in their domain. You can learn the theory behind the practical. Right now, we are more focused on, let's say, a particular technology. Even if you're talking about theory, we are talking about that theory of that technology. We are not going deep into the theory of computers and the maths behind it. For example, a distributed system would have mathematics all behind it before actually even starting. So if you want to learn those things, then you have to go for a formal education and learn that from a good university. If your aim is to socialize, or to do a PhD, or to do a Master's afterwards, then formal education is also the right way because you build relationships there. But if your aim is to learn, especially learn in the domain where you want to work on specific technologies, or become a data engineer, let's say, then there is nothing better than self-taught. You will never learn the same things that you will use in work with formal education anyway. In formal education, the things you will learn are a bit different than what you will learn if you do self learning and focus on a particular subject._x000D_
Alexey_x000D_
I remember my education – my Master’s. I don't think most of the things I learned were useful, by which I mean directly applicable at work. They were useful. For example, we studied database internals. We actually needed to implement a database. That was fun. I'm not sure how useful that was for my work. It's also two years. Maybe after half a year, you realize that data engineering is not for you, so what do you do then? [chuckles] With self-learning, it's kind of safer, right? You can just say, “Okay, this is not for me. Let me try something else.” But if you're halfway through your Master’s, then you have this, “Okay, should I finish it? Should I not finish it? It's just one year. It becomes difficult. Then there’s this thing called Master’s thesis. [sighs] I remember how much trouble I had. I actually wanted to do a PhD. Before doing Master’s, I thought, “Okay, I'll do a Master’s and then I’ll do PhD.” And then when doing the Master's thesis, I realized that I hate writing papers. This is what basically PhD students do all the time. It was fun, but not something super useful for work._x000D_
Ankush_x000D_
And now you're thanking that Master’s is only for two years, right?_x000D_
Alexey_x000D_
Yeah, exactly. In Germany, it can take longer. Easily."
Data Engineering Zoomcamp;2022;Are there plans to write some articles on Medium related to this Zoomcamp? Articles from teachers and students?;"Alexey_x000D_
We don't have it in our curriculum. We only have a project at the end._x000D_
Victoria_x000D_
But people can write Medium articles. _x000D_
Alexey_x000D_
Of course, yeah. Maybe the question comes from the fact that in the other course, Machine Learning Zoomcamp, we had an article as one of the activities. But I think towards the end of the course people felt a bit tired of doing this. There were also three projects, not just one in that course. I think by the time you finish this, you will be pretty exhausted and you will not want to write articles. But if you do want to write, please write. _x000D_
You can write, of course. We can talk about this. If a lot of you want to do this, let us know. We can then think about whether we should incorporate this into the syllabus or not."
Machine Learning Zoomcamp;2021;Is it necessary as a data scientist in the workplace to have expertise in end-to-end â€“ from data collection, cleaning, modeling, until deployment?;I would say so. It's not a must – I would say that it's a big plus to get exposure into all these areas. I also think being able to deploy your models is a really crucial part here. That's why there is so much emphasis in this course on the deployment part, because you should be able to do this. As a data scientist, especially as a machine learning engineer, I'm sure you will probably have to work with Kubernetes at your job – or something similar to Kubernetes. So knowing how to do this is quite beneficial. You should learn that and if you have this experience, companies will get in a queue to hire you. I think that's very useful experience.
Machine Learning Zoomcamp;2021;How many project submissions were there this time?;44. Two people submitted twice. So actually, it's 42 unique submissions. I hope with the extension, many more people will be able to work on the project. I know that maybe many of you were putting things off until the last day and then something didn't work. It happens to me all the time as well. But if you now focus on putting things in Flask and then putting them in Docker, you should be good. If you’re training a model right now, stop it and go to the deployment part. Then you'll be good.
Data Engineering Zoomcamp;2022;From my research Spark can do what DBT can do. Can you use both Spark and DBT (ETLT) or what are the use cases where one is more preferable than the other?;"Alexey_x000D_
I don't know what the ETLT is but maybe I can summarize because this is something we discussed in Slack recently. I would say if you can write something in SQL and you already have things in your data warehouse and you're not afraid of costs – I think this is what Ankush wants to mention and he will probably talk more about that – but if you can do it with SQL and with a data warehouse or something like Hive or Presto, something managed like Athena in AWS, so that you don't need to worry about having this Spark cluster. It needs maintenance. You can use this managed Spark cluster from AWS or something like that. It gives you the opportunity to just write the SQL query and then put this SQL query to your Airflow or DBT and it just runs the things. But sometimes you need things that are very difficult to express with SQL. Sometimes you need more control over things. Then you go with Spark. _x000D_
Ankush_x000D_
Sometimes you just want to unit test your code so you write that in Spark. I think DBT is completely different from Spark. It's not a one to one comparison. DBT is something sequential. It's sequential SQL queries and it has many features that Spark would not offer out of the box. You can write, let's say, 20 steps in DBT and it's understandable using DBT Graph / DBT cloud. I think if you do the same thing in Spark, it would get hard to debug over time. So think about your use case. I think having a one to one comparison is not fair for either DBT or Spark, in this case._x000D_
Alexey_x000D_
In the end, both are used for batch processing, right? There are some similarities._x000D_
Sejal_x000D_
I think it also depends upon the larger use case. DBT is particularly a tool used by analytics engineers, and more from the standpoint of creating data marts or transformations for that could generate views for your analytical dashboards. Meanwhile Spark is more multi-purpose technology that can be used in ML as well, for distributed processing. Namely, its core value is in distributed processing. Just like Ankush said, it's not really comparable._x000D_
Alexey_x000D_
You also have this flexibility of using whatever you want from Python or from Java or whatever environment you use, which you do not have with DBT. In DBT, you can only use SQL. In BigQuery, you can have some user-defined functions, but they're pretty limited and hard to test and then it will turn out a nightmare at the end. Right?_x000D_
Ankush_x000D_
Absolutely. I think maybe we can also see it as something like this. If your company is using something like DBT and Airflow – go with that solution. If your company is only focused on Spark and you are very familiar with Spark, then feel free to explore more Spark, SQL and that area. It will depend upon the maturity of the team and the company as well – basically what you are doing at that moment._x000D_
Alexey_x000D_
A use case, when you don't yet have a Spark cluster, maybe it's a good idea to try to avoid having to maintain a Spark cluster. If you can express things with SQL and use serverless things like BigQuery or Athena or something like this, where you do not have to manage a Spark cluster, or YARN or whatever, I think it's better in the beginning. But then eventually, you will probably need to look into this direction and get a Spark cluster. As Ankush said, it depends on maturity. In the beginning, you probably don't want to deal with all that._x000D_
Ankush_x000D_
Yeah. It also depends upon what your clients are. If you have more analytical engineers in your company who are not familiar with Python and not familiar with Spark, then you also don't want to debug each and every use case they have or every issue they have. Maybe having DBT that can run SQL will be much better for you maintenance-wise. You might also want to compare things in that respect. And obviously, cost."
Machine Learning Zoomcamp;2021;Can you remind us of the prize for the first 100 in the leaderboard?;Glory. That's all. There is no other prize. Of course, if you're one of these 100 and you want to put your name there, I will create a page on the course page with your name. Yeah, that's it. A name and any link you want, like LinkedIn or Twitter, or whatever. That's the prize. Everyone who took two projects will get a certificate. It's not just the first 100, but anyone who completed the projects will get a certificate.
Data Engineering Zoomcamp;2022;During project review, will we be able to contact the project creator to ask questions, or is it anonymous?;"Alexey_x000D_
Well, it's not really anonymous because you will be asked to submit a link to your GitHub. I don't know if there was a need to ask questions in the previous course. I think you can ask for contact information in the project description. I don't know._x000D_
Ankush_x000D_
I think we should not encourage that because it might be unfair for different people. First of all, maybe you're not available to answer all the questions. Let's put it like this, if you are grading then maybe it's unfair, but if you want to learn, then it's definitely helpful. So I think the grading should be done based upon what is submitted, because it's also really important to document all the steps. _x000D_
Alexey_x000D_
Yeah, I guess if you need to contact the author, then it means that you cannot really give four points for reproducibility. _x000D_
Ankush_x000D_
In that case, maybe instructions should be clear in the report itself, not via contact. _x000D_
Alexey_x000D_
Yeah, exactly. But I don't remember this being a problem in the previous course. Maybe you can also create an issue in GitHub. I don't know."
Machine Learning Zoomcamp;2021;Can you clarify the difference between using Gunicorn or Waitress and plain Flask? I understand this is related to dev stuff, but I'm not sure.;The idea there is that Flask comes with a web service, but this service is very simple. If you send it multiple requests, I don't think it will be able to handle multiple requests at the same time. It will not be able to do multi-processing (set up multiple processes to handle multiple requests) and things like this. It's very limited in this sense. You can only use it for testing when you run things on your machine. If you want to deploy something to a production environment then you need to use Gunicorn, which is optimized for real production traffic. There are multiple requests coming in at the same time – 50 requests per second or 5000 requests per second. Flask will not be able to handle that. This is my understanding. To be honest, I didn't really look into this much.
Data Engineering Zoomcamp;2022;Do you have any advice or a story about your friends who shifted from DBA to data engineer? I want to shift my career after five years of working as DBA to data engineer?;"Alexey_x000D_
Ankush, do you know anyone who did a shift like that?_x000D_
Ankush_x000D_
Not really, no. I don't know anyone like that. Do you?_x000D_
Alexey_x000D_
I actually, I'm thinking now. People who shifted to data engineering are mostly software engineers – Java developers, for example. I also know a Python developer who because became a data engineering. I know a PHP developer who became a data engineer. Mostly, they come from software engineering. For DBAs, I usually see them become platform engineers or DevOps engineers, with more focus on the Ops part rather than the engineering part. Perhaps maybe for you, this will be a more natural transition. Maybe you first get into the infra side of things, and then go to data engineering. Maybe that._x000D_
Ankush_x000D_
As I always say, “data engineering” is a very broad term or job profile. You can have DataOps in it, which basically would be a good fit for a DBA. Then you have analytics, which is more focused towards analysis, and there you can write SQL scripts. So if you are already doing SQL as a DBA, then this shift would also be pretty natural for you with respect to at least working with SQL and writing pipelines in SQL. Then once you get more comfortable with it, start learning more Python. I think from there, you can just pick up different other technologies, which you think are appropriate for you or are interesting for you."
Machine Learning Zoomcamp;2021;Is AUC 93% and F1 63 good or bad?;It's suspicious. AUC looks quite good, but F1 score is below, compared to AUC. In general, an F1 score of 63 is good. Don't forget that AUC evaluates your model on all thresholds, but F1 score is just one threshold. Maybe the threshold you used for evaluating F1 is not the best one? If you play a bit with thresholds, you will see that actually your F1 is better. Actually, these numbers look good to me, so I cannot say they are bad. Also, when AUC is around 90% or slightly less, it can sometimes be a bit suspicious – but yeah, it's good. Maybe it's a bit too good. I think it's fine.
Data Engineering Zoomcamp;2022;Why is a table in BigQuery called an external table?;"Ankush_x000D_
The reason it’s called this way, because the data itself is lying externally in another storage, like Google Cloud Storage, in our use case. Once the data is imported inside BigQuery itself and BigQuery saves it in its own format, whatever the format is, that will basically be an internal table to BigQuery. That will normally just be called a table. But when the data is lying outside BigQuery, like in CSV format, in this case, they are calling external tables because the data source is external._x000D_
Alexey_x000D_
External tables are slower, and it's also more expensive to query them. In general, we would use these external tables only as a temporary place where we would keep our parquet files before they go to our data warehouse. Right?_x000D_
Ankush_x000D_
Exactly. Generally, that's definitely one of the use cases. You can also use it to load it into your internal tables. You can also use it to do some quick analysis, to see if your data is correct or not and things like this. Also, you can build external tables with partitioning and clustering. That can be a big problem in the future._x000D_
Victoria_x000D_
So in this case, the bucket is ours and all that, but you could technically have a bucket that someone shares or something like that and if they drop it, you lose it. You lost that table entirely because it was not in your storage. You didn't own it, technically. [Ankush agrees] I think that would also be something to consider._x000D_
Ankush_x000D_
Yeah. External tables are just metadata information. The metadata information would still exist if you create it. But if you try to query the table again – basically select star from this particular table that no longer exists, because the data source is deleted – the query will run into issues._x000D_
Alexey_x000D_
I ran into these issues before. It says “source does not exist” or something like this and it just refuses to execute the query. _x000D_
Ankush_x000D_
Exactly."
Machine Learning Zoomcamp;2021;How will the reviewers be assigned? Randomly?;Yes, randomly. I think that's the easiest way.
Machine Learning Zoomcamp;2021;Is there any threshold or some standards about how many features we can use to build a good model?;I don't think so. It really is problem-dependent. Sometimes adding a lot of features can hurt your model. You don't want to add too many features and just hope everything will work out. You need to be careful. Sometimes adding a feature can result in model degradation. But there is no standard. It really is problem-dependent. For some problems, you need to have a few features and for other problems, you need to have a lot of features. For example, we talked about online advertisements. For these models, they use millions of features, like the device type – they use one-hot encoding. And when you use one-hot encoding, you get millions of features. It's really problem-dependent.
Data Engineering Zoomcamp;2022;Any advice on how I could apply what I learned that the course? We will have a 3-week project at the end of the course. Could you suggest more project ideas?;"Alexey_x000D_
You can just take a look at the thread with datasets that we had in Slack and I’m pretty sure you will find something interesting there. Then just go from there. I would start from a dataset. First, I would find the dataset and then try to think what exactly I can do with this dataset._x000D_
Ankush_x000D_
I can say that if you finish one project where you are basically using more batch processing, then maybe the next project can be more real-time processing. Similarly if you're doing less Airflow, then do more Airflow. Something like that. Try to explore different areas to broaden your knowledge._x000D_
Sejal_x000D_
Nothing else to add from my end, really. Basically just what you guys said. I think it really depends on the needs are. The situation is based on what you feel like learning, what you think is important for you to learn in order to implement, with respect to what you're currently working on. I think you'll figure it out. In addition to what you're learning in this course, maybe you could also try learning some standard software engineering skills on your own, which can be applied in data engineering as well, such as orchestrating CI/CD pipelines, workflows, GitOps, automated test cases, especially unit tests, maybe with pytest. You can use pytest in combination with Airflow. Make your code more production-friendly, basically. We haven't really covered that. I'm not sure if we will be covering these modules in the next iterations. If we find the time, then definitely we will add it as bonus lectures. This is also something you can try out."
Machine Learning Zoomcamp;2021;Could we extend the project deadline by one day?;Most likely, no, because I think it will complicate the logistics. I also want to see how many people actually manage to submit by the deadline and then make a decision. But I am a bit hesitant to extend the deadline. The reason for that is, remember that we have a third project. If, for some reason, you do not have time, you can still do this project but just a part of the third project, for example. To be able to get a certificate, you need to do two projects out of three. The reason we have a third project is exactly for this – maybe somebody is behind and still wants to take part. That's why, if you don't have time now, you can do this as part of the third project.
Data Engineering Zoomcamp;2022;Is it sufficient to complete the final project to get the certificate for this course or one should also complete all the homework successfully to get better scores?;"Alexey_x000D_
Yeah, so you don't need to complete all the homeworks to get a certificate. The homework is for you to make sure that you understand what is going and to get feedback. In the homework, we try to ask questions to make sure that you understand the lesson. For example, one of the questions for this homework was to run Terraform in it. We want to make sure that you have set the environment to be able to continue in the following week. For example, if you already know some things but you want to maybe revise others, you don't have to do that homework. You just need to focus on what you want to learn. Then you will do a project and then you will get a certificate. This way, we want to emphasize the importance of doing projects, because this is how you actually learn. You might have the question, “But why do I need to bother with homework?” For homework, we give a little bit of incentive in terms of scores. That's basically it."
Data Engineering Zoomcamp;2022;When can we say to ourselves that we are junior or senior data engineers? Is there any scale of how we can know (Spark, Hadoop, certifications)?;"Alexey_x000D_
To me, it's not about certificates. It's not about whether you know Hadoop or not. It's more about how you can lead a project, be a technical leader in the project, make decisions there, and drive these decisions. When you can do this, then you’re a senior. When you can say “Okay, we want to use this technology, not this technology, because XYZ reasons.” Then you can call yourself a senior. Of course, it varies from company to company. In some companies like Amazon, a senior is already a leadership role. In other smaller companies, maybe a senior is less of a technical leader, but still. To answer your question about having a scale – you can just Google for “public career progression matrix”. I think Dropbox has a public one. There are some other companies that have it public and then you can check there. This is how I would answer that question._x000D_
Ankush _x000D_
Yeah, absolutely. It really depends, if you are talking in terms of the companies, then it really depends upon the company. But if you are talking in terms of personal consideration, then you can set up a scale based upon what you think specifically._x000D_
Alexey_x000D_
As for certificates, I personally don't care much when I’m hiring, for example._x000D_
Ankush_x000D_
I have zero certificates."
Data Engineering Zoomcamp;2022;Can you please elaborate a bit on the project review part (how, when, etc.)?;"Alexey_x000D_
I'll start with “when”. When? Next week, because we extended the deadline by one week. This means that many of you are still working on the project. The deadline for that is next Monday, April 4th, 2022. That's the deadline. After that, the review starts. The review will start next Monday and on the Monday after that. _x000D_
When it comes to “how,” we have the guidelines (criteria) that you will need to look at. There will be a Google form. You will need to enter the ID of the project that you're reviewing. There will be a section in the forum that will say “For problem description, is it 0.1 point or 2 points?” and you will have something like this for each of the criteria. In the previous course, for problem description, you would need to select one of the points and for each dimension, you would need to select the option given. _x000D_
At the end, we have a script that takes some sort of average from all these points. This is how we get your final score. To see what projects you were assigned to review, we will have a table, where there is a hash of your email and the hash of the project that you need to review."
Machine Learning Zoomcamp;2021;Could you give us a breakdown of what is left in the course and when we're expected to finish?;"While Ninad was presenting, I did a quick sketch of what is still left to do. Today, we're starting with the capstone project, and then there will be peer reviewing. For the capstone project, we will have two weeks. The capstone project starts today and we will have two weeks. Then we will have one week to review the project. I am not sure this time that we will be able to extend it because in many countries, there will be Christmas break, which starts on the week when we should finish the peer reviewing. I'm not sure how flexible we can be here with the capstone project. So please try to finish before the deadline because I'm not sure we'll be able to extend it. The reason I shuffled things around a bit is because of that. I know that many people will have vacation and holidays on Christmas break. That's why we should focus on the capstone project right now. There is that. By now you probably have an idea of what is expected, because we already did the midterm project. I hope you got inspired by the two presentations we had right now. _x000D_
In terms of other things, Kubernetes and Kubeflow will still be there. For Kubernetes, I will try to record everything this week and make it available. If somebody wants to use Kubernetes in their capstone project, you should be able to do that hopefully. But the homework for Kubernetes, I'll publish the videos now. But you will have a lot of time to finish it. Actually, Let's say if your Christmas break is at the end of December, then the first week of January, you can spend on solving the Kubernetes homework, or however you prefer. I know in some countries, the break actually starts on the 31st of December and then finishes like on the 10th of January. In Russia, for example, it’s like that. You're free to choose how exactly you want to spend your time to do Kubernetes homework. _x000D_
For Kubeflow – setting up Kubeflow is really difficult. If you're really motivated, you can follow the videos and do that, but I will not have you do the homework. There will be no required homework – only optional, if you want to practice Kubeflow. There will be something, but no graded homework. Also, I promised a third project. For those who were a bit behind with the midterm projects and didn't have time to finish it for various reasons, we will have a third project. If you just like projects, you can also do that. It's not like you have to do only two out of three, you can do three as well. For the third project, officially, we will start it on the 10th of January. Of course you can start it earlier because it will be exactly the same as the midterm or capstone project, just a different dataset. We'll have one week for peer-reviewing it. We should finish the entire course by the 31st of January. _x000D_
The last thing – articles. For the article, you just take a topic that we haven't covered during the course and you cover it – write a short article about it with code and so on. For that, I haven't really figured out the logistics yet, but this is something I want to prepare now. You have all this time – December and January – to work on the article. If you want to do this, this is of course, optional. You don't have to do this – only if you want to. You’ll have two months for that, basically. _x000D_
Just to remind you – to get the certificate for the course, you need to finish two out of three projects. That's the main requirement and the article is optional – you will get some extra points for that as well. That's roughly the plan."
Data Engineering Zoomcamp;2022;How does one move from a junior data engineer to a senior data engineer and then to a lead engineer?;"Ankush_x000D_
Vic has experience with being a lead right now, so feel free to jump in. I think one of the most important things about being a data engineer is to know a lot of things and you need to know different stuff. You might want to be an expert in Spark, but you still would need some knowledge of Kafka. If you want to be an expert in analytical engineering and go deep into DBT, SQL, but you still need knowledge of data warehousing. To go towards a senior data engineer, I do feel that you need at least some knowledge of the other areas, but you also need a core competency in at least one or two of the areas. For me, when I was a mid-level, and I wanted to go to a senior, I realized that my company really depended upon Kafka. Luckily, that was also an interesting area for me. I took a lot of interest and developed a competency into that. But at the same time, I was also interested in data lakes in Spark and that gave me the opportunity to work on projects which could then lead to me being a senior data engineer, and then also leading a team later on._x000D_
Alexey_x000D_
I would also add that you should check out the DataTalks podcast, where we cover things like that. Maybe not all of them are specific to data engineers, but I think you can find some of the talks interesting. Maybe this should be another podcast episode, because I don't think we can cover that in detail in 10 minutes."
Machine Learning Zoomcamp;2021;How are we expected to document model performance in case we use a competition dataset since test labels are not available and we have to submit predictions for score?;In this case, you take the dataset that is available on Kaggle and you pretend that this is the only data you have. Then you do the split into the usual thing – test, validation, and train – or just test and full train. And then you decide how you want to validate your models using cross-validation, just holding out one part of the dataset. Just the usual thing. You don't have to rely on the test dataset. If you want to submit your model and then just show (maybe take a screenshot) your final performance, that's also fine. But it's not required.
Machine Learning Zoomcamp;2021;How to get better at coding? I understand all your lectures, but when it comes to implementing in Python it takes quite a long time.;You just need to practice. I don't think there is a better way to do it.
Data Engineering Zoomcamp;2022;Would you say DBT is the next big thing in the data stack?;"Victoria_x000D_
Yes."
Data Engineering Zoomcamp;2022;I failed to deliver the homework so far, hence, I missed the deadline. Is it over for me?;"Alexey_x000D_
It's not over for you, because we only want to look at your project. If you're behind right now, don't worry. Just take your time. We will decide whether to give a certificate or not only based on projects, not on homework. So it's not over for you – you can catch up. _x000D_
Week 3 should be lighter. Week 1, you can maybe fast-forward directly to Docker Compose and then run it there. Week 2, depending on how much time you have, maybe you can go through the solution that will be published today and then it will give you everything you need for week 3, or you can use a transfer service. The video is already there. While watching the video, you will probably learn a lot already, like the homework solution."
Machine Learning Zoomcamp;2021;When converting from dataframes to NumPy arrays, is it better to use df.values or df.to_numpy()?;I don't think there is any difference, to be honest.
Machine Learning Zoomcamp;2021;Can we write an article, save it on GitHub and share on LinkedIn?;Yes, I think writing an article on GitHub is good. You can basically have an article there. I would suggest trying Medium. It's not as difficult as it seems. On Medium, they can also start recommending your article, so more people will see it. But it's up to you. GitHub is fine. If you want, you can actually turn any GitHub repo into a website. It's called GitHub Pages. You can just take a look at that. Maybe instead of just putting your article in a Git repo, it's possible to turn this Git page into a blog page. You will need to follow this instruction. For me, it's not difficult. If you don’t not come from an engineering background or you haven't done this before and you're not very comfortable using the command line, it will take more time for you to actually figure this out. But I think it's worth investing some time into seeing how it works because you can easily have your own website for free – entirely for free. You don't need to do anything else. Actually, I have a secret. It's not a secret, actually. The DataTalks.Club site is actually a page that is run on GitHub Pages. Everything you see on the website is on GitHub. I use GitHub Pages to host it and it's free. All I need to pay is for the domain – DataTalks.Club – and the rest GitHub provides for free. I hope I convinced you to try it
Machine Learning Zoomcamp;2021;If two numerical features are highly correlated between each other, shall we drop one of them?;Use cross-validation to check that. If on validation, you get better scores – if you drop one, drop one of them and then go for this. Sometimes it makes sense to drop even if there is a decrease in your score simply because maybe one feature is more complex to compute than another one.
Data Engineering Zoomcamp;2022;During the review, is it expected for us to try and install/run the code following instructions? Or is it fine to just review the documentation?;"Alexey_x000D_
Well, it depends on how much time you have. Ideally, yes, you should try to run it. You should try to learn as much as possible from this code. I'm pretty sure if you try to run it, you will learn a lot from the other projects. But I know if you do that for all three projects, then you might not have time to actually, you know – have a life. So it's up to you. You can just check the documentation and see that things make sense there, and that you don't see any big mistakes that just jump out at you. You can do that, or you can run everything. It's up to you. _x000D_
But if you have time, please run it, and please try to learn as much as possible from your peers. Based on what I heard from students from the Machine Learning Zoomcamp course, they learned a lot from trying to run the projects. So please do that. By the way, I saw that one of the projects was run on the Yandex Cloud. I don't think it would be possible for you to actually try to reproduce it and run it. In some cases, you will not even be able to do that. If that's the case for you, then the only thing you can do is just review the documentation."
Data Engineering Zoomcamp;2022;I have missed the current Zoomcamp (2022). Will there be another one in the near future?;Maybe. Maybe not. We will see. But all the content is available.
Data Engineering Zoomcamp;2022;What is the best strategy to update records on a parquet file or BigQuery? How about appending data to parquet files?;"Ankush_x000D_
Can you append to a parquet file?_x000D_
Sejal_x000D_
I think what they probably meant was – I think Alexey and I were also talking about this – for incremental loads, since the end result of week 2 has a BigQuery external table, I think by default, it's not in append-only mode. Right, Alexey?_x000D_
Alexey_x000D_
When I was executing it, I saw… I executed a bunch of DAGs runs and then when I went to BigQuery to see the results, it was the same table that we created after the first DAG run. Basically, it didn't append to that table for some reason. I don't know why. But then I think I saw in the week 3 videos that the way we create the statement when we create a table, it needs to use some kind of sort of wildcard. _x000D_
So we say that, “It's 2019, then minus 10*” and then BigQuery picks up all the files with this pattern. This is probably how you typically do this. I don't know – I'm not a BigQuery expert. But at least for week 2, you will need to upload data to Google Cloud Storage. You will not need to create these BigQuery tables. Then we will create a BigQuery table in week 3, and Ankush will show you – I saw this in the video. There is a SQL statement for creating the table._x000D_
Ankush_x000D_
In BigQuery, yes, you need to use wildcards to pick up multiple files. If that's the aim of doing it, I would still not append to a parquet file, because parquet files are not meant for that purpose. They're not append-only kinds of file, they're more like columnar storage. Basically, if you append, you need to rewrite the whole thing. In that case, I would not suggest that. Basically, keep it open-ended. Close the file after you're done writing to it and then probably let BigQuery do the read. _x000D_
On the same side, if you are writing to a BigQuery table internally, and I think that's also covered later on in the videos, what BigQuery does internally is figures out after a couple of writes (let's say an hour or so) it figures out that there is an imbalance between your internal file structure, and it reclusters (automatically reshuffles) the data and creates a new file for you. So you don't really have to worry about them. There is no cost on your side for it. But that's how internally BigQuery does that, if that was the question._x000D_
Alexey_x000D_
Maybe I can add a few words to that. The way I see it usually happens is, you have a folder in a data lake for a specific date, let's say. First, the structure is the name of whatever table, then year, month, day. And then within this folder, you have a bunch of parquet files. Then if you want to add more information, you just add another parquet file to that folder. This way, you get more data to this partition."
Machine Learning Zoomcamp;2021;Linux Mint works way better than Ubuntu. It is based on Ubuntu, but it is more user-friendly and better in many other ways.;Yeah. If you want to experiment with Linux, you can try Mint. I'm using Ubuntu, as you can see. Mint is probably good as well.
Data Engineering Zoomcamp;2022;How important is Docker to use for data engineering? For example, installing Hadoop or Spark on Docker?;"Ankush_x000D_
I think in today's time, if you're doing any sort of software engineering or data engineering or back end engineering, you need some knowledge of Docker. Because the world has moved around us in a way that you need to build your application, you need to deploy your application, and you need to maintain your application. And the easiest way to do this is to use Docker. I would say it is definitely important. Would you install Hadoop or Spark on Docker, maybe not? Maybe you will use dedicated hardware to do this, and you will not do it on Docker. But learning Docker, there is definitely no way around it. I think you need to have at least basic knowledge of Docker. I myself am not an expert in Docker, but I do feel that knowing the basics, knowing how to get around it, and knowing basic commands, will really help you to grow in your career._x000D_
Alexey_x000D_
Maybe I could add a point of view of a data scientist. In data science, we often need to use libraries (things like XGBoost) that require some sort of native dependencies. Native dependencies are these things that you install with AptGet install in Linux and Ubuntu (like OpenMP and things like this). Let's say I want to use XGBoost in Spark, then I have a lot of trouble like, “How do I prepare my package with all the dependencies in such a way that I can submit to the Spark cluster and it doesn't fail?” When I found out that in Spark, we can actually submit Docker containers – we can prepare and package all the dependencies in Docker, and then submit it to Spark and then Spark would run it, so Spark would use all the dependencies specified with Docker – then my life became a lot easier. Then I didn't even need to worry about how exactly I prepared the ZIP archive. For those who don't know Spark, maybe what I'm saying doesn't make much sense for you. But believe me, my life became easier because I can just prepare a Docker file instead of figuring out how exactly I should run AptGet on Spark nodes. That was quite helpful for me, personally._x000D_
Sejal_x000D_
What to do after that? I'm sure you must have read about the advantages of Docker and why it is useful in every stage of software engineering, be it backend or frontend or even infrastructure. It has a lot of advantages in terms of usability and portability. Also, it really downsizes your effort in terms of preparing deployment-based code. Very low maintenance and you can mock your production environment onto your Docker version and test things out without having to deploy it to production. This is why I would say it’s important."
Data Engineering Zoomcamp;2022;I want to understand peer project review for late joiners. How will it work?;"Alexey_x000D_
For homework, we will use automatic grading. You will not need to review each other’s homework assignments. The project is something that we’re planning for week 7 to week 10, so basically the end of the course. Nobody's late there yet because it hasn't happened yet. So don't worry about that."
Machine Learning Zoomcamp;2021;In cases of imbalanced datasets, when do we apply SMOTE/downsampling techniques â€“ before splitting data or after?;Always after. Let's take this case. [image 1] Here, you split the data, so you have your test dataset. Then you split the data again – you have the validation dataset. Let me just do that here, just to show it. This would be a data frame. And now we'll split this data frame. Use 20% and 25% here, then validation. Okay, and now have data frame validation. [image 2] It’s still the same. Now all this SMOTE, downsampling, upsampling, and so on – you do only on this data frame train. You do not touch data frame validation at all. So you experiment with this. You need to make sure that in validation, your distribution of the target variable stays the same, because this is the distribution of the target variable you will see in real-world data. You don't want to change this, because if you change this, then you will not be able to evaluate your model against a real world scenario. It's very important that you do not change validation here and you only change train. Here, you can do downsampling, upsampling, SMOTE, and so on. You do this, you train your model, and then you test it on the validation dataset set. Then you check whatever target metric you want and when you train a full model, you do the same. You keep your test dataset intact – you don't touch it, you don't apply any downsampling there. You only apply all your techniques to the data frame train full.
Machine Learning Zoomcamp;2021;There have been a couple of models of deployment you covered in the course. What is the preferred industry option? Is it Kubernetes through cloud?;"This is what mostly I see – usually companies prefer to go with Kubernetes, especially bigger companies, where they have people who can support the Kubernetes cluster. This is quite a popular approach. This is why I decided on covering when I was thinking whether I should cover Kubernetes or not because this is a fairly advanced topic and it's very difficult to cover Kubernetes in just one lecture. Similar to neural networks, it was not very easy. I think it's quite important to get some understanding of how Kubernetes works and to not be afraid of using kubectl. _x000D_
I would say, yes, it is a preferred industry option. Other options are also quite popular. For example, at OLX, where I work, we use Lambda. We don't use Elastic Beanstalk, for some reason, but we do use Lambda. We do use Kubernetes a lot, and we usually deploy things either through FastAPI or Flask. Mostly Flask, but we have a couple of projects that use FastAPI. Go with Kubernetes, although if you're doing a project alone and you don't have people who know Kubernetes quite well, I wouldn't advise using Kubernetes. Use something simpler, like maybe Lambda. That would be a better option."
Machine Learning Zoomcamp;2021;How many models and how much parameter tuning are you expecting?;More than one? In this course, we covered linear models and we covered tree-based models. In linear models we saw the C parameter, or if it's rich regression, I think it's called alpha – you should tune that. And then you should try decision tree, you should try random forest and XGBoost, and then different parameters, and then select the best model. Basically, just going through what we learned so far, and trying different models, and then different parameters for these models. You don't have to use XGBoost, for example – you can just use random forest. I suggest using XGBoost, or some gradient-boosting tree implementation. Just try, let's say, at least three different models and then for each model, try to tune the parameters. That should be sufficient.
Machine Learning Zoomcamp;2021;How many submissions were there this week?;I think around 200? 204, I think. Something like this.
Machine Learning Zoomcamp;2021;What are some things that we should look into in order to select a good dataset?;For example, if you go on Kaggle and you see the number of notebooks available for the particular competition, it could be a good indicator that it's a good dataset. Other than that, we didn't really talk about multi-class classification. I think it's okay if you want to do multi-class classification, but the first dataset should probably be either a binary classification (if you see that the target is clearly zero or one) then I think you can go with this because you already know how to deal with numerical variables, you know how to deal with categorical variables, you have some idea of how to deal with missing data. So if you see that this dataset has these things and it's similar to what we did before, this dataset is a good one. If you're not sure, just ask in Slack and we will help you.
Data Engineering Zoomcamp;2022;Avro versus Parquet?;"Ankush_x000D_
Avro and Parquet are totally different things. Avro is row-based and Parquet is columnar, so if you are doing something like batch processing or data-analytical kind of work, Parquet is a perfect solution to use with Spark or something because it's super fast, to calculate something like a sum or account from a Parquet file or read a particular column rather than the whole column or the whole row itself. Avro, on the other hand, is very strong at providing flexibility in terms of backward and forward compatibility. So if you're using Avro with Kafka, or using something like a Protobuf with Kafka, that would be really useful. So think of them like different technologies altogether and use it for different use cases._x000D_
Alexey_x000D_
And Avro, I guess, the use case would be streaming and Parquet is for batch jobs?_x000D_
Ankush_x000D_
Yeah, I think definitely, in the beginning, that would be the basic use cases. Yes._x000D_
Alexey_x000D_
There is also something called OCR. _x000D_
Ankush_x000D_
That's also columnar storage. _x000D_
Alexey_x000D_
Oh, that's a different ORC. [chuckles] I remember watching a presentation on Berlin Buzzwords about this format. I remember seeing how cool it was in the presentation, but I never actually tried it._x000D_
Ankush_x000D_
Neither did I._x000D_
Alexey_x000D_
So it's an Optimized Row Columnar. Interesting. I never knew what it stood for. But is it widely used or most people just go with Parquet? _x000D_
Ankush_x000D_
I have seen only Parquet until now. But I guess there will be some companies which will definitely be using it. It's not a bad technology at all. I think it's just that Parquet is famous and there are more compatible solutions for Parquet so people just tend to use that more often."
Machine Learning Zoomcamp;2021;Is the use of a WSGI server like Gunicorn or Waitress still required for deployment or can we deploy Flask app directly on Cloud?;I think it is required. It depends on the cloud. If you want to deploy a Docker image to the cloud, then you need to put Gunicorn or Waitress inside Docker because as you saw this warning, Flask says do not use it for production. So you do not use it for production because the Flask developers will cry if you do this.
Machine Learning Zoomcamp;2021;Can you please upload the live session recording and homework answers on GitHub?;Yes, of course. That's what I'll do today.
Data Engineering Zoomcamp;2022;How does AWS Step Functions compare to Airflow? If I have my infrastructure fully on AWS, should I use Airflow? If so, what would be a good case?;"Sejal _x000D_
In my previous setup, like I said, we had a bunch of Lambda functions where the execution was coordinated by Step Functions. This is how it would work in the case of Airflow._x000D_
Ankush_x000D_
I have not used step functions at all. And honestly, I've also not dug deep into Cloud Composite of Google Cloud Platform. But there are certain criteria you need to choose and one of the biggest criteria is price. If you think that your price would be super less in comparison to the price you will pay for maintenance, for developing this, for deploying this, and most probably, scaling it in the future, then go ahead and use AWS Step Function. _x000D_
But if the price is just too high for your requirements or your use case, then maybe digging deeper into your own deployment of Airflow would be better. That would really depend upon your use case and how you calculate price._x000D_
Alexey_x000D_
I think AWS Step Functions are relatively cheap, but they do not have as convenient a UI as Airflow. Things are more complex there. This was my impression of not using them personally, but seeing a demo of AWS Step Functions at work. We tried one project and we decided not to use them, because it wasn't as easy to configure these workflows. But I guess we’re also more used to Airflow."
Data Engineering Zoomcamp;2022;I will be extracting weekly Reddit data for my project and appending this to a data warehouse table. However, I'm not sure what kind of DBT transformations are required.;"Alexey_x000D_
It depends on what exactly you want to do with the Reddit data, right?_x000D_
Ankush_x000D_
Yeah, exactly. I would focus on the final goal. Let's say, “I want to calculate for Reddit data.” You might want to say “What are the most keywords being used?” Or something like that. In that case, your transformations will directly depend on that._x000D_
Alexey_x000D_
Or something like, “Active number of posts per day.” _x000D_
Ankush_x000D_
Exactly, or, “Per hour distribution of posts.” I'm pretty sure that in the US, it goes up during the daytime and it goes down at night. So what are you trying to answer? That's the first question I would focus on. Once you know the answer to that, then I will know which transformations to apply._x000D_
Alexey_x000D_
Yeah. That's why the end goal of the project is a dashboard, so “What exactly do you want to put on the dashboard?” and then go from there. “For this dashboard and for the data you use, what kind of transformations do you need to do in order to go from this data to the dashboard?” Then it will become clear that you need to do this group-by, that you need to join with this or that table, and so on."
Data Engineering Zoomcamp;2022;What are similar tools in AWS and Azure for DBT?;"Victoria_x000D_
You just, again, set up a warehouse and then at the end, you define the connection. The only difference is that you would have to use the adapter, but DBT has several adapters. What I would do first, if you want to use DBT, is go check the adapters and make sure that it's supported. There are several that are official. For example, Redshift you could use. There are some that are community-supported and that where Azure is._x000D_
Alexey_x000D_
Yeah, this is AWS, so Athena. Azure and Athena are here. Redshift and Presto here as well. For example, we have a Presto cluster, so we use AWS – and within AWS, we have our own Presto cluster. Potentially, if you have a similar setup, then you can connect DBT with Presto._x000D_
Victoria_x000D_
In general, to add a little bit more to the answer. Let's say you have the project right now, and you have it in BigQuery. What you would do is change the profile and that's it. Then you set up the connection. You may have to make some changes if there's something in SQL that supports BigQuery or not the other way around. But other than that, you will be able to run the project in a few minutes."
Data Engineering Zoomcamp;2022;If Kubernetes can spin up containers when needed, what is the use case for Terraform?;"Sejal_x000D_
Maybe someone can help me answer this because I am not really an expert on the Kubernetes side. But Terraform lets you build static code templates for your infrastructure in an IEC-style manner. So it would work in cases where you want to have a static-based infrastructure or in cases where a set of infrastructure resources are destroyed at any point in time and you want to restore it from a certain image or a certain state – Terraform is useful here. In the case of Kubernetes, it's actually more like a spin-up cluster. You can deploy your services onto a Kubernetes cluster. The use cases are really different in this case. Maybe Ankush or Alexey can add more on Kubernetes._x000D_
Ankush_x000D_
Yeah, definitely. I think Kubernetes and Terraform are very different. When you think about Kubernetes, you are thinking about deploying microservices or some sort of applications. That can be done by Terraform using AWS Fargate or AWS ECS solutions. But Terraform does very much more. Let’s assume that you do not have Kubernetes, then you will use Terraform to set up your Kubernetes cluster. But you will also use Terraform to set up, let's say, S3 buckets. In next week’s course, you will see that we use Terraform to set up a transfer service from Google Cloud Platform. All of this is not possible in Kubernetes. Yes, you can argue that there are certain services like Spark and Flink that are coming up with these new solutions, which can be run on Kubernetes. But still. For example, say you want to run Kafka clusters. In those cases, Kubernetes is not a good solution. You would like to have a more stable solution or a stable infrastructure in that case. That's where Terraform would definitely help you to set these kinds of clusters up._x000D_
Alexey_x000D_
As a person who is quite far from infrastructure, I would add that usually we have people dedicated to that in our company. Terraform helps you not go to the web UI and click things. Instead of going there and clicking, you just have a file where you say that “I have this bunch of resources that I set up in our cloud.” Then, let's say, if you need to move from one account to another for whatever reason – this has happened multiple times at my work, when we needed to migrate to a different account, so we just do Terraform destroy in one account. Actually, we would destroy later, but we will do Terraform “plan and apply” in one account, and then go back to the old account and do Terraform destroy. Of course, it still takes some time, but you have everything in your code and you know exactly what kind of services you use, for example, what kind of buckets there are. We use AWS so for us it would be what kind of lambda functions there are. Basically, all the resources that are there. Kubernetes could be just one resource in this Terraform file. There is a comment that says, “Terraform is infrastructure as code and Kubernetes is infrastructure.” I think it's quite a concise way of summarizing it."
Data Engineering Zoomcamp;2022;In the original plan, the project was going to start this week for three weeks. What do you think is going to be more or less the current plan for the Zoomcamp?;"Alexey_x000D_
Well, I think now we finished Kafka week – I think we should have called them modules, not weeks, because they didn't really correspond one-to-one to physical calendar weeks. Now, when we finish Kafka week, then we'll start the project. I think three weeks is reasonable, because we don't want you to spend too much on working and working on projects. Instead of being too ambitious, maybe you should think, “Okay, what can I actually do in two weeks?” The third week is for reviewing your peers. Let's say right now – today is the deadline for Spark. And then for Kafka, let's say the deadline is next week, so we will have until March 27 to finish the project, and then one last week of March to do peer reviewing. That's roughly the plan. We will, of course, see how it goes and maybe adjust. But for now, that's the plan."
Data Engineering Zoomcamp;2022;How much time do we have to submit the final project?;"Alexey_x000D_
You mean how much time do you get? The deadline would be in two weeks, so it will be 28th of March, 2022. Then you will have one week for the peer reviewing."
Machine Learning Zoomcamp;2021;Consider train data for which some validation needs to be separated for validation. Should EDA be done on the entire train or after splitting into validation?;I don't think I understand the question. Some validation needs to be separated through, I don't know what you mean so maybe just ask in Slack. I think if you're not sure, just do EDA on the training dataset – on the first train dataset, not on the full. But if you do it on the full train dataset, nothing bad will happen.
Machine Learning Zoomcamp;2021;Is there a hash tag or something similar to find the public learning contents of others?;Yes, there is a hashtag. The hashtag is #mlzoomcamp. You can use that – put that in Twitter, in LinkedIn and I think you'll find some posts there. And of course, you can use this hashtag for your own social posts as well.
Machine Learning Zoomcamp;2021;With respect to encoding, should we encode after splitting or split after encoding the entire dataset?;As I said, take your test data and put it away – don't look at this at all. For your validation, you can look at this, but do not use it for encoding. Do not use it for anything. You only want to apply what you learned on the training dataset. Let me draw it. [image 7] This one “test,” you take it and you hide it. You forget about it. Then “validation,” you only do transformations here. Here, you will do things like fit and so on. And here, you only do transform. Here, you never fit anything (on the validation dataset) you only fit things on “train.” You fit your model, you fit your transformers (like dictionary vectorizer is a transformer because it has this transform method). You only transform the validation dataset. Of course, for test, you also transform. You don't train on this, but you hide it away. You put it away, you don't look at this.
Data Engineering Zoomcamp;2022;Do you know any literature for testing in data pipelines?;"Ankush_x000D_
What we use is basically unit testing. We rigorously unit test other small components and then there can be an end-to-end test in order to test the whole pipeline, basically. So that's the literature. [chuckles]_x000D_
Alexey_x000D_
I think there is a thing called Data Kitchen. They have a book, I think. They have this DataOps Cookbook, and I think it goes a bit into data quality topics. I think they talk a bit about that. But I don't remember how detailed it is. Maybe this is something you can check. Have you seen this one?_x000D_
Ankush_x000D_
No, I'm just checking it out right now. So what's the point of Data Kitchen?_x000D_
Alexey_x000D_
I think they're consultants. “Stop band-aiding in your data pipeline.” I think they consult on how to do DataOps properly. Here, of course, you need to provide your email so they can try to get you as the client. [chuckles] But multiple people told me that this is a useful book."
Data Engineering Zoomcamp;2022;Have you ever seen a data engineering position that required Spark or Kafka?;"Victoria_x000D_
Yeah, I was at one in software that uses Spark. I don't use Spark, but I still got the job. I didn't take it. Going back to what you said before – no one is going to reject you if you don't use exactly what they use. What's more important, I guess, is that you can learn it. Regarding Kafka, it's important that you understand what's going on because you'll build a pipeline on top of Kafka as an analytics engineer. It depends on how much your data engineers will work, but you'll still have to consume that data. As an analytics engineer, you are going to be the bridge between those data engineers and the analysts, so it's very important that maybe not that you're an expert in Kafka, but that you understand what Kafka is. That would be the least you would have to know. I wouldn't recommend anyone to go and learn Spark if they are mostly focused on analytics engineering. As I said before, you need to understand the concept. It's more important that you understand what you have to model, how to do it, and all of these things."
Machine Learning Zoomcamp;2021;What is the deadline for the midterm project?;It’s two weeks from now (October 18, 2021) meaning the first of November, 2022
Data Engineering Zoomcamp;2022;Can you please make a video on Docker?;"Alexey_x000D_
I'm very glad you asked that, because I think we actually did a few videos about that in week one. So check it out."
Machine Learning Zoomcamp;2021;Any guidance on how many *.py files should a notebook be broken down into when deploying?;I don't think there's any rule of thumb. If your notebook is doing multiple things, then you deploy… Basically, each Python file should ideally be doing one single thing. There should be one Python file for training, for example, one Python file for testing the model – or something like this. I don't think there is any particular rule of thumb. Sometimes it makes sense to have larger files, sometimes smaller files.
Data Engineering Zoomcamp;2022;Are we planning to touch on MLOps topics here?;"Sejal_x000D_
We were initially planning to, but we decided to move it out. We think it will be better to have a separate course on this. Alexey will be sharing more details on this. We do not have anything specifically planned on this, but maybe Alexey has something to add._x000D_
Alexey_x000D_
Well, after this course finishes, we'll have another one that is called (as you might guess) MLOps Zoomcamp. Maybe some of you were wondering, “Why Zoomcamp?” We’re actually using Zoom right now to show the four of us, while the rest of you watch this on YouTube. But it's just a funny name. You can think of this as Zooming into some of the topics. That also works._x000D_
Ankush_x000D_
I think it started from “boot camp” to “zoom camp” meaning it’s going online._x000D_
Alexey_x000D_
With Zoom there is this limit of 100 people per call on the cheap plans. That's why for Machine Learning Zoomcamp, we decided to go with YouTube and kind of stayed in YouTube. Now we have more than 600 people. We wouldn't be able to fit into one Zoom call. But the name, I think, kind of got stuck. So it's funny._x000D_
Ankush_x000D_
If you are interested in MLOps, take part in the DataTalks Slack group. In a couple of months, we will release some information about that – with more details and maybe a structure."
Data Engineering Zoomcamp;2022;Is it possible to have the ETL process but for data lakes? Or ETL has to be to a data warehouse and ETL to a data lake?;"Ankush_x000D_
This is part of your next week’s (week 3) course. If you look into the first video, you will find the answer there."
Data Engineering Zoomcamp;2022;Students will post their projectâ€™s code to public GitHub repos. Will the projects change for the next Zoomcamp run?;"Alexey_x000D_
The projects will be individual. Each one of you will need to come up with your own project and do this end to end. That's why it's not really a problem. For the next round, the concept will be the same – you will need to find a project on your own and solve it. We will help you find datasets for that, but it will be something that you will have to do end to end. That's why it's not a problem if your projects stay in GitHub. Maybe your question is also about homework –whether the homework will stay the same or not. This is something up to us to figure out. Right now, if every one of you posts the solution to the homework in GitHub, then theoretically, it's possible to find the answers and then use them. But then I would really question why you would do that? Why would you find answers online and put this in the form? What does it actually give you? Are you learning or you're just doing this to get this course certificate? I hope you're doing this for learning, not for the scores._x000D_
Ankush_x000D_
Do we have a plan to do the next Zoomcamp?_x000D_
Alexey_x000D_
Let's see how overwhelmed we are by the end of this one. [chuckles] If we have any energy left. _x000D_
Ankush_x000D_
The next one will at least be easier since we don't have to create videos again. [chuckles]_x000D_
Alexey_x000D_
That's the plan with Machine Learning Zoomcamp. I want to restart it in September with the same videos. I just need to figure out what exactly we're going to do with the homework – whether it should be new ones. There was a question in Slack about whether it's a problem that people are starting to share the answers in GitHub to this homework because somebody can reuse it. Just don't reuse it. Just pinky swear that you will not do this. And if you do this, then I don't know, you will get on Santa Claus’ naughty list and you will not receive a gift. [chuckles] That's the best that we can suggest here. Doing other things will complicate the process._x000D_
Ankush_x000D_
There was a message on the live chat that, “The homework points are only for the leaderboard. There is no real benefit.” And I think that's true. Plus, if you have seen, we use hashes, so it's next to impossible to know whose email id it is. So there is actually no fame in it._x000D_
Alexey_x000D_
Actually, what I wanted to do is create a page in GitHub for the ML Zoomcamp with the top 100 people there. So there will be some amount of fame at the end for people who agree to be there. We'll have to ask for explicit permission like, “Do you allow me to put your name there?” But again, I would question your motivation if you really want to get on that leaderboard without completing the homework. I'm not talking about anyone specific, but why do you even ask that question? I remember for Zoomcamp, the question was, “What if I cheat? Will I get caught?” [chuckles] Cheat. It's not my problem, right? It's your problem if you do."
Machine Learning Zoomcamp;2021;Do you use unsupervised learning in daily work? Is it much more difficult than traditional supervised machine learning?;"I don't remember the last time I needed to use unsupervised learning in my daily work. I did need to use it for my Master's thesis. I think I used it for some Kaggle competitions as well. At work, I did something similar to clustering, which is called “locality-sensitive hashing,” which allows you to group similar items together. You can think of this as clustering. At OLX, we actually have a project that needs clustering, I’m just not the one who does this. But these projects don’t tend to happen very often. _x000D_
That's actually one of the reasons why I decided not to include unsupervised learning in the course, because I don't think it's as widely used as supervised learning. It is more difficult than traditional supervised machine learning in the sense that it's not easy to actually evaluate the performance of your clustering results. Let's say you group your clients into five groups, but how do you know if these groups are good or not? This is difficult to know. That's probably the main challenge of unsupervised learning – how do we evaluate the quality?"
Machine Learning Zoomcamp;2021;How will the code review routine for midterm projects be organized? Will all the users be involved in this process?;Yes, everyone who submitted their homework for their midterm project will have to review others. I believe I answered that elsewhere, so if something is not clear, we can talk about it more in Slack.
Machine Learning Zoomcamp;2021;Should I include code in the homework solution?;"Alexey_x000D_
One thing that maybe I wasn't clear enough on – please always include the code to your solution. Maybe some people didn't watch the first, where I said that if you don't submit the code for your homework, you will get zero points. I saw that in some of the submissions, some people put a dot there or empty or put some link to some Towards Data Science article, for example. Please don't do this. Please put your own code. If you don't, then you'll get zero points for the entire week. I'll make it explicit in the form for week three as well. So please don't forget to put your code there."
Data Engineering Zoomcamp;2022;Can we see a leaderboard? Just to understand if I'm right or not in my answers.;"Alexey_x000D_
If this is about week 2, then yes – after we upload the video with a solution. I will also update the leaderboard. It will probably take some time, depending on how clean the data is – if I need to do a lot of data cleaning, it will take a lot more time. So please don't put things that don't look like emails to the email field. When you fill it in, please be careful, because it takes time to clean this later."
Data Engineering Zoomcamp;2022;Will there be summaries of the videos, like course notes, Jupyter notebooks, etc.?;"Alexey_x000D_
I don't think we will actually have time to summarize everything. Of course, I got inspired by Sejal – she prepares some notes and she's recording a video based on those notes. So we will share these notes. But you can also take notes and share your notes with your fellow students. This way you will learn in public – you will sort of regurgitate this content, you will produce something new – it will be useful for you and it will be useful for others. We do not promise summaries, but we encourage you to take notes and share them with others. Jupyter books? I think this is something that we will publish if in that particular week we have a Jupyter notebook or some code snippets. Of course, we will share that._x000D_
Ankush_x000D_
We will definitely share code snippets and all the code that we use in the particular week. That will definitely be shared on GitHub"
Machine Learning Zoomcamp;2021;When do you use functional versus sequential?;Actually, most of the time, I just go with functional because it's a habit. The reason for that is that I found many tutorials. For example, in transfer learning they use. They use functional style. More complex models also use functional style. That's why I'm kind of used to functional style more and that's why I thought it would be a good idea to cover functional style. From functional style, you can easily go to sequential, but when you go from sequential, I think it's more difficult to understand functional style.
Machine Learning Zoomcamp;2021;Are there any points to keep in mind when augmenting images (ratio of generated to original images)?;"Dmitry_x000D_
Usually just testing different approaches. I don't know whether a golden standard or something exists._x000D_
Alexey_x000D_
There is no science to it. Or maybe there is a science in the sense that you have your validation dataset and to treat the augmentation is a hyperparameter. You try it with this augmentation, without it, and then see what works better. This is annoying, because in neural nets you already have a lot of parameters and then on top of that, you also have parameters for augmentations. I don't know if I understand the second part – the ratio of generated to original images. Maybe it comes to the way we trained the model in this homework? First, we trained it on original images and then on generated ones. In this case, the ratio was kind of 50/50. It was an arbitrary choice, right? Was there any science to it? [Dmitry says “No”] Okay."
Data Engineering Zoomcamp;2022;How did you load the data to the S3 bucket from the NY data website?;"Alexey_x000D_
I am not sure how to answer that question. Actually, here I have something, but I think this is a spoiler to the homework solution. [chuckles] But using Airflow helps. You can use Airflow to do that. That's what we've been doing this whole week (week 2) is using Airflow to first download the data from the NY taxi website, then parquetize it and upload to Google Cloud Storage. It wasn't S3. _x000D_
Oh! Now I see where this question is coming from. I guess. If we go to the NY taxi data website – we have a transfer service there – one of the videos was about transfer service. There, the transfer service was moving data from S3 to Google Cloud Storage. Now I understand the question. This data is already in S3 – if you look at the URLs, you can see that the URL contains “S3, Amazon AWS”. This is the name of the packet. _x000D_
This data is already in S3. We didn't need to do this. That was probably a long answer to what should have been short, but the data is already in S3, so we didn't need to do any of that ourselves."
Data Engineering Zoomcamp;2022;Do the model building features in BigQuery or AWS or Azure provides efficient capabilities as compared to building machine learning models ab initio on Jupyter?;"Alexey_x000D_
I think the question is asking if the model building features in BigQuery are good. I think they are good. I haven't done this myself. From what I understand, it’s pretty basic. “Ab initio” is something Latin, right?_x000D_
Victoria_x000D_
“Starting from or based on first principles.” It sounds very like Spanish because it's Latin, but I just wanted to check. [chuckles] It's usually used in legal terminology._x000D_
Alexey_x000D_
Maybe it's coming from a lawyer who wants to become a data engineer. Anyway, the model building features are pretty basic, but it's a good start. You can try that and then switch to Jupyter Notebook for more advanced things, let’s say – when you need more flexibility. This is what I think. I haven't really had experience doing this in BigQuery. _x000D_
In AWS, it's a little bit different. I am not aware of a similar method. In AWS, you have Athena, which is similar to BigQuery. I don't know if you can create a model in Athena. Maybe you can. But in AWS, I would use Sagemaker and Sagemaker has Jupyter Notebooks. If it works for you, then it's good, and then when you need more complexity, you can switch to Jupyter Notebooks."
Machine Learning Zoomcamp;2021;If there is an unbalanced dataset, do we need to add parameter class_weight to the logistic regression model? How to add it properly?;Again, the answer is the same – use cross-validation to find this out. Try with different weights and see what makes sense for your particular dataset.
Data Engineering Zoomcamp;2022;What is a data mesh and will it be covered in this course?;"Victoria_x000D_
It's in the buzzwords, right?_x000D_
Alexey_x000D_
Yeah, it is. If you go to the repo, you can see that there. We will not have like, we will not have a practical week about this. It will be more like a five to 10 minute video explaining what it is and when to use it. But that's what we planned for this. No more._x000D_
Ankush_x000D_
Maybe, we can spend a couple of minutes on it today as well. Data mesh is basically a concept. The idea is – Let's go back and say “How are data pipelines and the data engineering teams built in different companies right now?” You have people who are generating the data, there are people who are consuming the data, and then there is this one big team in the middle, called the data engineering team, DataOps team, or whatever you want to call it. Different companies have different names. These teams are responsible for ingesting the data, transforming it, putting it out there and making it in such a way that it's consumable in the right way, in the right fashion, and all that. So what data mesh wants to do is basically decentralize the particular role of this big team and wants to give the power or, let's say, the responsibility to the team who are generating the data and the teams who are consuming the data. Therefore there’s no layer in between. So instead of the team taking the whole role and doing this, it's kind of a better idea to build up services, and then give the responsibility, or give those services as a software, to these teams who are generating the data and consuming them and basically going on a higher level with respect to this. In this case, what happens is the team that’s generating the data is responsible for the quality of the data, for answering different questions about the data, and all those things. That's really useful because when you have multiple teams handling the data, the idea of the data, or the knowledge of the data, is lost. The best person, or the best team, to answer such questions, or the best team to have the knowledge of the data, is the team that's generating it. So if they are also responsible for putting the data into the data lake or data warehouse and then also responsible for the queries, then it attaches this whole thing together. That's kind of a very rough idea of data mesh._x000D_
Alexey_x000D_
It’s a very abstract concept. Actually, if we go to the YouTube channel, there is a longer explanation of data mesh. By “longer” I mean one hour long – an hour and six minutes. [chuckles] So if you're into this kind of stuff… By the way, this DataOps 101 is also quite a nice one. You can check this out as well. And this one, Modern Data Stack for Analytics Engineering, is also good. Basically, you can check out these three – they're good. But if you're interested in data mesh specifically, the first one goes into a lot of detail, the second one – in a bit of detail._x000D_
Ankush_x000D_
There's also a very nice blog post._x000D_
Alexey_x000D_
Zhamak is actually writing a book about this right now. I think it's in early release. She's written five or six chapters. I don't remember. But it's in progress. It's going to be quite a big book right._x000D_
Ankush_x000D_
Now, this blog post itself is like today's date. I read it in a couple of days, it was impossible to read it in one go. It's huge._x000D_
Victoria_x000D_
I think you can also download the first two chapters or something of the book. And they give a quick overview as well._x000D_
Ankush_x000D_
There's also another book by O'Reilly called Data Mesh in Practice by Max Schultze._x000D_
Alexey_x000D_
This one is free, right?_x000D_
Ankush_x000D_
I think it is free. It's free."
Data Engineering Zoomcamp;2022;Is it always good practice to put data in a data lake before putting it into a data warehouse, like we have done in the course exercises?;"Ankush_x000D_
Is it always a good practice? No, obviously not. Because there's never just one way of doing things. But in some cases, this definitely makes sense. For example, let's say you're using Snowflake as a data warehouse. That's a very expensive data warehouse, honestly. In those cases, you might want to keep a data lake layer for machine learning solutions. That will really help you cut down your costs and even maybe remove some data from the data warehouse if it's getting too expensive or something. There are definitely use cases in which I would definitely recommend everybody to have a data lake before a data warehouse, but not in all use cases. If you are a small company or you have a very small dataset, and you want to be quick, then putting in an extra layer of the data lake might just have a negative impact on the overall performance and speed._x000D_
Alexey_x000D_
I am thinking of any data we have in our company that goes directly to a data warehouse, and I don't think we have anything. Everything first goes to the data lake and then some of this data might end up in a data warehouse. But this is, let's say, only 10 to 20%. The rest stays in the lake._x000D_
Ankush_x000D_
Yep, I think that's because your company is very mature in using data. Your company is also very mature in saving this data. Generally, in a lot of companies, the maturity level is not that great. And in those cases, having a data lake might just create a data swamp. [chuckles]"
Machine Learning Zoomcamp;2021;I can build and run the Docker container but then it just switches me to root in /app. I'm not sure what to do next. Any suggestions?;I have no idea what you mean. What does it mean “It just switches me to root in /app”? Maybe ask in Slack and put how your Docker file looks like and what command you used for building the image. I hope you watched the video about Docker that we had a couple of weeks ago. If you haven't, or it was a long time ago, maybe you should rewatch it and maybe it will clarify some things. Again, please share the Docker file, the commands you used for building the Docker image, and let's try to figure out what's wrong.
Data Engineering Zoomcamp;2022;Will I be able to kick-start an entire data pipeline including Airflow by using just Terraform (by enabling Docker provided in Terraform)?;"Alexey_x000D_
I'm not sure about the last part –how can Docker be provided in Terraform?_x000D_
Ankush_x000D_
Can you run stuff from Terraform? I think that has always been just for... You can set up an Airflow cluster with Terraform, but I don't think you can execute a job there._x000D_
Alexey_x000D_
So it's something like, you use Terraform to prepare the environment (to prepare the thing where you will run it) and then you will need to actually run something. There will be at least two separate commands. First, “Terraform apply”, and then “Python run” or something like this. _x000D_
50:02 [1 upvotes]_x000D_
Are we allowed to contact the instructors directly to discuss projects? There are things I am unsure of regarding my project as I'm using some new tools. _x000D_
Alexey_x000D_
I think it's best if you ask these things in the Slack channel because A) others will be also interested in learning the answer, so it scales better instead of everyone writing to us directly – we just answer once and then everyone sees the answer. And then B) Others can answer this. If we are not available right now, somebody who knows the tool that maybe we don't, for example, Pulsar, one of the students might know it and then they will be able to help. So it's best to use the channel for that, and not contact us directly."
Data Engineering Zoomcamp;2022;Would you consider displaying the question submitted to Slido based on arrival time?;"Alexey_x000D_
I think this question was already asked last time. I'm not sure about doing that. It makes sense in some cases, when you have a follow up question to a question, maybe. But I think voting makes more sense, because maybe if a question is more interesting for 3, 4, 5 people, then to me, voting is more natural. I don't know. Maybe?_x000D_
Ankush_x000D_
I think voting is more beneficial for the bigger group. But if some question is not answered, please ping that in Slack, and I'm pretty sure somebody will pick it up."
Machine Learning Zoomcamp;2021;Do we have to evaluate all models to find the final one?;I think I showed this multiple times in the lessons. You use the validation dataset, you compare your different models on the validation dataset, and the one that has the best performance should be the final model. This is a good rule of thumb. It's not always the end of the story. Remember, when we talked about cross-validation. Let's say if you do cross-validation and you see that your model has very good performance, but also has very high standard deviation, it's not a great model. You probably want to have a model that has lower standard deviation, even if the performance is slightly worse. Right now, this might be just too much information for you, so just stick to this rule that whatever is the best performance on the validation dataset wins. Later, when you work with this, you will develop this intuition of how to actually pick the best model.
Machine Learning Zoomcamp;2021;Do you need to upload the Docker image to the hub?;No, you do not. Building locally is fine.
Machine Learning Zoomcamp;2021;How do you handle imbalance in datasets?;"There are multiple ways. The first way is to not. Just train a model as is. You just need to be careful not to use accuracy as your evaluation metric. You use precision, recall, F1 score, AUC, and that's it. The second option is… Actually, there is a very good talk in our YouTube channel about that – Machine Learning Design Patterns, where Sara talks about one of the patterns, which is about handling imbalanced datasets. I think this is the rebalancing data pattern. _x000D_
You can just take a look at this and Sara will explain how exactly you can deal with unbalanced datasets. One thing to note here is that you do this downsampling and upsampling – you do this only for your training dataset. For the validation dataset and for training dataset, you leave it as is, because you want to evaluate your model. Let me just draw it, I think it's better. _x000D_
Let's say you have your dataset, and you split it into the usual 60/20/20. Then you set the training data aside. What you have is this training dataset and this validation dataset. Then you can do all sorts of tricks for training datasets – you can do oversampling and you can do undersampling. And you can check out Sara’s talk for details on how to do this. There are techniques like SMOTE for generating datasets. You do all that, but only on the training dataset, and the validation dataset you leave alone. You leave it untouched. You do not touch the validation dataset at all. You do all sorts of things there and then you evaluate your model on the validation dataset. _x000D_
It's important to use precision, recall, F1 score – you don't use accuracy here. Here, you can also experiment with different things like, “Okay, if I oversample the minority class, or if I undersample the majority class, which way of dealing with this works better?” And then you just apply to the validation dataset and see which one works better. [image 1] This is the idea. Then, of course, after you find the best approach, you can test it on the test dataset, and you also leave the test dataset alone – you don't modify it at all. You do all this oversampling/undersampling on the full train dataset."
Machine Learning Zoomcamp;2021;How can we train on encoded features (DictVectorizer or OneHotEncoder) using â€˜sparse=Trueâ€™? Are there such use cases?;Yes, of course. You train it in exactly the same way as with ‘sparse=False’. You do that and think the code shouldn't change at all. From the example, you can see it was even faster. This is how we train on sparse matrices. There are some advantages and disadvantages. I just didn't want to go into sparse matrices for this course, but they are useful. There was a question, “What if we have 100 categorical variables?” This is the case when you want to use sparse matrices – when you have a lot of categorical variables. If you use dense – let me maybe draw it here. This is dense, and let's say, you have zero here and the rest are zeros. This is column number 0, 1, 2, 3, 4 and this is row number 0 and 1. A sparse representation would be – for row number 0, you have 4 as 1 and that's it. Then for row number 1, you have 3 as 1. So you don't write zeros here. Since four out of five values here are zeros, you just don't write them. This is when you use sparse matrices. [image 3] I just came up with this, but I think it's a good rule of thumb. Let's say 50% of these values are zeros, then you can just go with a sparse matrix. But, actually, it was a bit faster. So the use case is – when you have a lot of categorical variables.
Data Engineering Zoomcamp;2022;We looked at quite simple examples using DBT. What about complex business logic?;"Victoria_x000D_
I'm not sure what to answer. I mean, yes, we can do it, definitely. I am not adding that as part of the project, at least not in the workshop. The reason for that is because we want people to learn and we have to also consider that people are entirely new to this concept. I thought it would be too much overhead to try to cover things that are already quite complex day-to-day. Even though I work daily with DBT, I work with slow-changing dimensions on a daily basis, for example. So that's also why I think it's something that you will look up at the moment you use it. But you can definitely do that with DBT, especially deletions after implementing a load step, this can be done with hooks. I think there's a section I did with advanced knowledge and I link hooks. Like incremental models that I mentioned before. As for changing dimensions, I didn't link it, but I could link it. This is something that you could use with snapshots, which I think I added as a concept. For updates, I guess you could also do it in a pre-hook and a post-hook. But this is also something that you would do with an incremental model. In an incremental model, when it loads that new chunk of data, you can use a merge. Or you can do, depending on the workers I use, you use it could be an insert and an update. So that's something that would work on a unique key. For this project and also for the final project, it's up to you. The more you understand, the more complex you can go. There's no limit. I hope that answers that."
Data Engineering Zoomcamp;2022;Do you know of any examples/code where object-oriented programming is applied to data engineering?;"Ankush_x000D_
If you're using Java or Scala, you might be somehow using object-oriented programming under the hood. Let's say, for Scala, if you're using Spark, you would be creating some user classes – if you're not using Dataframes, if you're using RDD. And if you create that, that's already object-oriented programming in some ways. Obviously, it will not be as extensive as it might be for some libraries, if you are working on some libraries, but you can definitely do that. However, I would still stick with data frames and maybe other stuff, rather than using core object-oriented programming technology for data engineering. What do you say, Alexey? _x000D_
Alexey_x000D_
I think for streaming, like producers, consumers – they are objects, usually. Right?_x000D_
Ankush_x000D_
No, they are just bytes. You can convert them in Python. You can just convert them to a dictionary. And  in Scala, you can also just convert it to a dictionary if you want... [cross-talk]_x000D_
Alexey_x000D_
I think that consumes – let's say you have this Kafka consumer... Usually... I'm trying to... I remember that in Kinesis we kind of used our own thing, so that was a class. And then in this class, we will have a method like “consume,” that we define. We extend from the base class and then we say, “For this consumer, the run method (the consume method) is this one.” And then we just define the logic for processing the message and the logic for pulling the message or committing lives elsewhere._x000D_
Ankush_x000D_
Yeah, but that's just a style of programming. Regarding object-oriented programming, I understand it more like interfaces, inheritance, polymorphism, and all that. That's why I say – Don't overcomplicate it, because it's just data._x000D_
Alexey_x000D_
I think we did over complicate it in that example._x000D_
Ankush_x000D_
[laughs ]See, you can use OOP, but you might just overcomplicate things._x000D_
Alexey_x000D_
But yeah, I think it was inheritance – so we had a base object that can pull the message, that can commit, and then we extend it, and then we define this consume method. It's useful. Maybe it would be easier if we just had a function “consume” that would do the same thing, right? I think in some cases, you can have a better modularity (modular, nicely-documented code) if you use OOP (object-oriented programming). In some cases, it's just a mess and you don't know what's going on._x000D_
Ankush_x000D_
Exactly. For simple stuff, you generally won't need it."
Machine Learning Zoomcamp;2021;How to go about choosing the regression models and the corresponding parameters that we feed in as arguments (such as C, alpha, etc.)? How can we learn more about this?;This is why you need a validation set. You use a validation set exactly for this. You use a validation set to try different ways, like let's say you want to try different C's or you want to try different alphas – you use a validation set for that. Or you want to get rid of some of the features and try to see if the score improves or not – you use a validation set for that. Also, this week, we will talk more about evaluating models (binary classification models in particular) and we will talk about cross validation, K fold cross-validation that are also useful for that. But in general, just use your validation set for that.
Data Engineering Zoomcamp;2022;In the project, if we use Spark for transformation, can/should we embed it into Airflow?;"Alexey_x000D_
This is something I didn't show how to do in week five. Ideally, yes, because it is one of the steps in your workflow. So yeah, you should use this. You definitely can and you should._x000D_
Ankush_x000D_
Yeah, absolutely. And if you decide not to, for some reason, please document it – how do we reproduce this? Or rather, how does the person evaluating it reproduce it?_x000D_
Alexey_x000D_
But there are Spark Operators. Here, you can see how to do this. It's just an operator._x000D_
Ankush_x000D_
I think the SparkSubmitOperator one would be the important one for us. I think this one would be the one you have to submit your Spark job, right? So maybe this one might be helpful._x000D_
Alexey_x000D_
To be honest, I don't really know how to use this because at work I use a wrapper around Airflow. I just write a YAML file saying, “I want to execute this job on this cluster.” Our data engineers made it very simple for us to use."
Machine Learning Zoomcamp;2021;What are the must-have resources/materials for beginners in ML/AI?;"That's a very broad question and really depends on the background you have. Let's say, if you're a data analyst, then you probably learn things differently from a software engineer. Or if you don’t come from an IT background, you probably want to take a different approach. So it really depends on your background. For me, what was helpful is the course on Coursera by Andrew Ng. It’s very old – from around 2012. This was one of the first courses I took. _x000D_
Another thing that helped me was Kaggle. Of course, the current course is based on my book Machine Learning Bookcamp. I hope that at some point, somebody will say that this is a must-have resource for a beginner in machine learning and AI. I'm a bit biased here, of course. I might say that it is, but I don't know if it's necessarily true. I'll leave this up to you to decide whether that's the case or not."
Machine Learning Zoomcamp;2021;Can you explain more about what JSON is, when to use POST, and more on the general use cases for Flask?;"Flask is – let me draw it. Let's say you want to create a web service. [image 5] The web service gets some requests and then the web service responds with something. It could be predictions, it could be something else – it doesn't have to be related to machine learning. To implement such a web service, you can use Flask, you can use FastAPI, you can use any other framework for creating web services. Here, I use Flask. You can use something else – up to you. But Flask just allows you to do that. _x000D_
When to use POST? For example, let's say for “get,” you usually parse your parameters of a query. Let's say if we go to Google and put in “test,” you see that this q=test (https://www.google.com/search?q=test&oq=test&aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1&sourceid=chrome&ie=UTF-8). [image 6] This is a parameter that we pass through a “get” request (this is a “get” request). But sometimes, let's say if you want to score your customer, then you don't want to put here that gender=female, contract=one year, and so on in the string. Then you send JSON in POST. So in POST, you can also add some body into your request. This is when you generally use post. _x000D_
General use case of Flask is to be able to create a web service."
Data Engineering Zoomcamp;2022;I have some experience in data engineering. Are we planning to focus on writing good unit test cases?;"Ankush_x000D_
No, I don't think that's the focus of the course. I think the focus of the course is to learn data engineering basics, and to learn how to develop software in Kafka and Spark. We might cover some test cases, yes. But we will not be an extensive writing good unit test cases course"
Data Engineering Zoomcamp;2022;How relevant is Terraform in a data engineering job? Would getting the Terraform Associate Certificate make sense for a junior DE?;"Sejal_x000D_
I wanted to address this based on some feedback we've received on the course that we have scoped around. Terraform is particularly DevOps-based technology. Because data engineering has a close overlap with DevOps in terms of building the infrastructure stuff, this is why we also introduced Terraform more as an extra, in order to use it. This is how we designed our course. We did not scope it in a way where we are covering the basics of Terraform. That was not the intention. It was more like a part of the week 1 prerequisites, where we are using Terraform in order to build our infrastructure so that we can focus on the more core parts of data engineering. To answer this question, I would say that for a junior DE, maybe it is not as important, in my opinion. Maybe a Terraform Associate Certificate is something that you can ask whether your current job or your company is requiring you to do it – if you're heavily focused on infrastructure work. Otherwise, I would just say to just learn the basics of Terraform if Terraform is really required at your job in order to build the infrastructure stuff."
Data Engineering Zoomcamp;2022;What if our data doesn't require further transformation once it's in a data warehouse? Do we still need to use DBT?;"Alexey_x000D_
Well, I guess if you want to do a dashboard, then you will have some aggregates. I have a hard time trying to imagine a dataset that you can just take in raw form and put it into a dashboard. Maybe if it's already grouped (if it's already aggregated), but then it's probably a very small dataset and that's not suitable for this course. Then I think you should maybe select a reasonably big dataset that does require transformation. But if you're in doubt, just ask in Slack. I really don't know what kind of dataset you can just put on the dashboard without any transformations. _x000D_
Ankush_x000D_
Yeah, some minimum transformations would be required. _x000D_
Alexey_x000D_
Some counts, some group-bys, maybe joins._x000D_
Ankush_x000D_
Exactly. I think it's better to choose a dataset that will allow you to do all that."
Machine Learning Zoomcamp;2021;I couldn't go ahead with the model deployment steps. Are the first steps to saving the model adequate to pass the midterm?;"I will tell you that you're really missing out if you're not doing the deployment step, because this is one of the most important steps in a machine learning project. If you train a model, and you finish with this –you have your notebook with a pickle file – this model is useless. Nobody can use this model or you have to rely on somebody else – you will need to give this model to somebody else and say, “Hey, can you please deploy this model?” And they will reply “Yeah, but maybe next year. This year, I'm busy with my stuff. Sorry.” So I think the ability to be able to deploy a model is one of the most important skills you can have. I do suggest, maybe not for this midterm project, but in general – if you're serious about data science, I think you should learn Docker, you should learn Flask, you should learn some of these tools. They will be very useful for you. So please do this. _x000D_
To actually answer your question – you couldn't finish this, maybe you didn't have enough time. That's understandable because this is complex. I understand that. Especially if you're doing it for the first time and all this Docker stuff is new to you. Will it be sufficient to pass the midterm project? I do not know yet because I want to first look at the distribution of scores. Let's say a lot of people, for example, 90% of people submitted a midterm project and didn't have this step, then I'm not sure if it's okay to say it's not adequate. Basically, I want to decide on the threshold for passing the project after seeing the scores. Probably it will be better if I do it then. _x000D_
That said – I strongly encourage you to do this. You still have a couple of days. There is a good tutorial from Ninat in Slack, where he shows how you can use Google Cloud Shell for doing Docker stuff. You don't even need to install Docker on your computer. You can use your Gmail account, your Google account, to do this stuff. Take some time, go through these tutorials, and you still have two days. I think you'll be able to do this. If you're still training the model, stop doing this and try to do deployment, please. You will be grateful when you learn how to do this."
Data Engineering Zoomcamp;2022;I had issues with the Airflow â€œDocker-compose upâ€ command, but after removing the triggerer, it appears to have worked. Could you explain why that could be?;"Sejal_x000D_
The official Airflow setup, like I mentioned, the documentation is quite overwhelming, because they have decided to provide at the default level all the services that are required for a multi-node setup. Now, the workshop that we have is using a single machine. A multi-node setup which is more compatible with scalable clusters, for example, is not suitable for a single-machine configuration, especially if you are just working on something like an 8GB RAM. _x000D_
This is also why I made the recent announcements and created the no frills version, where we removed unnecessary services such as the Redis queue, which enables a multi-node setup, also the triggerer, workers etc. Only the main things that are required are actually the scheduler, the executor which is not a threaded executor or multi-node executor, but instead a single-node executor, which is a local executor. Along with that, the web server if you want to use a GUI-based platform for Airflow. So this is why your RAM and CPU usage is conserved and this is why it worked for you._x000D_
Alexey_x000D_
When it comes to this smaller version of Airflow, if you have problems with it, like I do, maybe it's worth trying to give it a try and run it on a cloud virtual machine. This is where I don't have any problems with this. I see other questions about Airflow being a memory-intensive resource. The best way I solved it, for me personally, is using a cloud virtual machine. On my Windows computer, with my Chrome open at the same time, it's not ideal. You have to choose either Chrome or Docker Compose with Airflow. Chrome is quite useful so that's why I opted to do it on a virtual machine. So maybe it'll work out for you as well._x000D_
Ankush_x000D_
Looks like it's time to buy a new machine._x000D_
Alexey_x000D_
Yeah, I just did. I think I bought this computer something like a couple of months ago and it's already like this. But I blame Chrome. I don't know, no matter how good your computer is, the moment you open a couple of tabs in Chrome, it just says, “Okay, there is some RAM. Let me take it.” And then there is not much left for the rest of the processes."
Data Engineering Zoomcamp;2022;For the week 3 homework, we need â€œyellow taxiâ€ data for 2019 and 2020, and â€œfor hire vehiclesâ€ only for 2020?;"Alexey_x000D_
For the homework, you need “for hire vehicles data” only for 2020, but for the lectures (to follow along with the material) Ankush uses 2019 and 2020 “yellow taxi” data. So to be able to follow along with the videos. To execute the same queries, you need “yellow taxi” data for these two years, but for the homework it’s only 2020. You wanted to mention “green taxi” data, Victoria?_x000D_
Victoria_x000D_
Yeah. Do you think they'll need 2019 and 2020? As long as they have some green tax data, their project will work._x000D_
Alexey_x000D_
We're now talking about week 4 – about analytics engineering – because we also use some green data there. In the homework solution, I show you how to upload “green taxi” data as well. This is quite a small change, so if you already know how to upload “yellow taxi” data, you will just need to change a few lines of code. Basically, you just find/replace “yellow” to “green” and that's pretty much it, because the URLs have the same pattern. But this is for week 4._x000D_
Victoria_x000D_
To summarize, by week four, you should have the “yellow taxi” data for 2019 and 2020 that you use for the videos to follow Ankush. _x000D_
Alexey_x000D_
For “for hire vehicles” there are two options – one is high volume, the other is low volume. So we need “for hire vehicles” not “high volume”._x000D_
Victoria_x000D_
“For hire vehicles” = 2019. And then you can use the same to get green taxes data for 2019 and 2020 to be able to do week 4."
Machine Learning Zoomcamp;2021;You should try Linux Mint as well.;Maybe I will. I don't know. Actually, I started to like Windows, because editing videos on Linux is terrible. On Windows, it's much easier. And a bunch of other stuff. When it comes to development, I don't think there's anything better than Linux. But when it comes to other simple day-to-day stuff, like editing a video or editing an image, then I don't think Linux is the best environment for that.
Machine Learning Zoomcamp;2021;I don't trust others to give me good scores on the project.;I think people have good intentions. I don't think somebody will intentionally give you a bad score. If somebody wants to intentionally give bad scores, please don't. Because why would you do this? Seriously, please don't. We're here to learn. If somebody's giving their best and you see that, we will have a matrix that you can use for evaluating. Just give the score the person deserves. I don't have a link with me, but Coursera actually did some research on peer reviewing and they found that peer reviews actually work quite well. People don't give bad scores just for the sake of it. Another reason why we have three people reviewing one project is exactly to avoid that. If there is one person who is giving everyone bad scores, then we'll take a median to avoid that. That's why we have three scores. Also, think of this as your opportunity to get feedback. People will write something saying, “Hey, I had problems running this.” It's good when somebody tries to actually run your project. So I don't have any answer here apart from that. Please have a bit more trust in people. I think they have good intentions.
Machine Learning Zoomcamp;2021;I could get through the homework, but I could not go around setting up the environments.;What can I say? If you want to work as a machine learning engineer, you need to be able to set up environments. Practice makes perfect. You just need to train more and then eventually you will be able to do this.
Data Engineering Zoomcamp;2022;Do you need to speak German to work as a DE in German companies?;"Victoria_x000D_
I don’t think you need to. I've been in Berlin for three years. I do speak German, but not for work. It will depend. If you go for a startup, of course, you're going to be safe. Just try to make sure to ask that before and all of that. It can happen to you that you end up in a team where there are a lot of Germans, in my experience, and then you will feel like you need to speak German in order to integrate. _x000D_
That's the thing I would recommend, though. This is outside of the learning part – if you live in Germany, try to learn German so you can actually integrate. At least in my case, I feel like it has helped me a lot to feel more in the country and be able to go to a doctor and or receive calls and stuff like that. I speak at least a bit of German, I don't speak super-fluidly. But it has been quite helpful. But not to work._x000D_
Alexey_x000D_
Maybe it's also helpful to separate Berlin and the rest of Germany here. In Berlin, it's very international. In many companies in Berlin, if you go and start speaking German with people there, they will just look at you and say “Sorry, I don't understand you.” But if you do this in Munich or any other German city, they will speak to you in German. My German is quite bad. I’ve lived like five and a half or six, seven years (I lost count). It’s just been too long. Now I start feeling ashamed that I still don't speak German. [chuckles]_x000D_
Victoria_x000D_
Everything I said before, disclaimer – in Berlin. But it's true. You can even sometimes go to coffee shops and they reply to you in English because the person doesn't speak German, which is quite crazy for the German capital. _x000D_
Alexey_x000D_
Most of the couriers who deliver things also don't speak German. I can hear that from the way they speak that they don't speak German. And that's good for me, because I don't speak it. [chuckles] We just switch to English._x000D_
Victoria_x000D_
Yeah. And it's true that you can get everything in English. There was also one small part of the question “do you need a visa” and all that. It will depend. I have Italian citizenship so I don't need a visa, for example. But then every other country, I would say yes if it's not European._x000D_
Alexey_x000D_
You don't need German for a visa. If you already have a work contract, then you just use this contract and that's enough to get the blue card visa. “Blue card” is a special program. But if you want to get a job seeker visa, then I think you need to show some level of proficiency in German. I don't remember if it’s A2 or B1 one? You don't need to spend a lot of learning to get to that level._x000D_
Victoria_x000D_
For a job seeker as well? For someone like a data engineer? Because I think for IT, the requirements are less strict._x000D_
Alexey  _x000D_
I think so. Maybe not anymore. Five years ago, I think it was a requirement to get at least A1 level._x000D_
Victoria_x000D_
Not anymore. Because I know a lot of people there have job seeker one and they definitely don't speak German._x000D_
Alexey_x000D_
Okay. Then ignore what they said and look it up._x000D_
Victoria_x000D_
Yeah, definitely look it up. [chuckles] It also will depend on your country and if you have some kind of degree or something like that, then it may also change things._x000D_
Alexey_x000D_
Right. For the Blue Card, you need to have a degree. Without a degree, you cannot get into this program. Check the internet."
Machine Learning Zoomcamp;2021;Do articles on our own website work for the final 20 pt article?;Yes. You can submit them anywhere you want. Did I say 20 pt? I don't remember. But yeah, they do work. But it should be a good article. “Good” meaning, it's not you copy/pasting the code and nothing else. You actually need to explain what's going on there. Also, about the article – closer to the time when we will be working on the article, I will have some guidelines for that as well.
Machine Learning Zoomcamp;2021;With different thresholds, do we also just do distribution of target class? For churn data, churn and non-churn are not 50/50. I think non-churn is more than churn.;Yeah, indeed, that's the case. When you move your threshold, like when we looked at the precision recall curve. This is recall, this is precision. When our precision is high (this is a very high threshold) this could be 0.7 or 0.8. The precision is very high and recall is low. The distribution would be – a very tiny amount of customers will be predicted as default and the rest will be okay – no default. [image 4] Of course, again, use your best judgment. If for this model, it's really important to be precise but you don't care about identifying all defaulting or churning users, then you can set a higher threshold. But of course this changes the distribution.
Data Engineering Zoomcamp;2022;Is there a preference of OS (operating system) to use with this course?;"Ankush_x000D_
We have all the operating systems among the four of us, I guess. I'm using Mac. I guess, Alexey, you're using Linux._x000D_
Alexey_x000D_
I have Windows and Linux._x000D_
Ankush_x000D_
As you can see, we all have different operating systems, so it's pretty open to all operating systems. But generally, I think the most supported ones would be Mac, Windows, and Linux. _x000D_
Victoria_x000D_
We have a lot of things in the cloud anyways. A big part wouldn't even be dependent on the OS. That's also nice to know._x000D_
Sejal_x000D_
There will be instructions available for any alternative OS you're using. For example, some part of the GCP setup may not work in a standardized fashion if you're using Windows, but Alexey has prepared some special instructions for that as well. There will be updated materials specific to the kind of OS you'd be using. But if possible restrict to the top three. [chuckles]_x000D_
Alexey_x000D_
Yeah, I decided to actually do this course on Windows and prepare all these materials on Windows, because for the previous course, most of the students had Windows and I had trouble trying to figure out why some things don't work there. That's why for this one, I decided to do it on Windows. This way I already know the answers to some of the questions why something doesn't work, hopefully."
Machine Learning Zoomcamp;2021;Is there a limit for the number of options in categorical values for one hot encoding implementation?;"Alexey_x000D_
I think in the lectures we used five. Actually, in the next lecture – this week – we will cover classification. One of the lessons there is about one hot encoding. There we will see how to actually use all the values, not just top 10, or top 5._x000D_
Dmitry_x000D_
I think the question is regarding what we discussed with you yesterday._x000D_
Alexey_x000D_
Oh, okay. If you have a variable with 20 options, I think one hot encoding is still good. But let's say if you have a variable with 100 or 1000 options – what do you think, Dmitry? Is one hot encoding still a good option? _x000D_
Dmitry_x000D_
Usually, if we're talking about a linear model, then no, it’s not. For sure. I mean, it can work out in the ensembles of the trees, for example, random forest. But if we were talking about linear models, it cannot be good in this situation and you can use, for example, label encoder. Basically, instead of creating n numbers of columns with, let's say, zero and one, you can use one column and specify the numbers there. So if you have, let's say, 1000 different values, that will be 1000 different numbers. This is how the label encoder works. _x000D_
Alexey_x000D_
Well, I think you said that for linear models, it's… maybe I got confused. But I think for tree-based models, this way of encoding (label encoder is good) but for linears, it's not._x000D_
Dmitry_x000D_
I think it also kind of depends on the curves of dimensionality or how big your feature set is in the end."
Machine Learning Zoomcamp;2021;Can you explain how Docker works internally? I was impressed by it. It is like the evolution of traditional virtual machines but not sure how it is possible.;I don't think I'm the best person to do that. I'm a Docker user, not more. I don't know exactly how it works. I think it uses something like cgroups. I'm not sure – don't quote me on that. Perhaps if you just look up “How Docker works.” There are articles about this that can be found on Google. Basically, the short answer is – I don't know how it works internally, but I know how to use it.
Machine Learning Zoomcamp;2021;Can you explain more on different Flask modules, when to use Flask and what more to learn?;I don't know much about different Flask modules. For machine learning, you probably don't need to know them. You just need to play around in Flask. Most of what you need as a machine learning engineer is in that course. There are not so many, but that should be sufficient, in my opinion. Maybe I'm wrong. I'm happy to talk about this and we can do this in Slack.
Machine Learning Zoomcamp;2021;Any suggestions on how to get started with cloud?;Yeah, we actually have some good tutorials on GitHub. If you use AWS, you can just follow them. The materials we have – deploying with AWS. If you don't use AWS, we have some articles from the community in the “Deployment tutorials” section. How to use PythonAnywhere, how to use Heroku – there are some tutorials here from the community that are quite good. Let's say if you want to use PythonAnywhere, from what I see from this instruction, it's relatively simple. And it's free. You can just start with that and use that. Other than just going ahead and using this, I don't have any other suggestions.
Machine Learning Zoomcamp;2021;Is it okay if two guys work on the same dataset, but individually?;If two people want to work on the same dataset, but individually, please feel free. I imagine there is no way to control this. If you go to Kaggle and you both select the same competition, even if it’s just accidentally, it can happen. As long as it's not exactly the same code – as long as you don't copy from each other – that’s fine. Yeah, please don't copy your code.
Data Engineering Zoomcamp;2022;Is it okay to follow the videos without fully understanding the why behind them? Sometimes I don't understand line by line. Anything I can do to supplement?;"Alexey_x000D_
I think it's okay. If you have any particular problems, maybe ask in Slack. But if this is the first exposure for you to all these tools, it might be overwhelming. There is a lot of information and it's understandable that some things aren't clear. When I actually rewatch some of the videos I did, (when editing, for example) I realize “There is no way people will understand what I meant here.” It is fine that some things are unclear. But if you have a particular doubt, for a particular line that you don't understand, and you want to understand it, you can just ask on Slack. Anything you can do to supplement? I would just say to go with projects. Then, when you get stuck, Google, or ask for help and this is how you will learn and fully understand things."
Data Engineering Zoomcamp;2022;In what scenario should I use a database like Postgres instead of something like the Google Cloud Storage data lake?;"Victoria_x000D_
In the course, specifically, I think we cover Postgres mainly because it's easier and because many people couldn't set up a Google Cloud account. Then it's a nice alternative because at the end, you still have a database and you can query the data and you can load the data. _x000D_
You still get the concept and get to practice that. In real life, I've only seen it in smaller companies. Since it's also very easy to set up, it's an easy way to start. Locally, you'd have it probably in an instance or something like that, so there’s a little bit more of a setup. But I would say, at least from what I've seen, it would be under that scenario._x000D_
Alexey_x000D_
I would add that Postgres was originally created and is still intended to be used as a transactional database, not as an analytical database, which means that this is for – let's say you have an ecommerce shop and you fill your basket with all your orders. It is for these kinds of queries, like a block or something like this – so when you have some transactions and then it has transactions, consistency, and all that. You would use different databases for analytical workloads, when you want to analyze a lot of data. _x000D_
Some of these databases are actually based on Postgres, for example, Redshift, or Greenplum. There are quite a few that are based on Postgres. You don't usually use (you can, but you usually don’t use) plain Postgres for these kinds of things. But for a local setup that we have right now, this is just the easiest way and it can work reasonably well up to quite a lot of databases – quite large databases. I don't think we will actually hit the performance bottleneck of Postgres in this course, because you need to have a lot of data to notice it."
Machine Learning Zoomcamp;2021;Could you create two Slido sessions â€“ one for soft skills/resources/career questions and one for course content?;For soft skills, resources, and career questions, I would recommend you actually go to the DataTalks.Club Slack. There is a channel called Career Questions. Go there and ask your questions there and use the Slido link for the course. This way we can separate the two and during these sessions I can talk about the course and not career, while in Slack, we can talk about careers.
Machine Learning Zoomcamp;2021;Do you have any scripts to process Google Forms?;Yes, I do. There is a script. I will not share it with you now. Maybe after the course finishes.
Data Engineering Zoomcamp;2022;When choosing datasets for the project, what are some things we should consider? Either to get the most out of the project or to avoid unnecessary headaches?;"Alexey_x000D_
One thing is that you should not use images unless you really know what you're doing. It probably shouldn't be a super small dataset. It should be more than one megabyte, ideally._x000D_
Victoria_x000D_
Probably more than one file. In the taxis, we have up to four. At least two. I think also, it's important to understand that the course is more focused on the concepts and everything that they're going to use is more focused on building the pipeline and processing that data than the actual presentation. Maybe it's super cool to do an analysis on images, as you were saying before, but in reality, what you need to practice is more on processing that data. That's why it's easier to use something else._x000D_
Alexey_x000D_
Maybe if you come across a dataset and you're not sure if it's a good dataset or not, just ask in Slack and we'll help. I think the list I gave is quite good. It's a good start. Then, of course, if we talk about Kaggle, there are tons of different datasets. Some of them contain images, some of them are tweets, for example. By the way, you can also use tweets. There is some structured information that you can extract from tweets, as well. Maybe this is actually good if, let's say, you want to practice streaming, because you can just get a stream of tweets coming from Twitter and then you can process them on the fly. I think this could also be a good project. Or sometimes you can also parse something. For example, there are websites, like eBay for selling and buying things. You can build a parser for getting the data. This thing could go every day, take the data, do something with this data (put this to CSV, for example) and then put this to a data warehouse or do something like this. But don't spend too much time on scraping. I think there are already scrapers available. Maybe if you just Google “eBay scraper,” you will find some code, so don't spend a lot of time on trying to build the scraper yourself._x000D_
Victoria_x000D_
Also something like Twitter, as you suggested, and Instagram as well – they have APIs, which makes that part super easy. If you want something like that and you can find something that already has an API, then that's probably the easiest option. You can also get to practice that, which is also very good."
Data Engineering Zoomcamp;2022;Just wondering, before the deadline extension, how many project submissions were there?;"Alexey_x000D_
We have 33 submissions. I think the decision to extend the deadline was a good one. Hopefully, by the end of this week, we will see a higher number. I understand that doing a project is much more difficult than just watching the course and following along. These 33 people – you probably put a lot of effort in, so thanks for doing that. I saw one person write that he put 100 hours already into the project. This is outstanding. Sorry for being so cruel with you. 100 hours is a lot."
Machine Learning Zoomcamp;2021;Can the ports on host and the container be different using Flask and Docker? Could you please give a hint on where exactly the port changes would be reflected?;Yes. When you run here [image 3], I always forget whether the first one is a host machine or container. Let me open the Docker file. [image 4] Here we specify the port on the container, in the Docker file. Then you need to map this port to your host machine and this is where you do this. This is where the port changes would be reflected. It's up to you to decide which port you want to use.
Machine Learning Zoomcamp;2021;Is KNN unsupervised learning?;It is not because it still uses the target information. When we fit our KNN, we take the entire dataset that we have and put it inside our model – and it contains labels. So it is supervised, because when we look up the price for a house, we see what houses are around and then we look at their prices. In this sense, this is supervised learning because we look at similar objects to make our decision on what the price should be.
Machine Learning Zoomcamp;2021;Any tips for working with really large datasets?;Yes, use linear models. They’re usually quite good for large datasets – better than, let's say, tree-based models. Another tip is using this SGD classifier (stochastic gradient descent classifier) which is also for linear models. It is faster than the usual logistic regression because it can look at a part of data when training – it doesn't need to have the entire dataset in memory. For large datasets, you can use this or you can just go to Amazon and rent a bigger machine and just train XGBoost there. This will probably be better than trying to train a linear model on your computer. Also, you can do sampling. Let's say if you have a pretty large dataset, you can take a sample of 10,000 rows and it should work reasonably well. Again, it's case-dependent.
Data Engineering Zoomcamp;2022;Is it possible to increase the deadline for the project? I might struggle with just two weeks.;"Alexey_x000D_
We want to keep it two weeks in order to... if we give you more, then you can get too ambitious. Then you might decide, “Okay, I will use Pulsar. I will use Prefect. I will use Azure,” and you might go crazy with it all and it might not work out well. So we don't want you to be too ambitious. We want you to select something simple, and then have an end-to-end workflow – something that you can do in two weeks. That's why we purposely made it only two weeks and not more, because if we will give you more time... so, just come up with something simple._x000D_
Ankush_x000D_
The idea is not to have a full-fledged production-ready system. Keep it simple, but also use all the technologies and see how the grades are being given, so you can then develop maybe a bit more sophisticated solution. But I think it should be reasonable in two weeks._x000D_
Alexey_x000D_
Yeah. And, of course, if there are any problems, or if you're struggling with something, we have Slack. You can ask your questions there and people are very helpful in our Slack. Usually, when I check the channel, I see that most of the questions are already answered. This is really amazing. Thanks for your guys' help there. I'm pretty sure if you get stuck and if you ask a question there, people will help you, and we'll also try to help you, of course. So don't be afraid to ask for help and don't get too ambitious. Just try to do something simple. Keep it simple and you will do fine in two weeks."
Data Engineering Zoomcamp;2022;Will the course prepare us for taking on a role as a data engineer?;"Ankush_x000D_
Absolutely. I think this course is meant to give you all the knowledge required to be on the job. We are going to talk about analytical engineering. We are going to talk about streaming. We're going to talk about Spark. Maybe not all of it will be useful for your job when you start it, but it will eventually be very important as you progress in your career. It will definitely help you land your first job and it will definitely also help you progress in your career if you are currently already an engineer."
Machine Learning Zoomcamp;2021;Is the regularization strategy of adding small values to the main diagonal of the feature matrix related to ridge, lasso, or other regularization techniques?;Yes. It's very related to ridge regression. There is a mathematical proof that shows that it's exactly the same. It's a bit hairy. There are a lot of derivations that you need to go through to show that they are equivalent. But yeah, they're the same. Lasso is a bit different. I don't know if I should go into detail now. It's a different way of regularizing. One of the key features of lasso is that it forces some of the weights for your model to be zero. If a feature is not important, it makes it zero. But for ridge regression, it just makes it very small. In short, they are very related.
Data Engineering Zoomcamp;2022;How can I set execution_date in Airflow to the past? For example, to January 2019.;"Alexey_x000D_
For that, you probably haven't watched the one hour and twenty minute video, but spoiler – this question is covered there. I do not remember the exact time in the video. It’s somewhere at the beginning. Maybe you can skip around until you see it. You can take a quick look and then you don't have to watch the entire video. I know it's quite long and probably daunting when you see that the video is an hour and twenty minutes long. [chuckles] Sorry about that._x000D_
Sejal_x000D_
I would just like to add to that – not specifically to execution_date, but in terms of the announcement on the best practices videos. Alexey and I just discussed that I think rather than creating a separate video for best practices, it would be best if we merge some things together. I prepared more of a text-based version with the time codes – Airflow/Postgres video that Alexey has prepared. _x000D_
These would provide reference links to what concepts Alexey has explained including [unintelligible] and also using Airflow configuration variables, such as execution dates, and also backfilling and so on and so forth. I'll be sharing that by today or tomorrow. Stay tuned for that."
Machine Learning Zoomcamp;2021;In deployment, what do you mean by â€œmonitoring the quality, maintainability, and scalability of the modelâ€?;"When you deploy your model – let's say this model is deployed as a web service, meaning that if we have a phone (this is an old iPhone with a button) So you have some details about the listing of a car, like title, model, make and so on, and you want to predict what the price of a car, right? This phone sends a request to some web service and then the idea is that, eventually, this request ends up in a model. And that model replies. It gets all the information about the car, like model, make, age – all these things – and then it replies back with predictions, “this car costs that much”. This is what we recommend to the user. Let's say $50K. This model – we say that it's deployed to production, because the client can talk to this model (can communicate with this model) using a web service. And they, let's say, use HTTPS to communicate with this model. This is the deployment. [image for reference] _x000D_
Monitoring the quality means – let's say we deployed the model, and then one year after that, a car that one year ago cost $50K now probably costs $40K. Cars change, new cars appear, cars that were expensive some time ago become less expensive, maybe some cars become more expensive – so things change. This is what we need to be able to detect – these changes. This is what we call model monitoring. We need to see if things that we call changes in distributions or “drifts in distributions”. If these things change, we need to be able to detect it, and let's say, retrain a model. _x000D_
Scalability is – let's say, that now you don’t have just one phone, but you have millions of them. One simple web service cannot handle that much load. So what you need to do is deploy not just one instance of a web service, but let's say, 100. 100 together can deal with 1 million requests at the same time, but one single one cannot. This is what I mean by scalability. We can scale our web service, add more instances of our model there to be able to process all this traffic. Maintainability here means – let's say, there is a bug in this model and want to fix this bug. How easy is it for us to fix this bug? What do we need to do to debug the model? And so on. This is maintainability. How easy it is for us to move around the code base of this web service. Usually, if you follow best engineering practices, then it's easier. This is not something we will cover in this course. For that you probably need a course on software engineering practices. _x000D_
Actually, I talked about model monitoring, but there is also simple monitoring, like “How many requests per second is this model getting?” Things like this. For that, we also need to set up monitoring."
Data Engineering Zoomcamp;2022;Any good streaming datasets apart from Twitter?;"Alexey_x000D_
Victoria, do you know any good streaming datasets? _x000D_
Victoria_x000D_
I've only used Twitter. _x000D_
Alexey_x000D_
I think you can get some data from stock exchanges in real time. But I don't know whether it's free or if you have to pay for this. Maybe there are some exchanges that can give you this data for free._x000D_
Victoria_x000D_
Yeah, they are a few that give you the data for free, now that you mention it. I think there’s also one from the US that has sports news. I couldn't remember the name now, but they also have an API. That could also be a good streaming source. But it's news. It might have something structured, but…_x000D_
Alexey_x000D_
Yeah, I think if you just Google some public data streaming datasets, that could look promising. Coinbase, Twitter. Here’s a good list."
Data Engineering Zoomcamp;2022;Are data engineers expected to know Redshift, Snowflake, and BigQuery and be able to integrate multiple data warehouses, or is it sufficient to know one of them?;"Victoria_x000D_
Same as we've already said, you don't need to know everything. No one knows everything. There's a point. At the beginning, everything is a bit difficult. Everyone has a lot of problems with Docker and BigQuery and stuff like that. But you also get to a point where learning everything starts getting a bit easier. For me, for example, I worked several years with Microsoft SQL Server and then I changed jobs, and they used Redshift. I learned it very quickly because I knew the basics. Then we migrated to Snowflake and, again, it was very easy. The basics are always the same – a data warehouse is a data warehouse, SQL is SQL, Python is Python, and ETL processes are ETL processes. So don't focus that much on the tool. _x000D_
As long as you know one well and you understand the concept behind it and how everything works and things like that, you'll be able to work there. If you're able to integrate, again, if you know how to do an ETL process, for example, then you will also be okay to integrate, regardless of the tool that you'll use at the top. I also don't believe, or at least I wouldn't do it – if I were hiring someone and the person doesn't happen to know the exact data warehouse (for example, we use Snowflake) I won't ask very specific things for Snowflake. And if they do know Snowflake and they've been certified or whatever in Snowflake – that's a very big plus, but that's not something where I would reject someone because they don't have it. If you understand the data warehousing concepts, you're good. _x000D_
Alexey_x000D_
Yeah, it's like any other technology. Let's say we use Airflow, but somebody hasn't used Airflow before, and they used Luigi instead, comes to us – it's not like we're going to reject them just because they didn't have experience with Airflow. These concepts, I think, are transferable. The same with cloud. Let's say somebody has experience with GCP, but we use AWS. Again, if the person has experience with cloud, it's enough because I think many things are transferable from one cloud to another. _x000D_
Of course, there are specifics – there are some particular things that are specific to each cloud, but most of the concepts are pretty transferable. Take any technology that we covered in the course, there are many alternatives. If you know one of these alternatives, you are good. Ideally, as long as you can build a data pipeline and you know all these concepts that we covered and you know at least one technology from all of these concepts, that's already quite good."
Machine Learning Zoomcamp;2021;At OLX, do you work only in the data science team, or do you also leverage your SWE experience from time to time?;Yes, from time to time, I do use my software engineering skills. Mostly I now spend my day in meetings. I don't do a lot of hands-on work. A few years ago, I would just code all day long, but now I do a lot of meetings as well. These days, I don't leverage my past software engineering experience as much as I did before. But yeah, there are always cases. Let's say I need to deploy something. So this is very useful, because in my opinion, data science is still an engineering thing – you need to write a lot of code. Having a background in software engineering and in programming is very helpful.
Data Engineering Zoomcamp;2022;What do you use for recording videos?;"Ankush_x000D_
I just use QuickTime on my Mac, and then I use iMovie to edit it. That's why you don't see my face. [chuckles] But I like your tool more, actually. _x000D_
Alexey_x000D_
What I use is called Loom. I can just press a button right now. And now you can see that it starts recording. Now, what I am saying is being recorded. Then I press another button and then it just creates this thing that I can share. That's really cool. This is very convenient. I really love this tool. _x000D_
Ankush_x000D_
How do you edit it?_x000D_
Alexey_x000D_
There's a download button in Loom. I download it and I use Kdenlive. I actually finished editing a video for a different course – for ML Zoomcamp. There was one video that I kept putting off. So now it's done and I used Kdenlive. The KDE in the name comes from Linux, but it works surprisingly well on Windows as well. I use this on Linux and when I moved to Windows, it also works here. It actually works even better than on Linux. I find this surprising. Of course, it's free and open source._x000D_
Ankush_x000D_
Oh, really? That's impressive._x000D_
Alexey_x000D_
Loom is not free. It costs like $5 per month or something like this. I don't only use it for the courses, but for other things as well. There's a free tool._x000D_
Ankush_x000D_
What other things?_x000D_
Alexey_x000D_
Well... for recording documentation, let's say. [chuckles]_x000D_
Ankush_x000D_
[chuckles] Oh, so you're recording documentation and putting it in OLX?_x000D_
Alexey_x000D_
Not for OLX. For DataTalks.Club. For documenting processes. The other tool I use for recording, which is also free, is OBS. There is a virtual camera, so you can start that and, for example, do that. Here I can use this to just start recording, and then it can capture the screen and then save it to a file. Victoria also uses OBS for recording videos._x000D_
Ankush_x000D_
I think the best thing I like about Loom is that you have feedback, right? In the video, you are talking and you have a face there. _x000D_
Alexey_x000D_
Yeah, the face. And it's cool that it's shareable immediately. So you can just record and then, “Okay, here's the homework solution.” And then the next day, after sleeping, I have to download, upload it to YouTube. So I don't need to do that. I can just share this file. Well, of course it's unedited. So then I can also edit this and then upload to YouTube, but it's shareable immediately. This is pretty cool. I don't know what Sejal used. Do you know what she uses for recording? I know that for a couple of videos we recorded in Zoom. We just had a call and recorded it, but I don't know what she used for other videos."
Data Engineering Zoomcamp;2022;Can we use Azure blob storage instead of AWS to transfer to GCP?;"Ankush_x000D_
Yes. Yes, you can. But I think this is a transfer service, right? So yes, you can. But the only question is that the data which we are using is on AWS. If you want to run and test around with some other data source, then feel free to do this. Also leave a comment, maybe, so we even know how it goes with Azure blob storage."
Data Engineering Zoomcamp;2022;In between Spark and Airflow there is a different machine (VM) so the DAG file must be able to do Spark submit for executing the Spark job?;"Alexey_x000D_
Yes, I think so. I don't think I can draw anywhere. Let's say you have one machine with Spark, and you have another machine with your Airflow. The machine with Airflow needs to be able to send the request to Spark, to do Spark submit. They need to live in the same network. If this is the case, you can do a Sparks submit and then save the URL or IP address of that Spark computer, that's enough. One thing that you might need to do though is  that you will probably need to install Spark to Airflow in order to be able to do this. _x000D_
Maybe Sejal will correct me, but I think you will need to do this because you will need to have Java, you will need to have this Spark submit script in order to be able to actually send the jar over the network to the Spark master. So you will need to modify your Airflow container – if you used the Docker images from the course you will need to modify them, you will need to install Java, and you will need to install Spark. But when you do Spark submit, you just say “--master <master url>” and you specify the master of that virtual machine with Spark. This is how I would do it. Actually, instead of running Spark on the virtual machine somewhere, I would use Dataproc. It is relatively simple to run. You just need to click a few buttons and wait for five minutes when it creates a cluster, and then you have a cluster. So it's pretty convenient."
Data Engineering Zoomcamp;2022;Can you quickly summarize how to upload data to Google Cloud Platform without using Airflow? And is there a way to transfer data from GCP Cloud Storage to BigQuery without Airflow?;"Alexey_x000D_
I can try. First, there is a transfer service, which you can use if you have an AWS account. The reason you need an AWS account is because it asks you to enter your secret access keys. Basically, even though the data is publicly available, you still need to do this. There is also an alternative, which I think is actually something that Sejal shared. This is the script that you can use. You just run it. Preferably you run it in a cloud virtual machine, then it will be quite fast. You just uncomment what you need. So let's say if you need to have green data for 2019, you run it. And if you need yellow data, uncomment it. What this script does is, it downloads the file, saves it to CSV, and then it also saves it to parquet. And then I think what it's doing is uploaded in the parquet file. If you need to upload the CSV, then you can probably replace it somehow. Yeah, probably comment in the parquet part. So, this is how you can do it without Airflow. Probably the easiest is transfer service._x000D_
Victoria_x000D_
I also did something similar, a script from Sejal. And then I chose maybe not the best way, but I just drag and drop all of the CSV to the backup. And then from a backup, you just do this like Ankush explained on the first video out of the three videos. Also have it._x000D_
Alexey_x000D_
So you download the data with a script and then you just select all, drag and drop to Google Cloud Storage and that's it. That's smart._x000D_
Victoria_x000D_
Not the best way. But I didn't have a data engineer. So I gotta get that thing up somehow. [laughs] But yeah, I guess the question is very much targeted to what we're doing. But in general, there are so many tools that you could use as well. I believe I mentioned Fivetran and Stitch. But, in general, I guess any other tool like that could be used._x000D_
Alexey_x000D_
But without air flow is what you show in the videos, right Ankush? Insert into something and then you do select to start from an external table? And then this inserts data._x000D_
Ankush_x000D_
There is also a way to directly upload it to BigQuery. But I just chose the external table because that's easy."
Machine Learning Zoomcamp;2021;It seems that the code that I reviewed is copied. What do I do in this case?;If you have an issue like this – if you see that the code is copied. I didn't explicitly say that it's not allowed, because I assumed it's kind of clear that it's not okay to just copy code and say it's your own. But if you see something like this, and you see that only a part of the project is copied. For example, the part that is about training the model, but not productionizing it, then you just give zero points to the part that is copied – the model training, for example. But for the rest, you use the criteria from the table. For the next project, I will have it explicitly saying that any copying will not be tolerated and will result in zero points. So if you see that somebody copied their work, then you just put zero everywhere. Not for this one – for the next project. Also, speaking of the next project, I think it's actually a good idea to start thinking about it right now so that later you already know how projects look and what is expected. The capstone project will be very similar to this one. You can already start thinking about what you can do for the capstone project as well.
Machine Learning Zoomcamp;2021;How many submissions were there this week?;"Alexey_x000D_
246, which is 150 less than last week. Last week, it was 404. I don’t know if this homework was maybe a bit more difficult. But the next one is also fun. I think if you liked the homework we prepared for this week, the next one will also be fun. Right, Dmitry? _x000D_
Dmitry_x000D_
Yeah, for sure."
Data Engineering Zoomcamp;2022;I'm starting week three now, feeling left behind and dumb.;"Alexey_x000D_
Please don't feel dumb. Do not feel left behind or dumb. You can always ask questions in the chat and in the channel. You can also look things up. I don't know what the state of our list of frequently asked questions is. I think it was abundant. Maybe we can put some life there. If you have a question, we still have 10K history in our Slack. So if you have a question, look it up in Slack, and then if you find the answer, please put it into frequently asked questions. This way it will be preserved. Of course, please don't feel dumb because everyone has a different amount of time that they can dedicate to the course. Everyone has different backgrounds. For somebody, maybe it's the first time they’re using Docker. And for others, they've been using Docker for the last five years. Of course, for the second group of people it's easy, but it doesn't mean that it should be easy for you."
Machine Learning Zoomcamp;2021;Will some scoring system be available for the midterm project?;Yes, there will be. As I said, we will provide a matrix that will have multiple dimensions, like parameter tuning, data prep, etc. and for each of these dimensions, you will have some criteria saying, for example, “0 = no parameter tuning whatsoever,” then “1 = model tuned,” and “2 = a person tuned multiple models and selected the best one.” Then for parameter tuning, if you tried multiple models and selected the best parameter, you get a score of 2. Whoever is reviewing will just have three options in the form. It will say “parameter tuning,” and then option one, option two, option three, which will have the relevant points attached. They will just click the correct option and this is how you will get your score.
Machine Learning Zoomcamp;2021;For hyperparameter tuning, should I be tuning using the full train and test dataset?;No, you should not. You should use the validation dataset for hyperparameter tuning. You tune your model using the validation dataset. Then, at the end, you join the training and validation dataset to train your final model. Do not tune parameters on the test dataset at all. You only use the dataset occasionally, very infrequently – only to double check that you don't accidentally overfit. When you have your final model, then you use it preferably only once. So you try to not look at this at all. For hyperparameter tuning, you use the validation dataset
Data Engineering Zoomcamp;2022;What do you think about this 2021 data engineering roadmap? Is this a good guideline to follow?;"Victoria_x000D_
I think that's ne that someone already shared in Slack. I've seen it in the past and I really like it. But I don't know – I'm not a data engineer. But I think it's very complete. If you go to the website, they actually have a link to all of these resources to learn. I wouldn't be able to talk about all of the content in this roadmap. There's some content that I'm not super familiar with. But I think it's very complete._x000D_
Alexey_x000D_
I think this is a good note here: “Beginners shouldn’t feel overwhelmed by the vast number of tools and frameworks listed here. A typical data engineer would master a subset of these tools throughout several years depending on his/her company and career choices.” When you see a roadmap like this, don't feel overwhelmed. I think learning these things will take a couple of years, right? [Victoria agrees] It feels like a university curriculum here in one document. Computer science fundamentals will take you one year to master that. _x000D_
Victoria_x000D_
That's the thing, they have courses. Maybe this is not the one I saw, but they have several courses. This is also why they have these icons. But I remember having seen that they actually had links._x000D_
Alexey_x000D_
There are actually multiple roadmaps like this. Maybe this is a different one from what you saw. For example, I would go a little bit lean, if I can say that – maybe you don't need all the data structures immediately. You can just maybe pick the most important thing here, which would be basic terminal usage, and then a bit of Linux, then Git as well, then Python here, then SQL here. So maybe you pick one thing from each, you go through the entire thing and then you repeat. _x000D_
Victoria_x000D_
So the green check marks are general things, the hearts are personal, and the cloud icons are cloud-based._x000D_
Alexey_x000D_
It’s a nice roadmap, but there are too many things. I can see why it can be overwhelming. For example, Jenkins – I don't know. I think companies still use Jenkins, right?_x000D_
Victoria_x000D_
Yes. I haven't seen it, but I think they do._x000D_
Alexey_x000D_
We use it for legacy, and legacy is such a thing that stays legacy for very long. Somebody needs to support it. If you're into these kinds of things, Jenkins will be useful. We also use GitLab CI/CD for the CI/CD section, which is not here. But I think it's very cool. Terraform is here._x000D_
Victoria_x000D_
For example, CI/CD, I would say it's good to know the concepts. But it's not something that I would focus on in the very first year. It's good to know what it is, but it's something that you would only do once you have a more stable project or something like that. Even in a project, not even in the knowledge. Then you can pick it up._x000D_
Alexey  _x000D_
It looks very nice – the picture. It’s high quality. But to my taste, there's too much stuff._x000D_
Victoria _x000D_
I'm going to look for the one I was mentioning and I'm going to share it if I find it. It had a bunch of different courses and it had information on how difficult they were and things like that. It’s https://awesomedataengineering.com/. They also have a GitHub repo. This is actually the one I based my repo on – the one I talked to you about for analytics engineering. I think it's more linear and it doesn't have that much. But it has a lot of things. What I find nice is that it defines the coverage, the depth, and also whether it's free, or if you have to pay – so you can focus on only the free resources._x000D_
Alexey_x000D_
And then it tells you not only what you need to study, but also what you need to read to pick the topic, which might be more useful._x000D_
Victoria_x000D_
I find this one more useful. The first one is pretty nice, but it's maybe too much, as you said, and also probably more oriented to their courses._x000D_
Alexey_x000D_
I see that they suggest covering workflow management as one of the last topics, which I am not sure if it’s the right place for that. I would have put it a lot earlier. We actually did that. [chuckles] But yeah, it looks like a good map. Oh, you can even click here and then just get all the free resources. _x000D_
Victoria_x000D_
Or only see the books or only see the courses, if you want to do courses. It’s quite nice. They even have something about the best books – they only have three, but there was Designing Data Intensive Applications and then, the Data Warehouse Toolkit, which is the one that I use for as a refresher or a review of data modeling concepts. Seven Databases in Seven Weeks?_x000D_
Alexey_x000D_
Data Warehouse Toolkit is a classic book. It's pretty old. I recognize the Kimball name. This person created a lot of stuff. _x000D_
Victoria_x000D_
It's one of the fathers of the data warehouse along with Umair Imam. I think the first version was in the 80s."
Data Engineering Zoomcamp;2022;It feels very difficult to follow along. How should I approach this course?;"Alexey_x000D_
I'm curious to know what exactly your difficulties are. Is it not knowing what to do next or not knowing where to start? But maybe I’ll take a stab at that. First of all, you go to our GitHub repo, and then in GitHub repo, we have things structured in weeks. You click on week 1, for example, if you want to learn the materials from week 1. Week 2 is what we're preparing right now, so it will look different once we finish it. It will look like week 1 – it will be more concise. But to know about the details, you will need to go there. _x000D_
Here, we have everything you need, starting from the introduction, then all the code about Docker, and then GCP, Terraform and then setting up the environment. Actually, for the environment setup, if you didn't have any problems installing Docker, installing Terraform, or installing Google Cloud, then you can just ignore this thing. But if for some reason, for example, you have trouble running Postgres in Docker, you can check out this video. It's a step-by-step video that shows how you can set up an environment using a virtual machine on Google Cloud. This video is more like a bonus, and you can watch it at any point. _x000D_
You should follow the order that you see on the page. In the playlist, we actually have the Google Cloud intro first. I don't think it actually matters because this introduction to Google Cloud Platform is a very short video. It doesn't matter if you watch it before Docker or after. We will try to follow the same format for the other weeks. We'll have multiple sections in each week. For example, in week two, we have three sections, like data lake and orchestration. First there is one video on data lakes and then there is basics about orchestration and then there will be a bunch of videos about Airflow. _x000D_
To answer your question – go to our GitHub repo and follow the approach we suggested there. If you have any difficulties, feel free to go to Slack and ask there. If you have difficulties with something in particular, going through the Slack channels and looking through what kind of problems people had, it's very likely that somebody else had the same problem that you have, and there is a solution in one of the threads. _x000D_
Since we're talking about that, it's amazing to see how helpful you are to each other. That's having a big heart. Thanks a lot for doing that. I don't know about you, but for me personally, it's very overwhelming to open the Slack channel and see like 50 new messages. I'm very happy to see that you already helped basically resolve all these issues. So a big, big thank you to you all for doing that. That's very helpful for us. I'm not sure how we would be able to answer all these questions. So thanks a lot._x000D_
Sejal_x000D_
Yeah, plus one to what Alexey said. Thank you so much for helping each other out. If you have any further pressing questions, we will definitely answer and we are definitely looking at the threads. It’s just that we won't be constantly available, but we can and we will answer when possible._x000D_
Alexey_x000D_
I also wanted to say that if you need a refresher on GitHub, maybe you can just Google “GitHub tutorial,” or “getting started with GitHub,” and you'll find a lot of amazing resources. I'm not sure we will be able to do better than these available sources._x000D_
Ankush_x000D_
We will be using Git continuously in our videos. Obviously, we would be using the commands. So I hope that might help you in refreshing your GitHub knowledge. But the best way would be to do a small YouTube search on GitHub and I'm pretty sure there will be really good videos out there._x000D_
Alexey_x000D_
If anyone comes across a very nice GitHub tutorial that was very helpful to you please share with the rest of the students."
Machine Learning Zoomcamp;2021;Should we be using cross-validation?;Well, it's up to you, honestly, if you want to use cross-validation or not. I think you should but you can also just use the usual train/validation split. With cross-validation, as a bonus you can see how stable your model is – what the standard deviation is. Usually, it's useful. So I would suggest that you do this, but I will not require you to do this. Cross-validation is useful, but there must be some form of validation because without it, you will not be able to select the best model.
Machine Learning Zoomcamp;2021;Is time not a categorical variable because it has only 11 values?;I don't know. I haven't checked that, to be honest. Where I took it – I took the data preparation step – I didn't see any data preparation step for time. So maybe it is a categorical variable.
Machine Learning Zoomcamp;2021;What is the number of images generated after augmentation? Can we configure it?;"Dmitry_x000D_
Basically, we apply it on all the images, if I understand this question correctly._x000D_
Alexey_x000D_
We get an image when we iterate over our dataset, we load an image, and then we apply a random mutation to this image. Basically, from one image, we create only one new image. But if we go over our dataset 10 times, then the network sees 10 times a slightly different variation of the original image. I guess this is how we can configure it – by specifying how many epochs (how many times you want to go over our dataset). That could be it, I guess. [Dmitry agrees]"
Machine Learning Zoomcamp;2021;What do you recommend in terms of building on projects further (Jupyter -> Python -> Docker) or leave Jupyter notebooks? Is Jupyter ever used in day-to-day work?;I use Jupyter extensively – I use Jupyter a lot at work. All these initial experiments, like all this playing with different models, or doing exploratory data analysis, or cleaning the dataset – I use Jupyter for that a lot. The reason for that is because you have this interactive environment. You type something and you immediately get an answer. But if you do this from a script, I write something there, and then I need to run the script and see what changes. I think the feedback loop is a lot longer when I use scripts (when I don't use Jupyter). That's why I really love Jupyter. I come from a software development background, so for me, the idea of writing code in a web browser was alien until I got used to it. Now I cannot imagine my life (my work, at least) without using Jupyter. I think this is super convenient.
Data Engineering Zoomcamp;2022;I propose designing total cloud-based automatic pipelines using GCP (using GCP tools) for the project. How do I address reproducibility in this case?;"Alexey_x000D_
Well, by documenting your project. Just make sure you write everything out, that you show how to run the thing, by creating instructions, etc. Maybe one example could be, let's say, the DE Zoomcamp – if we take here, one of the modules here. I don't think my code here is good. No. Let's go with Docker, for example. There is some documentation that actually shows what you need to do to run this thing. So something like this, maybe a little bit more descriptive. First, describe what the project is about, then how to run these things – what are the things that are needed? What are the accounts you need to create? What are the commands that you need to run? So somebody who's looking at this doesn't just see a blank readme.md file, but there is some information. _x000D_
Then if they follow along this information in sequence and execute things, it works. This is what I mean by reproducibility. One tool that I like to make it even easier is Make. You can create a bunch of Make files. We do not cover them in this course, but they're pretty useful. Maybe I can show you an example. Instead of typing long commands, you just do “make all” and then it executes this, this, this and this. So it's a cool thing. But at the very minimum, having this documentation that describes what you need to do with code snippets – this is what we mean by reproducibility. Having this requirements.txt file is also important. Ideally, if you pin versions there so when you write, Python_Kafka==version, this is also good. _x000D_
Ankush_x000D_
I would also say that if you are using cloud-based, then you can assume that the person who's reviewing it would also have the same cloud. So Google Cloud Platform would be available with that person, and then you can use Terraform. So if you're creating buckets, then use Terraform. Then maybe in the readme, you can say, “The first step would be to create this bucket. You create this bucket using this Terraform file and that's how you run it.” For that, the person would need to do the Google Cloud authentication. The same way you did Google Cloud authentication, you can also highlight that in the documentation, “The first step would be to do the Google Cloud authentication,” and so on. Maybe you can use the same criteria (the cloud provider is the same) and then you proceed from there._x000D_
Alexey_x000D_
Yeah, please don't forget these steps, like authentication. Because if you assume that they are authenticated when they run, they might not be and there will be an error."
Machine Learning Zoomcamp;2021;When is it preferred to use MAE metric instead of RMSE?;MAE – we will not talk about this in great detail. MAE and RMSE are both metrics for evaluating the quality of regression models. In the course, we use RMSE, and MAE is an alternative one. For RMSE, the formula is here. For MAE, the formula is a little bit different. Instead of looking at squared error, we look at absolute error. Absolute means that we use bars. This is a formula. So they're a little bit different. RMSE penalizes very big errors. Let's say if our prediction is 0.1, but the actual prediction is 100. RMSE will get a higher penalty because it's 99.9 squared. This will be a huge, huge penalty. While for MAE, it will be just 99.9. There are pros and cons – you can just use both. So you don't have to choose one. You can always report both numbers.
Machine Learning Zoomcamp;2021;Do we need to scale numerical features for logistic regression?;"The answer is – it depends on the dataset. What you can do is – we typically have this train/validation split. First, you try with scaling, then you try without scaling, and then you see what happens. I think this is something we talked about last week. Actually, there is a notebook that I prepared for last week. We can go to classification, and the notebook is this notebook scaling. It shows you how you can scale your features. You can use a standard scaler, you can use Min Max Scaler – you can use these two different scalars and it shows you how you can do this. And for this particular dataset, it turned out to be a little bit better. _x000D_
You can just experiment and see if scaling makes sense or not. For linear models, sometimes it's actually a good idea to do this, but you add extra complexity to your pipeline. So instead of just doing this Dictionary Vectorizer and whatnot, now you'll also need to add the next step of scaling. Basically, now you have more steps in your prediction pipeline and it becomes a bit more difficult to maintain. So I would say that you should try to scale if you see that the improvement you get is significant, because maybe it's not worth the extra complexity that you get. But, again, use cross-validation, use your best judgment, and see what makes sense."
Data Engineering Zoomcamp;2022;How big is the project? Should I take some time off from work during that week?;"Ankush_x000D_
I think we will try to keep the project to a minimum. You will definitely need to spend time, but it’s not like there will be a deadline of one week. We would be giving a deadline of two weeks. And if the community wants an extension, there is a possibility of that as well. We do not encourage you to take time off from work just to do the project. But definitely spending time on it would help you to advance your career._x000D_
Sejal_x000D_
To add to that, this is a self paced course, because we are uploading the recordings of the videos that we are taking. It's not a live course. So please take your own time based on your capacity and available resources in terms of time and effort. You don't really have to squeeze all the things in your timeline to make this work._x000D_
Alexey_x000D_
Of course, everyone will have the same deadlines for the project to make it possible to actually do peer reviewing. But if you're behind with videos, you should still do a project and then you can just ask for feedback about this project – you can just post a link in our Slack channel and then say “This is the project. Can somebody please take a look and evaluate this?” And I'm sure somebody will be glad to do this and learn from that."
Machine Learning Zoomcamp;2021;Does an article mean a published one or a LinkedIn post?;It means a published one, but you can publish it on Medium, for example, or your own blog or whatever. LinkedIn posts – I don't think you can put a nice, long article in a LinkedIn post. But I think there are LinkedIn articles. They don't have code – you cannot embed code there. So I wouldn't recommend using LinkedIn articles. Medium is good. If you have your own blog, then use your own blog.
Data Engineering Zoomcamp;2022;Do you have plans for another Zoomcamp in the near future?;"Alexey_x000D_
Is the question about the Data Engineering Zoomcamp or a Zoomcamp? Because for a Zoomcamp, we want to do an MLOps Zoomcamp in the near future (in a month or two, probably). But when it comes to a Data Engineering Zoomcamp, we still need to talk about that. Probably next year. If it happens, then next year. _x000D_
Sejal_x000D_
For the Data Engineering Zoomcamp, there's a possibility that we may just revise some of the recordings of the videos. For example, the Airflow setup, which was a little time-consuming and also a little effortful on the compute resources side. Maybe we might just find an alternative to some setup instructions so that it works smoothly. Currently, the plans are just to refactor the existing Zoomcamp so that it makes it easy for the next iterations. We would otherwise just release a project-based iteration. I think the recordings would still remain the same. There could be a new project cycle._x000D_
Alexey_x000D_
For that, for what you're talking about, we finished the project right now, and then there will be another iteration for projects. So if for some reason, you weren't able to work on the project right now, there will be another iteration immediately after that. Let's say we finish reviewing the week after the next, then immediately after that, we'll start another cohort for projects."
Data Engineering Zoomcamp;2022;Where does Terraform come into the picture? I haven't used it so far since I installed it in week 1.;"Sejal_x000D_
I'm surprised that you weren't able to connect the dots on this one, because the infrastructure that we created for GCP – which is the GCS bucket, as well as the BigQuery dataset – was created by Terraform. If you did run the GCS DAG (the Airflow DAG) to upload the data to GCS, and then view the GCS data in the GCS external table, this is where your infrastructure that was created from the previous week is used. I don't know if you followed through the videos and understood the concept behind these things. _x000D_
But if you've skipped the Terraform and GCP part and went for the local installation and local setup to use Airflow and Docker with Postgres only, this is also fine (if you're running out of time). These are the two journeys we've prepared. One is a cloud-based journey and the other one is a local setup. For the users who are unable to install GCP, or unable to access GCP, you have the local setup. So if you're going for a Postgres-based operation, that is also completely fine._x000D_
Alexey  _x000D_
I would add that Terraform is a useful thing anyway, regardless of whether we use it for week 2 or not. Knowing it is quite helpful because I am sure that as a data engineer you will have to use it at some point._x000D_
Ankush_x000D_
Exactly.  I think that was my point that we added Terraform on the basis that you will learn it eventually. The idea was that we introduce it to you at the level that you can get comfortable with it. And you can then extend your knowledge on top of that, especially depending upon your role in the company._x000D_
Alexey_x000D_
Will we use Terraform in other weeks? I think so, right? I don't know about Spark – I still need to figure out how exactly it works with Google Cloud. There's probably a managed Spark, like in AWS. That could be created through Terraform as well._x000D_
Sejal_x000D_
In BigQuery, tables can be created by Terraform or by Airflow operators. We will disclose the information on that in the next few days._x000D_
Ankush_x000D_
We are already using it in week 2 in the transfer service. So we are doing something manually, but those manual steps are just to make it easy for you. If you do it in a company, or in production systems, you should always use Terraform to set up something like a transfer service."
Machine Learning Zoomcamp;2021;When to use a test dataset?;We didn't use the test dataset for this homework. But, actually, you used that when you trained your model. You did all this validation, you found the best parameters, you found the best set of features that you want to use, or you found the best model (you compared multiple models perhaps). Then what you do is combine your train and validation datasets, train one bigger model, and then you check it on the test dataset to make sure that everything worked out. Check the lectures – I think during every week, starting from the first to the third week, we covered that. So just make sure to check that out.
Machine Learning Zoomcamp;2021;I have applied min-max normalization to the data set, but Iâ€™m not sure how inference can be made for a single input transformation.;In the lectures, we used Dictionary Vectorizer. This is also a transformer, like min-max normalization. If you're talking about min-max normalization as a transformer from SciKit Learn, it's called min max scaler. Basically, there is a method to transform. First there is a method called fit, which you use to learn this x min and x max, and so on. Then there is a step called transform, which you invoke in the same way as you invoke Dictionary Vectorizer. If you're not using this scaler, I suggest you use it. It just makes things simpler. You fit the transformer when you fit your model and then you transform the data when you apply your model.
Machine Learning Zoomcamp;2021;Can you tell us the average number of hours to do homework? Today, I spent a lot of time more than usual so I want to know, please.;I will, eventually. I want to do some analysis to see how much time people spent on each homework. I also want to do a breakdown because I know that people who registered – when you sign up for the course, you say if you're a data analyst, or data engineer, or a student. I also want to see this breakdown per role – how much time it took for, let's say, software engineers. I think this week wasn't too difficult for engineers, but it was probably difficult for students, perhaps. Maybe other weeks were the other way around. I want to do these analytics at some point. Now, I still have to finish editing the videos for week six, so I will do that first. Then I'll need to prepare some guidelines regarding the projects. Only then maybe I'll do these analytics.
Machine Learning Zoomcamp;2021;There are many ways to write codes. Are short codes better in actual jobs as a machine learning beginner? Should I focus on writing concise codes? What is production code?;"It really depends. Actually, I don't think short codes are better than long codes, because long codes are usually more verbose. For example, if you look at what I wrote here [image for reference] – this is terrible code. Don't write like this. Maybe it makes sense to first select the subset, and select a subset of columns, and then drop duplicates – so actually split it into multiple rows. Because here, [image for reference] you can look at this and immediately understand what's going on. While in the previous solution, it was just one line with a lot of stuff happening and it was very difficult to understand, even though it was more concise (the second one is more verbose). _x000D_
I personally think that the latter way of writing code is better, simply because you can just look at this and it's easier for you to understand what's happening. I don't think your colleagues will like you if you only produce very concise code. For your question about production code – it’s just code that is running in production. [chuckles] This is a recursive definition. But if the code that you wrote affects customers, then it's production code. If this code is just running on your laptop and nobody can run it, then it's not production code."
Data Engineering Zoomcamp;2022;Any project suggestions for Spark?;"Ankush_x000D_
I would pick up a big dataset for a project with Spark, just to see how you can scale and how you can work with a 100 nodes in one go. You’ll most likely spend all your credits, but still. You can scale massively with Spark. If you choose any project, I do not have any suggestion directly for a project. But I would suggest you go for at least a minimum of 200-300 GB, maybe even one terabyte of data. Then you will see the power of Spark shining. You can actually do the comparison between this and maybe a normal script, which does it sequentially, you will actually see the power of something that is distributed and can handle these kinds of features._x000D_
Alexey_x000D_
That's a good thing to try to scale. But I'm not sure you can actually have the scale in your project environment that you would have at work. That would be pretty difficult, I think. Maybe you can also start small – by “small” I don't mean the Titanic dataset or Iris dataset. You need something that is preferably larger than one kilobyte – at least a couple of gigs – and then play with that. The taxi dataset we have, I think it's still quite small for Spark. You need something bigger to actually feel it – to see that you need to tune the garbage collector and all these things. But I do have a suggestion. It will not be close to what you will need to deal with at work, but look for ad click datasets. In advertisements, there is usually a lot of data. For example, in Kaggle, there were a couple of click-through rate prediction competitions with pretty large datasets. I don't know if this one is actually large. Yeah, it's not that large. There are larger ones. You can also look for datasets from Criteo – for example, this 1 TB click logs. There are similar ones. Look for ad click datasets and you will find a lot of relatively big datasets. They are also not that difficult to understand. There is some information about a click, about the device from which the click comes, so you can roughly understand what's happening there. Another dataset that probably could be challenging, but it's called common crawl. This is basically a copy of the internet. They crawl through the internet and they save everything to S3. Get started. Even if you just take a couple of parts of this dataset, let's say for January, it will be enough to play with this data. It's difficult because it contains natural text and it contains a lot of pornography, because this is a copy of the internet, so you will need to be very careful with how exactly you process this. Because it's the internet as it is – just a copy. This is also a good dataset to play around with, not necessarily for Spark, but in general, if you want to try processing large amounts of data. Also, there was a good list of suggestions in Slack. Check it out and you'll probably find good datasets. Another maybe complex example – there are the graph datasets SNAP Stanford. You can find good big graphs here that could also be quite interesting to process, especially in Spark. Because in Spark, there is this GraphX thing. I think it only works for Scala, though. I don't know if it works for Python. But if you want to experiment with this library, I haven't found anything better for processing graphs than this library. This also could be a nice project. _x000D_
Sejal_x000D_
I think there is also Spark ML, right?_x000D_
Alexey_x000D_
As a data scientist, I would not recommend using it. [chuckles] But maybe for something quick. I think the reason it exists is just for marketing, to say, “Hey, we have this.” And I don't think people put enough effort there._x000D_
Ankush  _x000D_
It's not powerful enough._x000D_
Alexey_x000D_
Models from Spark ML don't really work well. What we usually do in data science is use Spark for preparing the dataset and then we get a beefy machine to just load the data and use a simple Python script for training the model. So we still use Spark, but we just materialize the data that we prepared with Spark, save it to S3 and then the script would load the data and train a model on the prepared dataset._x000D_
Ankush_x000D_
And hopefully by that time, the data is already smaller in size. You just use the important information. _x000D_
Alexey_x000D_
Yeah, it's already prepared. It's in parquet, for example, then it's fast to download and load._x000D_
Sejal_x000D_
Do you also use Spark for training?_x000D_
Alexey_x000D_
No, not for training. We use it for preparing data and we use a Python script that does not have Spark. Then for applying the model, we usually do use Spark. We just do map partition. There is an example that I still need to finish. This is the last video. I still have to process two videos, one about RDDS, map filter, and reduce. And then the other one is about map partition. Map partition is a quite a useful operation and I use it quite often for applying machine learning models. Give me some time, and I will finish these videos, and you will be able to see them._x000D_
Sejal_x000D_
Nice. I think it will be helpful for me as well. [chuckles]_x000D_
Alexey_x000D_
I can give you a raw unedited version. It's very bad because I get interrupted, or I forget what to say. It needs editing."
Data Engineering Zoomcamp;2022;For the project, how long will be the timeframe to complete it?;"Alexey_x000D_
Right now what we have in mind is two weeks, but it depends on how it goes. This is the first time we’re running this course and we will just have to see how much time it takes for you. We don't want you to spend more than two weeks on that. We want to get something small. So don't get too ambitious, please. Don't process all common crawl data for your project. Ideally two weeks, but let's see."
Machine Learning Zoomcamp;2021;Do you know about any equivalent to Keras for PyTorch?;"Dmitry_x000D_
It’s quite a strange question, I think. I can interpret it in a way they're both frameworks. Keras is a high-level framework, meaning that you can create the network in, let's say, 10 rows of code. You just have the blocks, like model app, model compile, and then you can create something a Lego style? It's very easy. In PyTorch, it's more of a low-level framework, meaning that most of the things, for example, the layer definitions, all the functions, you need to write on your own. Right now, as far as I heard and checked PyTorch, they are going into the direction to have more predefined functions that the people don't need to rewrite themselves and can just use with minor tweaks. I think this can be equivalent, if I understand the question correctly._x000D_
Alexey_x000D_
I think there is a library. I didn't really use PyTorch before. Well, I ran some things that had PyTorch in it, but I never trained the model myself. I didn't write code in PyTorch, but I heard that there is a thing called PyTorch Lightning and there is also a thing called Catalyst. They're both higher level abstractions on top of PyTorch, which are probably similar to Keras, in a way_x000D_
Dmitry_x000D_
If you think about the high-level abstraction, then yeah, for sure – you can use Catalyst. But they're basically two frameworks. The main goal is the same. For sure, you can add something on PyTorch just like a wrapper, basically._x000D_
Alexey_x000D_
Keras historically was also a separate library, but then it was just a move by TensorFlow to get more users."
Machine Learning Zoomcamp;2021;What is the program that you used for a whiteboard during the course? I find it really useful to practice my thoughts for interviews.;"I use Windows and this is a surface, this is like a Windows tablet and it's called Drawboard. If you just put it into your favorite search engine, you will find it. It's also available in Windows Store or Microsoft Store (not sure what it’s called). I'm pretty new to Windows. There is some App Store on windows where you can just put this in. It's actually free. _x000D_
You can do a lot of cool stuff here. For example, I use only two colors, mostly. But you can use more colors. It's a cool tool, and I have this special pen for it. It’s all Microsoft. Let me again share a part of the screen. Maybe I don't want people to know that I have windows. That's why I'm trading on the part of the screen. But I also don't want people to see the taskbar, for example. It's not useful."
Machine Learning Zoomcamp;2021;Can you collaborate with startups so we can get a chance of an internship after the bootcamp?;I don't think I can because it just takes a lot of time. First, I need to record the videos, prepare homework, and do all that. I cannot imagine adding collaboration with startups to that as well. Maybe sometime in the future. I don't know. But if there is somebody from a startup who is listening right now and you want to hire people who graduate from this Zoomcamp, please reach out and let's arrange something.
Machine Learning Zoomcamp;2021;The theory and homework were really challenging to me as a data scientist. I'd like to do stats, ML, math and skip the Kubernetes part. Would that be enough to find a data scientist job?;"Yes. I would say this course is for machine learning engineers, not for data scientists. If you want to work as a machine learning engineer, I wouldn’t suggest skipping the Kubernetes part. I would actually say that this is quite an important part, so you shouldn't skip it as a machine learning engineer. For a data scientist, it's okay to skip it, but I still think it will be very beneficial. It will help you to stand out as a data scientist. _x000D_
For example, at OLX, when we open a new position, we make a job posting and just within a couple of days, we get several hundred applications. It's very difficult to select candidates just because there are so many of them. I think if you want to stand out in this kind of volume – among these hundreds of people, it’s likely that very few of them know how to use Kubernetes. _x000D_
Therefore, I wouldn’t suggest skipping it. I would suggest that you still do this. But, of course, feel free to do this – just do what you like more and enjoy the process. If you don't like doing Kubernetes, that's fine. I think you're gonna find a job where you don't need to worry about this."
Data Engineering Zoomcamp;2022;Do we need to clean the data for homework? There are some values with leading/trailing spaces.;"Victoria_x000D_
No, because the homework just counts at the end, so there's no need. Ideally, the transformation will happen during this week, so the analytics engineering – right now we're focusing on loading the data. To be fair, I didn't really put that much effort in cleaning this part in the DBT because I focused on building a project that could give you all of the concepts, rather than the actual writing a SQL query that you are already familiar with. That was one of the prerequisites. This is something that you could add to the DBT project, for example._x000D_
Alexey_x000D_
Maybe this is also what we will see how to do in Spark, because I think this cleaning is a little bit easier than in SQL. But it also depends on the type of problems we want to clean. For example, leading/trailing spaces are relatively simple to handle in SQL._x000D_
Victoria_x000D_
Yeah. I did like more of a simpler query, and then just focused on the markers and stuff like that. Not to expect this to clean after week 4, but that's where I would add it."
Machine Learning Zoomcamp;2021;Do we have to first inform you about the dataset we're using?;You can start working. You don't have to inform me. Imagine if everyone started informing me, I will take a lot of time to answer and I will not finish answering all your questions by the first of November. So, yeah, you don't have to inform me. Please write this in Slack if you're not sure. If you think this is a good dataset, just go ahead and use it.
Data Engineering Zoomcamp;2022;If it's okay to ask, I uploaded yellow taxi data, supposedly according to Airflow, but some of them didn't upload completely. Do I need to redo this?;"Alexey_x000D_
You don't have to do this through Airflow. If you understand how it works, if you understand how to create a DAG, how to make dependencies – as you noticed, there is some overhead with Airflow and I experienced this today that sometimes it can just break on your computer and not work as expected, which is really annoying. What you can do is just run these things locally from a script and then upload them to GCP using Google CLI or just a web browser, because you will need some of this data the next week. _x000D_
If you want to follow along in week 3 the videos and do those queries, you will need this data. For the homework, which I think is more important – you need to have all these “for hire vehicles” data. There is a shortcut. You can just use a transfer service. It's just one click and then you have this data in your Google Cloud Storage. But you need to have an AWS account for that, which is a little bit more annoying. _x000D_
You don't need to redo this, but you need to somehow get data to be able to do week 3. We can figure something out in Slack, if you have trouble running this."
Machine Learning Zoomcamp;2021;Is there any way to know exactly how many bins are needed to be set in histplot to make a proper visualization of the data (price var on homework 2, in this case)?;"Dmitry_x000D_
I think it really depends on the use case and the distribution of the data that you have. Also, it depends on what you would like to receive in the end – What is the purpose of your plots? Because there are different types of plots and for different purposes. _x000D_
Alexey_x000D_
Is there any value you always start with?_x000D_
Dmitry_x000D_
I think you can start with 10, usually, and then you can go into the direction that’s needed. _x000D_
Alexey_x000D_
I usually start with 50. I don't know why. Just a habit, I guess. _x000D_
Dmitry_x000D_
For me it’s usually 10 or 15."
Machine Learning Zoomcamp;2021;Any help with Docker to work in Windows via WSL2 backend? I always get an error with exit code 1.;It's hard to say. I’m on Windows right now and I remember that I had problems with that. But eventually I sorted these problems out. I don't remember what I did exactly, but there are some tutorials. I saw some links in Slack that show you how to do this. You can always use this Google Cloud Shell or rent an EC2 machine for Docker. That's maybe also an option. Sorry, I wish I could help but it's just not enough information to help you. For me, Windows is not the main system that I use. I usually use Linux. If you remember the tutorial you followed, please share it and there are people in Slack who did this and who managed to make it work. Maybe they will help.
Machine Learning Zoomcamp;2021;Why Flask and not FastAPI?;No reason. If you like FastAPI, go for it.
Machine Learning Zoomcamp;2021;If I cannot finish my project 1 on time, but still want feedback, can I submit my incomplete project?;Yes, please submit your incomplete project. Actually, what I didn't mention is, 15 points is the max amount – but to pass paths you will probably need something like 10 points or 7. I will actually have to take a look at how many points we get to see what kind of cutoff makes sense, in order to pass the project. But you don't have to score all twos for every metric to pass the project. For example, if you have some problems running Docker, then you will not get two points for that – you will get 1 and the world will not stop. You will still learn something. And let's say if you finished at that point (you only did the first part and you didn't deploy) you will get 7 points, for example. You will still learn something and you will still get feedback. And then regarding the passing criteria, I'll just need to take a look at the actual scores at the end, because if nobody gets 15 points then nobody would pass. I hope this doesn't happen, but we’ll see. I want to look at this course first.
Machine Learning Zoomcamp;2021;Why did we first remove the target variable (â€˜priceâ€™) and then applied the log transformation to the price variable?;"Dmitry_x000D_
Actually, we applied the transformation, and then removed it from the data frame. That’s if I understand the question correctly. _x000D_
Alexey_x000D_
I don't think it matters whether you first remove it and then apply or the other way around. _x000D_
Dmitry_x000D_
Yeah, the most important part is that it will not be in the future set, because in the end, there will be corrupted results._x000D_
Alexey_x000D_
I think somebody wrote in Slack that they forgot to remove the variable and they spent time figuring out why the model is so good. That's probably the reason why it's so good. [chuckles] Right? That's why I try to remove it from the data frame, so that I don’t accidentally use it."
Machine Learning Zoomcamp;2021;I use Google Colab to submit homework 1. Can I use it for the rest of this program or is it recommended to set up a local environment in Linux?;"You can use Google Colab. Yes, definitely. I don't see a problem why you cannot – with the caveat that for deployment sessions, it will not be possible to use Colab, most likely. I don't know how to use this for that. So you will need to have some sort of local environment or something in the cloud where you have access to the command line and you can execute things like Docker, and you can access Python Interpreter there. Windows should work – Linux or Mac OS is probably the best option. _x000D_
But if you use Windows 10 – I’m actually on Windows right now and I can show you this. [image for reference] Here I have the Windows Subsystem for Linux. I'm basically running Linux right here, so I can have all the usual Linux commands, like Git and whatnot. I recommend you to try to do that. Otherwise, you can just use plain Windows as well."
Machine Learning Zoomcamp;2021;Why don't we do feature engineering before modeling?;"Alexey_x000D_
I think this is a question related to the homework – why we didn't create any extra features. I don't know. Dmitry, do you have any ideas why we didn't do this?_x000D_
Dmitry_x000D_
Actually, I just thought about simplicity, so to say. My perspective is that we should go step by step. Regarding feature engineering, some percentage will be covered this week (week 3) and in the next week. Therefore, I think it sets a good tempo, so that we can go step by step. But for sure, it's a good question. For sure, we usually need to do that."
Data Engineering Zoomcamp;2022;Will the certificate be issued individually right after the project is peer reviewed? Or will it be issued on a single day after a specific deadline?;"Alexey_x000D_
It will be issued on a single day because this process is quite manual. Of course, I will not have to type your name for every certificate and then save it. It's semi-automated. But I will still need to come up with a CSV file with all the names, emails, and so on. Even before that, I will need to come up with a design, then I have a script that we can just run and then upload this. I actually have a website where things will be published. So your certificate will be there. There is a script for creating a certificate. It will not be automatic. There is still some manual work. That's why it will be on a single day, after a specific deadline. A specific deadline in this case means that when we have the CSV file, when we have a design for the certificate, and when we hit enter in a command line."
Machine Learning Zoomcamp;2021;Do I do EDA first and then split data or split the data and then EDA? Which data do I need in order to do EDA? Train, full train, full data?;Ideally, as I said, you take your dataset – you leave the test dataset aside and don't touch it at all until you have your final model. So you don't do EDA on your test dataset. You can do this on train, you can do it on full train – up to you. Just don't do this on the training dataset, because you can accidentally see a pattern there and try to use it, and maybe build a feature around this pattern. Meanwhile, this pattern might not be true in the general case. So try not to do this – it's called data snooping. When you look at the data, you might accidentally see something and this something may seem very important, and you just accidentally overfit. Try to avoid looking at the test dataset.
Data Engineering Zoomcamp;2022;I'm thinking of using AWS Redshift, EC2, and S3 for the project. Are there concepts similar to GCP?;"Ankush_x000D_
Yes, they are absolutely the same. AWS Redshift would be quite similar to BigQuery – not quite similar, but still similar. You can think of AWS Redshift as a Postgres cluster – a big Postgres cluster. S3 is definitely a one to one comparison to Google Cloud Storage. And I'm not sure why you want to use EC2. If you want to deploy Spark jobs, maybe consider EMR. But if you're using EC2 maybe for just pipelining or Airflow, then EC2 is also pretty good. It's just a machine. _x000D_
Alexey_x000D_
Because, here we used virtual machines for many things. That would be an equivalent, indeed. You need a remote computer that is running in the same data center as the S3 buckets so they are very fast. So if you download something from S3, or upload something to S3, it's usually way faster if you do it from EC2. Another similar concept is Athena. I think it's more of a thing on top of a data lake, but it gives you this “data warehouse feeling”. _x000D_
Ankush_x000D_
It's SQL on top of your data lake._x000D_
Alexey_x000D_
I would say it's something like... to me it feels like something in between BigQuery and Redshift. It's not as fast. But it's also cheaper than Redshift. _x000D_
Ankush_x000D_
Yeah, I think you're maybe comparing it with Presto. It might be a better fit. It's a Presto. Basically, internally and outside it's a Presto cluster. _x000D_
Alexey_x000D_
Yeah, it is Presto. When I need to Google things – like when you need to Google syntax for Athena – I usually add Presto at the end. Then it works._x000D_
Ankush_x000D_
[chuckles] Yeah, exactly. AWS also has a managed workflow for Apache Airflow. So if you're thinking of using Airflow, then also check that out, so you have to deploy less stuff on your own._x000D_
Alexey_x000D_
And I think in Google Cloud, we also have that. In AWS, you can create EMR, as Ankush mentioned already. _x000D_
Ankush_x000D_
EMR for AWS is something like Google Cloud Dataflow._x000D_
Alexey_x000D_
You can create a Spark cluster with just one click. I think I shared the document that explains how you can configure a cluster. Maybe I'll share again after the Office Hours. So if you want to use that, you can do this."
Data Engineering Zoomcamp;2022;Should we use Spark for files saved in Google Cloud Storage? Where should the output of Spark queries be saved? How could we visualize the Spark output?;"Alexey_x000D_
For example, in Spark, if you Google “Spark GCP BigQuery connector” there is a good article that shows you what you need to do. You need a jar and you need to use this jar when you submit your Spark job. When you do, let's say your files are stored in Google Cloud Storage, you do something with these files, then you have multiple options. The first option is to save it back to Google Cloud Storage and then create it as an external table or something like this, like we saw in week 3. Another option would be saving it directly to BigQuery using this. You just say, “write,” and then you say you want to write to BigQuery. Then it uses this connector to connect to BigQuery and write it. I don't know how permissions actually work here – maybe you will also need to specify the key here somehow. But probably, if you're doing this already from Dataproc, then it should already have all the permissions that you need in order to write to BigQuery, or in order to read from Google Cloud Storage. So if you use Dataproc, theoretically, it should be as easy as that – just add an extra jar and write to BigQuery. Then once the data is in a data warehouse, then you can use the materials from the DBT week or analytics engineering week to actually consume the content from the data warehouse and visualize it."
Data Engineering Zoomcamp;2022;How are restartability and incremental loads usually made in BigQuery? In Redshift, there are stored procedures. What is the approach in BigQuery?;"Victoria_x000D_
I think there are two parts here. I can talk from the part where the data is already in BigQuery because we can do this with DBT. The other part would be loading the data incrementally. In DBT, there's a third materialization that I briefly mentioned but I didn't go into details while doing the project because it's more advanced. It's called incremental. What it does is exactly this – your models a table, it is materialized as a table, you'll find it as a table in your data warehouse. But it allows you to use an incremental block, where you would define what is new data for you. Every time you run the model, the first time you run it will do the “create table”. “Create table as” “select everything,” so selecting entirely everything from the source. But the next time you run it, it will insert all the new data, depending on this incremental clause. Because you have to tell it what the new data is. Let's say you have a loaded timestamp, then you can do “select everything” where the timeframe is greater than the max timestamp that you already have in that table. That way you load incrementally. Also, it's very useful, because most likely, you don't need to reprocess. Let's say, in our case, we're processing trip data. Let's imagine that we are processing this trip data every day, because we work at the company or the taxi in New York. We don't want to reprocess the trip data for the past years. We just want to process the new data that just came in. So that would make sense to also do it incrementally, instead of applying the whole business all the time (to the whole amount of years) of data that you have. I think that covers that part._x000D_
Ankush_x000D_
So what does restartability mean? Does it mean loading all the data? _x000D_
Victoria_x000D_
Yeah, I understand that. At least. Because, if you do it incrementally, there's a point where you want to refresh maybe. That's what I understand from restartability. Then you would load everything from the source._x000D_
Ankush_x000D_
Yeah, exactly. It would be the same way, basically. You can choose the different time ranges, or you can just say, “No filtering. Just give me all the data.” In that case, the table will be created from there. All the fresh data, basically. And to make it an automatic operation, just remember to create a replace table. Since that's an automatic operation, there would not be any issues if there are queries already running. I don't know if that was the question or not. But I think that's what is meant by restartability."
Data Engineering Zoomcamp;2022;Iâ€™m using Docker setup locally, so downloading lots of data for week 3 is not possible. Should I set up everything on the GCP VM or use a transfer service?;"Alexey_x000D_
Well, it’s up to you. If you have an AWS account, you can go with a transfer service. That's probably easiest because you just need to click a few buttons. If you want to set up a GCP virtual machine, you will need to spend some time setting everything up. It's worth doing this because for other weeks – for spark and probably for Kafka as well – it will be useful for you. This is probably worth the time investment. But if you just want to quickly copy the files, the transfer service is probably the easiest way."
Data Engineering Zoomcamp;2022;Is there any plan for some hands-on NoSQL database?;"Ankush_x000D_
When we were planning the syllabus, we actually removed NoSQL databases because we wanted to focus more on some of the most important tools of data engineering. That's covering Spark, data warehouse, Kafka, DBT, and Airflow. But NoSQL is definitely an important part for data engineers to know. If the community really wants it, maybe we can develop a couple of videos on NoSQL and how to use it – maybe give an example of Cassandra or something._x000D_
Alexey_x000D_
We also ran a survey before we started this course and there were eight different topics concerning what the community wants to hear about. Analytics engineering was the top one in terms of preference, and NoSQL was the last one. Whether it was about the internals of NoSQL, I don't remember. We based some of our decisions on that survey as well. _x000D_
It seemed like there was that much interest in NoSQL databases, but a lot more interest in analytics engineering. That affected the way we decided to come up with the syllabus for the course. Maybe you can also let us know what specifically you have in mind, because NoSQL is such a broad term. Are you talking about Redis? Or are you talking about Mongo, or Cassandra, or Dynamo, or what exactly?"
Data Engineering Zoomcamp;2022;If my post on social networks doesn't get any reaction, does this affect my score in the end? I do care about being on the leaderboard.;"Alexey_x000D_
It doesn't affect the score. We count links, not likes. Counting likes would just be too difficult. We would need to go to LinkedIn and then… It's just too difficult. There are many different social networks, so we just count the number of posts to make. But don't overdo this – we cap it at seven. Let's say you're following a course and every day you share your progress – that's why it's capped at seven. You can, of course, share more but we will only count seven._x000D_
Victoria_x000D_
Everything that I see where it says DataTalks, I go and react._x000D_
Alexey_x000D_
I also comment and then it usually gets a widened reach. But don't worry about that."
Data Engineering Zoomcamp;2022;Even if you don't turn the project in within two weeks, can we still get our project reviewed and some feedback?;"Alexey_x000D_
Yes or no. Let me tell you why “no” first. No, because we use peer reviewing, and if the project deadline is already up, then there are no peers that can review your project because we cannot, let's say, “support” the course forever. That's why, if you want to get your project reviewed, you need to do it now. You need to submit it within the deadline and then your peers will review it and give you feedback. This is why “no”. _x000D_
I also said “yes”. So yes, because as you might remember, a few weeks ago, we made an announcement that there will be another project cohort right after the first one. We will finish the project and then we will have another project immediately after that. Because of the war in Ukraine, people cannot work on the project right now. For them, and for everyone else who cannot work on the project right now for some reason, we're giving you a chance to work on this. _x000D_
Let's say you just joined the course and you cannot catch up with everything, then you can submit your project in the second “trial/iteration/cohort” – basically, after one month (after three weeks) we will have another project cohort and then you can submit your project there. But after that, no, there will be no way to submit the project again."
Data Engineering Zoomcamp;2022;Is there a need to use Spark for our project if the size of data is not big? Can't we just do the whole orchestration and have tables in the data warehouse with Airflow?;"Alexey_x000D_
You can. You don't have to use Spark for that, depending on exactly what you're doing. Maybe you can  tell us what exactly your dataset is and then we can maybe give you better recommendations. But you don't have to use Spark. It's optional. Of course, you can use it if you want. You don't have to use it if you don't want to. There are other tools. I think just using Airflow is also fine. Maybe consider using DPT. For your use case, it may be easier than just doing the whole thing in Airflow."
Data Engineering Zoomcamp;2022;I am having a hard time differentiating between a data warehouse and a data lake. Could you please quickly recap it?;"Victoria_x000D_
I would say the video from Ankush was a nice recap. [chuckles]_x000D_
Ankush_x000D_
Actually, that was the recap. All right. Data warehouse vs data lake. Let's just go through the differences quickly. Data warehouse is basically meant for structured data, where you already know your schema of the data. You already know how different tables interact with each other. You are more or less sure what your future data will look like and you're not changing that on a regular basis. In those cases, a data warehouse is the perfect solution. You put your data in a particular schema nicely into your tables in the data warehouse and then you can basically query across a huge amount of data with respect to joins or different SQL queries you have on top of that. This is basically the concept of a data warehouse. So you have a schema, you already know everything your data size is generally in terabytes, not petabytes, let’s say. And you have long-running SQL queries working on top of them. _x000D_
When it comes to a data lake, the basic idea is that you do not know what kind of schema you have. You do not know how this data will be used in the future. What you really want to do at this moment is to save the data as soon as possible in some place where it can be used in the future, or it can be used for different use cases that are maybe not covered right now. In those cases, what you will do is basically take your data, whatever schema it is – doesn't matter. It can be JSON, it can be parquet – it can be whatever. It's good to have some standards, but even if there aren’t any, you can still dump that data to cloud storage or an S3 bucket on an HDFS instance. You basically dump your whole data in whatever format you want. _x000D_
The idea should be that this data should be recoverable. You should have some sort of metadata attached to it so that this information is searchable. You know what the column names are and what they mean, so that if another team wants to interact with this data, they can. With the data lake, the idea would be that, “hey, I don't know what the use case is right now, but I dump it and then tomorrow or in one year, data scientists or data analysts come to it, use this data, see that this information is available to me, maybe transform this data, and put that into a data warehouse, or it put it into a different bucket and then use it.” A use case typically can be a machine learning model that collects a lot of data over the years. Let's say you have data for recommendations or data for people buying different products for the last five years. Then you can build up a recommendation system on top of that. But if you lose this data, then basically, you cannot generate these kinds of models. That's where a data lake really plays a vital role. _x000D_
Alexey_x000D_
That's a pretty comprehensive recap. _x000D_
Ankush_x000D_
I think it was not a good one. [chuckles]_x000D_
Victoria_x000D_
At the end, also what does the trick for me (a silly trick) is to think about the word. How do you store things in a warehouse and how do you store things in a lake? You don't. You just have everything available there. So the image, at least for me, helps me understand the concepts as well. In a normal warehouse, you would have shelves, things with structure and this is also how the data looks in a data warehouse, kind of._x000D_
Alexey_x000D_
The way I understand them, maybe this is not the best way of thinking about them, but a data lake is like your file storage. You write parquet files, it could be S3 or Google Cloud Storage, or HDFS – for us it’s HDFS – and then you have a bunch of buckets. These buckets have some structure but this is just a bunch of files. These files are documented. They follow the same schema more or less. But this is just a bunch of files. If you want to access these files, something needs to go there, read these files – it could be a Spark thing, it could be an Airflow thing, it could be a simple Python script – but it's just files. _x000D_
When it comes to data warehouses, it's not just files. It is a thing. It's properly structured. There are tables, you know the schema, there are some use cases, etc. It's also faster. Because for a data lake, usually you need to download these files, do something with them, put them somewhere. But a data warehouse is optimized for the specific analytical queries you have. Let's say you want to create a dashboard and then in the data warehouse, you model this data in such a way that it's relatively fast to do this. You don't need to go there, download the files, run aggregations – it's a lot easier. This is how I understand the difference. A very quick recap would be – files dumped on S3, that is a data lake. Nice schema database, then it's a data warehouse. Or is this too much of a simplification?_x000D_
Victoria_x000D_
I think it explains the big difference between a data warehouse and a data lake, which is the bigger concept. And then from there, you can always go deeper."
Machine Learning Zoomcamp;2021;Is it worth a plus point, if I share my experience on my personal public tech blog instead of social media?;Yeah, I guess. Of course, I would like it to be about the course. Otherwise, I don't see why you should get a point. But if your experience involves the course and you share it on your tech blog, then yeah – of course. You don't have to use Twitter or LinkedIn – you can use anything. I think a blog is perfectly fine.
Machine Learning Zoomcamp;2021;Is there a way to visualize what the images become after passing through the filters?;"Dmitry_x000D_
Regarding the question – does “filters” mean regarding data augmentation?_x000D_
Alexey_x000D_
I think this is talking about convolutional filters. We apply a filter to an image and we get a feature map. _x000D_
Dmitry_x000D_
But this is not the image anymore._x000D_
Alexey_x000D_
Yeah, but I remember the first big architecture, AlexNet – there was an article about that, or maybe even the original paper about AlexNet, where they showed the filters. These filters were different, like stripes or color, color shapes, something like this. I actually don't know what’s there if we, let's say, take filters from exception and try to visualize them. But in principle, these filters are available as weights of that layer. Maybe you can just use matplotlib and try to see what’s there. I have not done this. I remember seeing that paper about AlexNet and I found it really interesting that you have all these stripes there. There is a comment that somebody saw this in the AlexNet paper."
Data Engineering Zoomcamp;2022;Is there a good resource or website to learn about the logistics of implementing a pipeline for someone? A good server to use, cost, etc.;"Sejal_x000D_
A pipeline for someone… “for someone”?_x000D_
Alexey_x000D_
Maybe if you're a data engineer freelancer, somebody comes to you with a pile of money and says, “Hey, implement a pipeline for me.” How would you approach that?_x000D_
Sejal_x000D_
You would start with gathering the requirements based on how automated of a solution you want to make, how much you want to invest, and if it is a really simple and straightforward task. I'm just giving an example here for data ingestion – you can have your own custom pipelines built in Airflow or you could just use something like Fivetran, which has automated ETL solutions, where you just provide the input and output and integrate it with your targets, such as Snowflake. With a minimal or standardized level of ETL, it will do the job by just providing the schema and just playing around with SQL. But for more custom ingestion pipelines, you may need to use Airflow, for example, to add some additional steps to preprocessing. It really depends on the use case. What would you say are Alexey and Ankush?_x000D_
Ankush_x000D_
What I would do is, let’s say you have some pipeline in mind and you already know what kind of steps you have, then Google provides a very nice calculator. This is called Google Cloud Pricing Calculator. In this, you can basically put what all resources you want, you can put what kind of compute engines you want. There should be Dataflow also, GCP. BigQuery, for example, you have cloud storage, you have BigQuery, you have data prog, data flow. Now you know what kind of tools you want to use. Once you know that you can basically do this on-demand or flat rate and then you can have a very good estimation of your price. Obviously, you should always overprice, or overestimate, and then maybe cut the resources based on that. This can also help you to decide – maybe you don't want to deploy your own Kafka cluster, you just want to use Pub/Sub, because Pub/Sub is much cheaper for your use case. In all these cases, you can basically do these kinds of estimations on the fly. It gets tricky, because obviously, you cannot estimate everything, but you can make some signs of sort of comparison. I think similar ones are also offered for AWS,_x000D_
Alexey_x000D_
It’s maybe not as convenient as this one. I think for AWS, at least not to my knowledge, you don't have everything in one place. For example, if you want to calculate pricing for Lambda, there is a separate page just for Lambda. Maybe there is something now that is as convenient as this one but I'm not aware of that._x000D_
Ankush_x000D_
So I found that just Googling here because I'm more on GCP. There is this NetApp AWS calculator._x000D_
Alexey_x000D_
This is like a third-party thing, right?_x000D_
Ankush_x000D_
Yeah, I think it's a third-party thing. I'm not 100% sure what AWS has to offer. Over here you can also kind of estimate your costs by doing the same thing as you did in Google Cloud. But the Google Cloud one is just way better._x000D_
Sejal_x000D_
You may also want to think about… it's always a good option to begin with, if you're building it for someone, to begin with a cloud-based solution, because most of them are managed services. But in cases where, let's say for security reasons, they want to have their own cluster and manage their own stuff, if they already have a DevOps team, in the future you can also consider a Kubernetes cluster and deploy a self-hosted solution there with an open source stack. There are a lot of trade-offs that you need to make in order to satisfy the need._x000D_
Ankush_x000D_
AWS does have a calculator. Calculator.AWS._x000D_
Alexey_x000D_
I wanted to share this link for Scling. This is from of one of my DataTalks.Club podcast guests. The podcast is this DataOps 101 episode. He collected quite a lot of resources that are related to data engineering, like batch processing, stream processing, and so on. Maybe this is something where you can probably find something useful there._x000D_
Ankush_x000D_
The last link I saw there was Transitioning to Scala. See? You cannot live without Scala. Alexey, you need to learn it now._x000D_
Alexey_x000D_
I did have the pleasure of having to use Scala for too long. [chuckles]_x000D_
Ankush_x000D_
[laughs] Too long, okay._x000D_
Alexey_x000D_
Like it was three years or something like this. _x000D_
Ankush_x000D_
I also see there's also Moving a team from Scala to Golang. _x000D_
Alexey_x000D_
I think there was an article that I read about this star ship operator. If you come across this thing <|*|> in code and maybe try to Google this, and you get “No matching documents”. [chuckles] This is a funny article. Then they figured out that this is a starship operator from some functional programming library in Scala Z. I think there is a bunch of them, right. Cats. Yeah. That's a funny article. I think with Scala, you have to set some standards, and then make sure that people do not get too carried away._x000D_
Sejal_x000D_
Yeah, my rule is – learn Python unless you're joining a company like Spotify that requires Scala. [chuckles]_x000D_
Ankush_x000D_
True. And even if you start with Scala, don't start with all the functional libraries and stuff. Try to keep it simple in the beginning."
Data Engineering Zoomcamp;2022;How can I earn internships and full-time offers for a data engineering role as a fresher?;"Sejal_x000D_
Tricky question. Actually, there are multiple ways. I had to really either brush myself off and do multiple projects. I would say that you can start taking up opportunities within your current company or your current team and see what can overlap on the data engineering side. If you're a data analyst or data scientist maybe take some automation tasks and start from there. You could also learn from your team, who are already experts. I think starting with that aspect gives you an understanding of how to align with the standards of your current company, the technologies that your current company uses, the business domain, and the knowledge required with respect to your company. Many different companies have their different ways of doing things. For example, not everyone uses GCP or AWS. My recommendation would be to start from there._x000D_
Ankush_x000D_
I think that's definitely one of the paths to go. As a fresher you can, you can also learn on your own. There are plenty of resources available. We already recommended some books. There’s this course itself. If you complete this course and go through all the videos and do the homework, maybe even do the project, you will have a very strong foundation to develop into a data engineer. Most probably, you would be able to take more tasks as you grow your knowledge with these videos. That can actually help you to land a full-time job from an internship. If you're interviewing right now for an internship position or a data engineering position, I would say that the best way would be to do it via showing your work and showing your progress on LinkedIn, and sharing your updates on LinkedIn or other publicly available channels that will show to the eventual employer that you are interested in this domain, and that you are making gradual progress towards becoming a good data engineer. That would definitely be one of the recommended ways to go forward._x000D_
Alexey_x000D_
That's why we encourage this Learning in Public thing. That's why we give extra points for it. I know that it's not easy to actually tell the world that you're learning. That's why we want to give you a little bit of push and a little bit of encouragement by giving you extra points. I hope you will be successful in finding your internship soon and that you will not need to spend a lot of time on that. Good luck."
Machine Learning Zoomcamp;2021;Any good references on how Conda, Git, Pipenv, Flask, Gunicorn, and Docker work together?;Yes, there are very good references – the course we have. This course is called Machine Learning Zoomcamp. It will cover all these parts except Conda and Git. But I assume for the midterm project, you use Git. As for Conda – I use Conda as a Python interpreter, and then I install Pipenv there to manage dependencies. I don't know any other good references on that, to be honest. Maybe there are. I don't know if I answered your question or not. But I hope the references we have in the course are good enough. If they’re not, please let me know what you think can be improved there or what you think is missing.
Data Engineering Zoomcamp;2022;I'm doing a master's in computer science and aiming to be a machine learning engineer. I want to prepare for the job market. Is this course right for me or an overkill?;"Sejal_x000D_
Recently, I did a podcast episode on this, which Alexey coincidentally just published on Spotify and Anchor just a day before. This is about transitioning from data engineering to machine learning engineering. It covers a lot about how there are some overlapping skills. I would recommend you listen to that. In a nutshell, I would say this particular course would be really foundational for you to build the basics and see how it really overlaps on building the production skills – to productionize ML workflows, for example. There are a lot of technologies in data engineering that you can use on the ML engineering and MLOps side. So I think it is definitely useful and not an overkill._x000D_
Alexey_x000D_
We have another Zoomcamp which is here. This is a course that is specifically for training you as a machine learning engineer. My wife did this nice illustration. In this course, you will learn the basics of machine learning. If you know the basics, you don't need that. But you can take a few other modules there, like deploying machine learning models, serverless, deep learning, Kubernetes, and TensorFlow serving – these kinds of things. Of course, in that course, we don't cover data preparation. For data preparation, this course (DE Zoomcamp) is quite useful."
Machine Learning Zoomcamp;2021;Do you use any data science project structure or template (like Cookiecutter and so on) for storing projects in real tasks?;Yes. At work (OLX) we quite extensively use Cookiecutter. There are some who have some internal structures. For example, there is a thing called AWS patch, which is a way to deploy models that we use often. And for example, we have a Cookiecutter template that kick starts a project for AWS batch. Then we already have this template and we just use it. Yeah, Cookiecutter is a great tool. If you haven't heard of this tool, I suggest checking it out. Sometimes we just know, “Okay, we need to have this folder, this folder, this folder, because in our team, everybody makes projects this way.” So sometimes there are just these informal guidelines as well.
Data Engineering Zoomcamp;2022;What should the ideal pipeline look like? How to connect everything and make it reproducible with minimal effort? Is there an example project anywhere?;"Ankush_x000D_
I don't know if there's an example project._x000D_
Alexey_x000D_
Well, we for sure will have some good example projects after this project, right? [chuckles] Some of you will create great projects and these will be example projects. But right now, I cannot think of any good project that is open and I can recommend. There must be some projects, but I just don't know. “Ideal pipeline” is such a vague thing._x000D_
Ankush_x000D_
Yeah, it really depends on your use case. For me, an ideal pipeline would be one that has the least manual steps and requires the least intervention even for adding new data sources. So if you have, let's say, similar kinds of data, then it can consume that. If you are producing more data, then it can scale accordingly. There is as minimum intervention from your side as possible, as a data engineer. That would be a good pipeline, I would say._x000D_
Alexey_x000D_
In this definition, can we have an ideal pipeline if we stitch together all these open source tools like Airflow, Spark, and all that?_x000D_
Ankush_x000D_
[chuckles] No, I'm pretty sure the product manager will come up with a new use case in which your ideal pipeline would no longer be ideal. [laughs]_x000D_
Alexey_x000D_
I think there are tools like Fivetran, which I never used, that are closer to this more ideal one. But in the case of Fivetran, it works until, as you said, a product manager comes with a use case that no longer fits that, because this is more of a closed-source platform. Therefore, you will need to figure out how to actually make these things work. But I have no experience working with these things, so I don't know. At OLX, we love to stitch together open source products and hope for the best._x000D_
Ankush_x000D_
If you are going for open source, I think the tools we discussed are the most used ones out there._x000D_
Alexey_x000D_
Are they ideal? I wouldn't say so. But they work._x000D_
Ankush_x000D_
I think some of the enterprise ones are just very much better – they are much easier – but then they are less flexible as well. If you have basic use cases, that's perfectly fine, but if you want to do something very specific, you would be stuck with it and you might need to pay more or ask for some improvements, which might take months to come. These kinds of things always really depend upon what kind of majority your company is in and what kind of use cases you have."
Machine Learning Zoomcamp;2021;Shouldn't we automatically scale the features before using the model?;"Alexey_x000D_
I think I just answered that question, more or less. Dmitry, what do you think? Do we always need to scale features or not?_x000D_
Dmitry_x000D_
It really depends on the model that you use, but if we're talking about the linear model, then for sure. You need to scale it before [audio corrupted]. There can be a situation where they’re already more or less scaled, and you don't need to do anything. But overall, yeah._x000D_
Alexey_x000D_
And for this particular dataset, we saw that we have features on different scales. For the model, it could be confusing – when we use an iterative solver to find the best solution, it can be confusing for these kinds of solvers (for iterative solvers)."
Data Engineering Zoomcamp;2022;Just wondering if Spark could also handle the streaming data like Kafka does. I see Spark also has Spark Streaming. Could I also use that to substitute Kafka?;"Ankush_x000D_
You can use it to substitute Kafka Streams, yes. If that's what you mean, yes, you can definitely use Spark Streaming to consume data from Kafka and then do your transformations (joins, windowing, whatever you want) and then put it out to another Kafka topic or to some other file system. Whatever you want. Of course, you need a Kafka cluster. You need the broker of the Kafka to actually publish messages to Kafka, but you don't have to use the Kafka Streaming thing. Because I think as a lot of people have realized, the Python library for us is very limited. If you're a Python person and you really want to use streaming, then Spark Streaming might be the only option._x000D_
Alexey_x000D_
Do you know if Spark can connect to Kinesis in AWS?_x000D_
Ankush_x000D_
Yes, Spark can connect to Kinesis, also._x000D_
Alexey_x000D_
So you have multiple options, right? You different brokers? What do you call these tools? Streaming engine sort of? _x000D_
Ankush_x000D_
Brokers, yeah. _x000D_
Alexey_x000D_
Brokers. So it can be Kafka, Kinesis, Pulsar – are there others?_x000D_
Ankush_x000D_
You can basically connect to Google Cloud Storage. You can connect to BigQuery. You can connect to anything you want. Not anything you want, but whatever connectors are available. But there are lots of connectors that are available in Spark. You can basically do that very easily._x000D_
Alexey_x000D_
Okay. So you do that and then you have your Spark Streaming thing, right?_x000D_
Ankush_x000D_
Yeah, that's the biggest limitation of Kafka Streams as well. Kafka Streams is only Kafka to Kafka. So if you want to do something else, you need to either export your data into Kafka or just use something else, like Spark Streaming._x000D_
Alexey_x000D_
And for Flink, you can do pretty much the same thing. You can connect it to Kafka, to Pulsar, to Kinesis, right? _x000D_
Ankush_x000D_
Exactly."
Machine Learning Zoomcamp;2021;How will we be able to review othersâ€™ projects if they work with Ubuntu and I use Windows? I won't be able to run their code. What should we do in this case?;In this case, if everyone uses Docker, you will not have this problem. If they don't use Docker, then they just don't get the point for using Docker. If you can’t run it, you just write, “Sorry, I cannot run this.” I hope this doesn't happen. That's actually the whole idea behind Docker – it's transferable between different environments. Let's say if they use Gunicorn and they don't use Docker, then you will not be able to test it. In this case, you will use the scoring matrix and evaluate this accordingly. If there is Gunicorn, you tick that box. But I would encourage you to still try to learn from that submission, even if you cannot run it for some reason. I think the only reason that you cannot run something on Windows from Ubuntu is Gunicorn. Other than that, everything should be runnable.
Data Engineering Zoomcamp;2022;Airflow seems to be very resource-intensive for running a couple of scripts. What kind of workflows benefit greatly from it?;"Sejal_x000D_
I think this is related to orchestration. Since you've covered the concepts of orchestration, Alexey, maybe you can take this one. But just putting in my two cents here – Airflow is definitely meant for orchestration of multiple jobs, sometimes even in parallel or asynchronously running jobs. It's not really meant for sequential run or small-scale usage, but definitely for more resource-intensive jobs. _x000D_
Alexey_x000D_
Airflow itself is quite heavy indeed, but the benefits that it gives you – the mechanisms, the UI, viewing the history – this is very useful the moment you have more than three DAGs running in parallel. When you have one, Airflow might be overkill. But if you know that you're going to have more pipelines, more workflows, then it's time to think about some kind of  workflow orchestration tool. _x000D_
It doesn't have to be Airflow, but Airflow is quite widespread. Let's say you have a team and you're making a decision on which workflow orchestration tool to use, Airflow would be quite a good choice because data engineers on the market know it. Data scientists also know it, more or less. Usually, like this is a good tool. But of course, there are disadvantages – it is indeed resource-intensive. Then it needs to run somewhere – you typically don't set it up on your computer."
Data Engineering Zoomcamp;2022;If we have already submitted our project before the deadline, will we be able to begin peer reviewing right away, or do we need to wait until after the deadline?;"Alexey_x000D_
Unfortunately, you will need to wait because this process is quite manual. We will need to create all these things ourselves and can only do this after the deadline."
Data Engineering Zoomcamp;2022;Is homework 3 going to require students to submit a public code URL or not?;"Alexey_x000D_
When we created this form, we forgot to include this field. I found where the form is located. Now I also have edit permissions for this form. I will just go there and add this URL field. When I do this, I will make an announcement and please resubmit your work. _x000D_
But if for some reason, you already submitted this and you do not have a chance to resubmit it, maybe we'll have to tweak the grading script a little bit so it will accept the work of people who already submitted without the URL. But yeah, it will change and to have a URL there."
Data Engineering Zoomcamp;2022;Why were you reluctant to use Spark ML LIB? Bad experience? What are you using for building machine learning for big data?;"My experience with Spark ML LIB is that it was added to Spark as more of an afterthought. It's not the focus of Spark. They thought, “Okay, it would be cool to show that we can also do machine learning.” But the models that you get from Spark ML LIB are not really good. Usually, in Python, you use a library called SciKit Learn for training machine learning models, and they typically are a lot better than Spark models. They're also faster to train. My explanation for that is that ML LIB was added to Spark just to show that it's possible – like a proof of concept – and they really didn't invest much time into that. I don't know if it's actively maintained or not. _x000D_
My experience wasn't good with this. The way we usually use Spark is for preparing the data and then once the dataset is there in a parquet file, then we would use something else – not Spark – to load this data and train a model. Then for applying the model, I already released a video about that. It's the last one about map partition. This is how we typically apply machine learning models with Spark. You can check it out. As for what I’m using for building machine learning for big data – you can just use a big machine that has a lot of memory (a lot of RAM) and then you just load this dataset and use something like Scikit Learn to train a model."
Data Engineering Zoomcamp;2022;Are there data engineer freelancing jobs (remote worldwide) or projects, as compared to web software engineering?;"Alexey_x000D_
I'm not sure I am able to parse the question. But to my understanding is, “Are there more data engineering freelance jobs than web software engineering?” I don't know if any of you are monitoring it, but my gut feeling is that no, there are not so many freelance jobs compared to web software engineering. Simply because there are so many people who need a website, like a WordPress website, compared to a data pipeline – maybe not so many people need that. But still, it's sufficient to find a couple of gigs here and there. But I'm not a freelancer, so I'm just speculating._x000D_
Ankush_x000D_
Neither am I. But what I can tell you is – if you are good, you will definitely find a freelancing job in data engineering. I’m pretty sure about that._x000D_
Alexey_x000D_
And it's not fair to compare it with web software engineering, because the demand for a simple website is so high. I think this is the most popular engineering kind of remote job."
Machine Learning Zoomcamp;2021;How do the following techniques compare for hyperparameter tuning? 1) Tuning features singly 2) Tuning them together using GridSearch?;"Lisa_x000D_
Well, I thought by doing them singly, it was a little more hands-on. A lot of times, I would just try GridSearch, but I think we were trying to demonstrate “Oh, we have skills that we can actually try these.” I have used things like DataRobot and things like that, which are great for, “Okay, let me try 100 models,” but then you usually have your own business sense or subject matter expertise. So you kind of know, “Okay. Well, maybe this influences things more.” Something like either the GridSearch or DataRobot auto-machine learning are I think good places to start. I wouldn't just use that. _x000D_
Also, what's nice about DataRobot is that you can put your own models in. [chuckles] But it's really handy in terms of saving you time. You saw how long the notebook got when I tried, “Okay, let me just tune the learning rate,” or “Let me just tune different things.” GridSearch does it a lot faster. Some of the auto-machine learning will dump out, “Here's your chart. Here's your comparison. Here’s your leaderboard.” It’s so much faster than I could code it. I think they each have their place. It depends on what your use case is, I guess – how fast you need to get something done, what your goal is, that kind of thing._x000D_
Alexey_x000D_
Yeah. With DataRobot, you just give it a dataset, you press a button, it does some magic, and then it gives you the best model, right? This is how it works, doesn’t it?_x000D_
Lisa_x000D_
Pretty much. You can only have one dataset, so everything has to be prepared to go in that dataset. You can tune a bunch of things. What's nice is that you can have the intern run it or the PhD. The intern may not know as much about the subject matter or how to work with it. I think both can use those kinds of tools. Yeah, it is a big button [laughs] to start with._x000D_
Alexey_x000D_
The way I would answer this question is – there are, of course, advantages and disadvantages. Both deserve a try. I would go with GridSearch if I know that my dataset is small, and I can try to fit a lot of models. Let's say we have only a couple thousand observations in our dataset, then training a single model doesn't take more than one second or two. Then you can go crazy and test as many combinations as you want. Because what GridSearch is doing – it's trying every single combination. Let's say you have the C parameter, then you have random forest. _x000D_
You can try with Bootstrap and without. You can try a number of estimators this or that. You can try max depth on that. That it basically creates like 50, 100, 1000 different combinations. If your dataset is small and fitting one model doesn't take a lot of time, you can just go with that and then see. But usually, when you have a relatively large dataset, like 20-30 thousand, then fitting one model takes some time. It's not one second – it's 10 seconds, 20 seconds, 30 seconds. Then trying GridSearch just takes forever. _x000D_
This is when I would actually go and try to tune the features singly, exactly like how I showed it in the course. For example, with XGBoost what I showed was more like a heuristic, which usually works. For XGBoost it doesn't give you the best parameters necessarily. So maybe with GridSearch, if you give it enough compute, GridSearch would find better parameters than when you turn them singly. But you can find an OK solution faster. So GridSearch takes a lot of time, but finds a better solution, a more optimal one. Tuning features singly takes less time, it's more menial, but if you have a large dataset, you can arrive at an OK model faster."
Data Engineering Zoomcamp;2022;Will there be a book based on DE Zoomcamp, like the book on ML Zoomcamp?;"Alexey_x000D_
Not today. [laughs]_x000D_
Ankush_x000D_
[laughs] Maybe after a long break,_x000D_
Alexey_x000D_
The thing with ML Zoomcamp, I already had a book and then I thought, “Okay, maybe I should do a course.” But here, that's different. But actually.._x000D_
Sejal_x000D_
But good idea though._x000D_
Alexey_x000D_
I think maybe, Alvaro, if you're watching this – I think you could be quite good at writing books, because the notes that you have here in your repo are quite comprehensive. You can just take this markdown and go to O'Reilly. Tell them “Hey, look what I have.” [chuckles] Anyway, maybe you don't have to go to O'Reilly, but I think notes from Alvaro are quite good. And other notes as well are quite good. It’s just that Alvaro has been doing this for quite a while. He was also taking a lot of notes for the other course. And he's always first to submit his notes in pull requests. Thanks for doing a great job with notes._x000D_
Ankush _x000D_
Feel free to use them, just share the royalties with us. [chuckles]"
Data Engineering Zoomcamp;2022;What is a driver node in Spark? Is it a master node or some other node that is dedicated for in-memory processing?;"Ankush_x000D_
Master node._x000D_
Alexey_x000D_
Master node? But not necessarily, right? A master node is something that coordinates the execution. But then, let's say if I do Spark submit from my computer, or I open Jupyter on my computer and connect to a master, then the driver node becomes the thing from which I do Spark submit. Right?_x000D_
Ankush_x000D_
I think that's where the Spark context lies. If you are doing it in-memory, then it will be your machine. The Spark context would start and then you would connect to different workers, give your graph to them, give your data to them, and then they will start executing. Right?_x000D_
Alexey_x000D_
If I open the video Anatomy of a Spark Cluster. [image] This thing here is the driver node, a laptop or whatever you use, if you submit your job from Airflow, then whenever you invoke the Spark submit, or you connect to Spark to master – as Ankush said, wherever this Spark context lives. Let's say if it lives in my Jupyter notebook on my laptop, then my laptop is the driver. Then master is the thing that coordinates execution of a job. It watches which executors are available and if one of the executors dies, it reassigns – so it has the coordinating role. You can also watch the progress of the jobs from the master node._x000D_
Ankush_x000D_
You are right. I just also looked it up. Basically, it can run on worker or master node. It's not always running on master node, but it makes more sense to run it on a master node because you'd want your Spark context to be alive throughout the process._x000D_
Alexey_x000D_
So when you do Spark submit, you can say where the process should live, “Do you want to keep it on the master node or you want to keep it somewhere else?” _x000D_
Ankush_x000D_
Yeah. You can specify with config spark.driver.host and that might provide the driver that would run on this particular host machine._x000D_
Alexey_x000D_
By default, I think this is the laptop or the computer from which you connect to Spark master. But then you can override this and you can say that it should live somewhere else._x000D_
Ankush_x000D_
That's what I see from the documentation I’m reading right now."
Data Engineering Zoomcamp;2022;Should the HW we hand in be SQL files? I put in an .md file.;"Alexey_x000D_
In terms of how exactly you should hand in your code – it doesn't matter. Just put it somehow in any format, and then add the link to GitHub. _x000D_
Ankush_x000D_
I actually have a solution here. If you are doing this just for yourself and to learn – maybe want to do it as something for your learning purposes, that's perfectly fine. You can do it in a .md file or whatever file. But if you are thinking of this as a project you would like to highlight to a future employer, or you want to attract future employers by putting this on LinkedIn or open source, then I would actually also focus on putting the right file format and everything. It's easier in GitHub to read those, it's easier when you download them locally, to open it up in intelligence to just have the color coding there. So I think it's kind of nice to do it. But don't stress too much on it. If you put it in an .md That's fine. But maybe in the future, you can put it in a .sql file."
Data Engineering Zoomcamp;2022;What are data models that we should use for warehousing? Star schema, Data Vault 2.0... What's the most â€œrelevantâ€ nowadays?;"Ankush_x000D_
The simple answer is “I don't know.” [chuckles] I think we should save this question for Victoria. I think she would be the best person to answer this. Maybe we can put this in the Slack channel and tag her._x000D_
Alexey_x000D_
But also, for example, for these taxi trip data – do we use any star schemas or anything? Well, let's kind of a star, just with one ray, right? [chuckles]_x000D_
Ankush_x000D_
Kind of like that. I think the most common one is Star. But I don't know, as I have not done enough data modeling to know what Data Vault 2.0 would provide us for a particular dataset. I don't know what's the most relevant thing right now, honestly._x000D_
Alexey_x000D_
But if you look at the criteria, you'll notice that there is no dimension that is called “data modeling”. Then, instead of DBT... you don't have to use DBT, because most of these things you can do with Spark, if you like Spark more. If you don't like Spark, you can go with DBT. Even though they are a little bit different, in the end, they're sometimes interchangeable. For example, you can maybe put the data in a data lake and transform the data for Spark and then put it to a data warehouse in the form that is ready to be queried by your dashboard. Or you can decide to put the raw data in your data warehouse, and then do transformation there with DBT. So it's up to you. You can go with either approach. I don't think there is a right or wrong here. [Ankush agrees]"
Machine Learning Zoomcamp;2021;Will you be creating a Top 100 leaderboard that you talked about in the first livestream? In the last livestream, you said you don't have permissions to publish names.;"Alexey_x000D_
For that one, I will individually contact the 100 people and ask for their permission and probably some links like LinkedIn, GitHub, and whatever you want to include. If you're one of the top 100 and do not want to be there, you can also tell me that and I will put your name there."
Data Engineering Zoomcamp;2022;The homeworks will get scored and the final project will also get a score. If you do not do the homework, you might not be on the leaderboard, but you will still get the certificate. Is that correct?;"Alexey_x000D_
Yeah. You will be on the leaderboard. If you open the leaderboard, you will see that it is long and there are people who only attempted some projects. So they're still here on the board. At the end, there will be another tab here saying who passed the project and there will be a separate leaderboard for them."
Data Engineering Zoomcamp;2022;I'm a data scientist but would like to learn more about data engineering. Is this the right place for me?;"Alexey_x000D_
I think it is. Actually, when we were talking about the course and who this course is for – this is one of the profiles we considered. I think this is very popular now among data scientists to think, “Is there anything else besides data science?” Things like preparing data for our machine learning jobs, or for our machine learning models, or what happens after we built our model. That's why this is definitely one of the profiles that we considered and indeed, this is the right place for you. It's not only for data scientists, it's also for software engineers who know how to program. This is also the right place for you. For analysts, if you're comfortable with programming and the command line and you know Python, this is also the right place for you. In general, as we discussed the prerequisites – if you know how to program, if you know how to use the command line and if you know a bit of SQL, then this is the course for you."
Data Engineering Zoomcamp;2022;Will there be a certificate at the end? Does this require our end project to be validated?;"Alexey_x000D_
Yes, as I mentioned, there will be a certificate. And it does require the end project to be validated. The way we will do this is through peer reviews. You will get to review three projects of your peers and we'll have some formal criteria that you will need to use to evaluate your peers and your score will be based on that. Some of you might wonder, “Okay, is it fair that somebody else judges my work?” We tested this way of evaluating projects in a different course that we were running, the Machine Learning Zoomcamp, and it worked quite well. This is a good approach and for you, this is also a chance to learn from your peers because every time you review a project, you learn something new. Some people already know something, and maybe your peers will use something else that wasn't covered in the course. Of course, in this case, we will ask them to document everything so you do not feel lost and get an idea exactly what the project is about. So you can also see this as a way of learning extra things and this is how we are going to validate the projects."
Machine Learning Zoomcamp;2021;Can we somehow learn how many points (for homework, etc.) we currently have?;Yes, there is a leaderboard. I think I shared the link a couple of weeks ago in Slack. Maybe I actually need to add the link to the course repo so it's just there all the time. But if you just look it up in Slack in the course ML Zoomcamp channel, you will find it.
Data Engineering Zoomcamp;2022;Have you ever had issues dealing with hype? How do you mitigate a hype mindset in the workplace?;"Alexey_x000D_
Maybe I was like that. Every time I heard that there was a new language, a new tool, a new something, I wanted to try it. But then I realized that these new tools are usually pretty raw. They look shiny, but when you start using them, a lot of problems come up. Sometimes it's better to use time-proven tools and be a bit conservative. This is something that happened with experience of trying these tools and then seeing that even though they look shiny, there are still some bugs, especially if it's a new tool. How to mitigate this mindset in the workplace? I don't know. I guess it depends on the use case, but just ask yourself (or the team) “How much does this new tool actually bring you? What are the pros and cons? How much work will you add to what you have when you integrate the new tool and what are the potential benefits?” If the benefits outweigh the headaches you will have, then you can go with this. Usually, when you do this analysis, you see that maybe you don't actually need that new tool and the old one is fine._x000D_
Victoria_x000D_
Have a critical mindset. Do not just go with something just because there's hype. I always go read everything and then I can kind of see, “Is this something that I'm already using? Will this add that much value? How much headaches will this add?” Then, also, it's another tool to maintain. So I consider all of this. Have a critical mindset, definitely."
Data Engineering Zoomcamp;2022;Do you think most of us have a grip of Airflow now? The most difficult part of the camp is over?;"Victoria_x000D_
Next week is a more difficult one, right? [chuckles]_x000D_
Alexey_x000D_
You think DBT will be the most difficult one?_x000D_
Victoria_x000D_
No. I'm already in DBT week. I meant week five._x000D_
Alexey_x000D_
Week five? I don't know. I think Kafka will be the most difficult one. In terms of setup, it can give more problems. Every time I need to set up Kafka, it's a nightmare. But I haven't really needed to set up Airflow locally before that. So I think it's similar. Let's see. I hope it will be easier. _x000D_
The week about BigQuery should be relatively easy because it's managed. You don't need to do any infra setup on your machine. The DBT part is also relatively simple. Maybe at the end of the course, we'll have a survey and we'll see what the most difficult one was. I hope it was Airflow. I hope Kafka and Spark will be simpler."
Machine Learning Zoomcamp;2021;If I want to do feature engineering (adding a new categorical feature) and random undersampling to balance my data, which process should I do first?;I don't think it matters, actually. I think these two things are quite unrelated to each other. You can add a categorical feature to the entire dataset, or you can take a categorical feature to a small sample. Maybe first taking a sample and then doing feature engineering is better, simply because your dataset is smaller, so you can do these things faster. But I don't think it actually matters
Machine Learning Zoomcamp;2021;How can I know my homework score after submission?;I still have to finish grading your homework for this. Hopefully, I'll do this soon. Sorry, I didn't have time last week to grade it. I will do it as soon as I can. But don't worry, I've saved all your submissions. I just need to do a couple of tweaks in my Python script for grading. That's why it's taking me so long because I haven't done these things yet.
Data Engineering Zoomcamp;2022;What do you think about Apache Hop? It's shiny, I know.;"Alexey_x000D_
Do you know anything about Apache Hop, Victoria? This is the first time I hear about it. According to Google it's a data and metadata orchestration platform. Interesting. Well, since it's Apache, I assume it's cool. But other than that. [chuckles] Usually, in order to become an Apache project – a top-level project – a project needs to undergo some process, which is not easy. For those projects that did undergo this, I think they're cool by definition, I guess. But I'm still not sure what this thing is exactly._x000D_
Victoria_x000D_
Loading big datasets, but maybe more focused on orchestrating your metadata?_x000D_
Alexey_x000D_
I don't know if it's shiny. I wouldn't classify this documentation as shiny, to be honest. There are shinier tools, I would say. [chuckles]_x000D_
Victoria _x000D_
Yeah, it's more old school. That's good. [chuckles]"
Data Engineering Zoomcamp;2022;Does a data engineer need to know algorithms and data structures? Some data engineers still face software engineering tests in tech interviews.;"Ankush_x000D_
If you're talking specifically about interviews – if the employer is sending you software engineering interviews for being a data engineer, maybe that's not the right role or maybe that's not the correct company for you. Maybe talking to the tech recruiter and understanding the role might give you more insights into what the exact role is. Because honestly, “data engineer” has been used a lot these days and a lot of companies also use “data engineers” as a way to describe data product developers or software engineers who are working on very data-intensive applications. So it might be worth clearing that up. But having knowledge of algorithms and data structures I think are the basics for any software engineering – be it data engineering, be it analytical, or be it data scientists. Learning algorithms and data structures would always help you in your career, irrespective of the fact that you are going towards data engineering, data science, or any other domain._x000D_
Alexey_x000D_
I would add that even for a data scientist, which is a less engineering-heavy specialization, it's very useful to know some data structures and algorithms. For example, using a list instead of set can degrade the performance of your data pipelines significantly, and then sometimes just one line change can increase the performance of these data pipelines like 10 times, 100 times. Knowing things like that is useful. Do you need to read Kleppmann's book cover to cover? Maybe not. But knowing lists sets, hashmaps, dictionaries – these things could be quite useful. Being proficient in LeetCode’s hard questions – I don't know how that translates to real life._x000D_
Ankush_x000D_
Yeah, I think you would be able to clear a lot of interview tests if you do a lot of LeetCode. That might not convert directly to you performing better in your job."
Machine Learning Zoomcamp;2021;As a part of a deliverable, we expect the peer evaluator to download/clone the Git repo, build a Docker image, and run a Docker container in local/host cloud?;I don't know if it's reasonable to expect this. I really want you to run it and learn from this, but if you look at the evaluation criteria, it doesn't say, “Okay, this is not runnable.” Ideally, you should do this – you should be able to do this and do Git clone, do Docker build, do Docker run, and test that it works. The evaluation criteria do not ask you to do this. You can just see what is there. For example, the criteria here say that there should be code for deploying or a Docker file is provided, and the readme should clearly describe how to actually build the container and how to run it. So if you want, you can clone it, and there should be no errors. Actually, maybe I should also add a section about errors here. But it's up to you, actually, if you want to run it or not. I advise you to run it, but I also recognize that maybe for some of you, it will require too much time – if you need to run three different projects and some of them do not run, it might be difficult. So I will leave this decision to you. If you have time, if you want to learn from others, I highly encourage you to actually run this. But I would say it's optional.
Data Engineering Zoomcamp;2022;Can we use something else besides GCP? Any recommendations?;"Ankush_x000D_
Yes, definitely. You can explore AWS and Azure if you are more comfortable with that. We will be using GCP for this course and we will be using some of the GCP tools like BigQuery, which might not be available in the other platforms. There might be similarities and alternatives to that and we might touch on that in the course. But this course will be based upon Google Cloud Platform. One of the biggest reasons for doing that is that if you create a new account, you get $300 credit. That's really useful for everybody who's not familiar with the cloud. They can just create a free account to do this course and, ideally, not spend any money on the platform services._x000D_
Alexey_x000D_
And I think another reason we decided to go with GCP is the connection of DBT with BigQuery. With Athena, I think it's a bit tricky. If we want to go with Snowflake for DBT, then it's super expensive. This is not something you want to pay for during the course. That's why we were thinking whether we should go with AWS or with GCP – A) GCP gives free credits and B) DBT works with BigQuery. That’s what led to this decision of doing this in GCP. But as Sejal mentioned, we will be able to run everything locally, except the BigQuery part, of course, because BigQuery lives in the cloud. But the rest of the stuff should be runnable locally."
Data Engineering Zoomcamp;2022;How difficult do you expect the course to be in the following weeks? Iâ€™m wondering if I need to allocate more time for the course. For me week 1 was 12 hours and week 2 was 10 hours.;"Alexey_x000D_
I don't know. Week 3 seems lighter to me than what we did previously because we don't need to do a lot of local setup. I think this local setup is where most of the time was spent. Is that the case? Am I correct? _x000D_
Ankush_x000D_
I think so. _x000D_
Alexey_x000D_
But then there is week 4, in which I don't know how complex it is to set things up._x000D_
Sejal_x000D_
No, I think it's cloud-based. Right, Victoria? _x000D_
Victoria_x000D_
Yeah, it's called cloud-based and then there's the option of installing it locally. But then it's just doing a simple pip install, or brew install, or something like that. We can also provide the links to use a Docker image, now that everyone is a Docker expert. But it shouldn't be super hard in terms of installation. I would expect a little bit more complexity in trying to understand the whole concept. The project is simple, so hopefully, you can follow along with the videos._x000D_
Sejal_x000D_
I would like to add the same as for week 3 as well. Most of the setup that we've done is already available from week 1 and week 2, the only thing is – we had to extend that setup to add more BigQuery tables. There would be barely any time invested on the setup side of things. And definitely no Docker or Docker related issues. [chuckles] Hopefully._x000D_
Alexey_x000D_
Yeah, but then we have week 5, which is about Spark. And for Spark, we need to have a JVM, which is short for Java Virtual Machine for those who don't know. We will need to install this locally and we need to install a specific version of the Java virtual machine. It should be, I think, 8th or 11th. They already have a tutorial for that for Windows. For Windows, this is where things usually get tricky. And I think for Kafka, we will also need to have a JVM. [Ankush confirms] But hopefully it will be smoother._x000D_
Ankush_x000D_
For Kafka, we are planning to do Docker. I guess we will not run into the JVM issue, but we might run into memory issues – we may run out of memory, basically what Alexey has said. I hope we can figure out a workaround for that. Maybe shutting down Chrome._x000D_
Alexey_x000D_
[chuckles] Maybe you can just use the browser from your phone. Somehow on a phone, it works better than on my computer. I don't know why._x000D_
Ankush_x000D_
Yeah, maybe Firefox? [chuckles]_x000D_
Alexey_x000D_
Yeah. But also, the amount of time you need to allocate also depends if this is your first experience with Docker. In this case, it could be tough. There is nothing wrong with this. We cannot know the background of everyone to give a good estimate of exactly how much time you will spend. What we will do, however, is at the end of each week, when you submit your homework, we will ask you how much time is spent on watching the videos and doing the homework. _x000D_
This way, we can get a better understanding of exactly how much time is spent on these things. For us, this is the first time we're doing the course, so It's very hard to have a good estimate. Everyone has different backgrounds, which makes it even harder. But I think it should be less time. Then, for week four, it's also cloud-based. Let's see. Hopefully not so much time."
Data Engineering Zoomcamp;2022;AWS datasets are already in S3 bucket. Can we bring it to Redshift or do we need to bring it to our own bucket first?;"Ankush_x000D_
I don't know, but I think in AWS, it should be possible to do it without any restrictions. _x000D_
Alexey_x000D_
Probably with a public open bucket, like we have with these taxi trips, theoretically, you just do the same thing as you would do with your own private S3 buckets?_x000D_
Ankush_x000D_
Yeah. I think the problem would be if the open source data is in Google Cloud Storage, then you definitely need to move it to an S3 bucket."
Machine Learning Zoomcamp;2021;I read in a book about the tran-dev set partition. Do you know what the purpose of this additional partition of the dataset is and when this should  be applied?;I think I explained that in the video. So just check it out. You'll find the answers there. It's called model selection. I don't think I should spend time now explaining this. If you watch the video, and you have questions about that. Actually, you said tran-dev set partition. Sorry. Dev in this case is the same as validation. So dev is just for testing the parameters. I think they usually refer to dev the same way as validation.
Machine Learning Zoomcamp;2021;Can we impute missing data before splitting the data? What if I wanted to impute? I need to look at the distribution and then it is data snooping.;It is data snooping and you cannot impute missing data before splitting the data. Because in real life, you cannot go in the future and see what kind of data there is. All you have is what you have. You only have the available data, so you need to kind of mimic this scenario of not having access to future data. This is your test dataset. You don't have access to that, so you just forget about this until you are ready with your final model.
Data Engineering Zoomcamp;2022;How close to your usual data engineering role are the workforce demonstrated? It seemed like you were not used to Docker Compose with Airflow. Is it rarely used?;"Alexey_x000D_
I can add a comment to that. At least in my workplace, we have Airflow Instance set up for us. I, as a data scientist, would just go to an instance of Airflow that is managed by data engineers and DevOps engineers, and I would just use this. I never needed to actually run it on Docker Compose. _x000D_
The moment you have more than multiple DAGs, an orchestration tool will be quite useful. I would personally maybe go with Luigi, because this is my favorite one. But if I need to think about other people as well, since very few people know Luigi and not many like it, maybe Airflow would actually be a better choice to consider._x000D_
Sejal_x000D_
I would also like to add to the question on Docker Compose. We are used to Docker Compose with Airflow, but not in a style where we use the official setup. I was seeing it for the first time as well, but it's unnecessarily complicated, especially with the XCOM variables and so on, so forth. Generally, Docker Compose acts as a wrapper to your Docker containers. In most production workflows, the Docker containers themselves are used in integration with your CI/CD pipelines wherever you're going to deploy it. Docker Compose. _x000D_
Only the Docker Compose part can be helpful in your development environments in case you have, let's say, a mock version of your Airflow environment from production and you want to test some DAGs on your local development environment. Then you can just use Docker Compose to make things easier for you, just running simple commands like ‘docker compose up’ would make it fine instead of running ‘docker, run, container name, etc, whatever you want’. So, just in terms of more convenience. But in terms of usage it’s generally used in development environments, not production._x000D_
Alexey_x000D_
What do you think about this first part of the question, “how close to your usual data engineer role are the workflows?” Because the workflows I see at work are more complex. We have more steps. This is probably also the case for you. But you need to start with something. We already have a – I wouldn't call it a complex pipeline – but there are three, four steps, which is already something. Eventually, it might grow bigger and bigger when we add other steps there._x000D_
Ankush_x000D_
I would also like to add that if you're in cloud, maybe you will not use Airflow and deploy it on Docker. Maybe you will actually use some service provided by the cloud provider itself, one that is similar to Airflow. I think Airflow would only be used internally, like a cloud composite and Google Cloud Platform. And for that, you would be using Terraform._x000D_
Sejal_x000D_
Yeah, I have used AWS Step Functions for the AWS setup. Step Functions in combination with Lambda in place of Airflow. But here, what Ankush said is also something that I wanted to say. It really depends. In production. If you have a self-hosted cluster on Kubernetes or wherever and you want to use a very pure Airflow version, then your company could have that kind of setup, and you just use native Airflow as it is on a cluster. Or you could have a cloud-based setup and that is a more convenient option, to use managed services such as Cloud Composer. AWS also has its own version now called Managed Airflow."
Data Engineering Zoomcamp;2022;What are the best practices to ingest data at scale? For example a log file getting created every minute and an API. What would you guys use?;"Sejal_x000D_
This is something that I was planning to cover in the code video for this week. The videos for that have not been updated yet. There are actually multiple ways – you can use cloud services, some of which are in an auto-scaling mode. Some of them provide that kind of usability to auto-scale clusters, such as EC2 clusters. The other way, if you have a very local project and still you want to run it on scale, then some data scientists might say that you need better hardware [chuckles] like having more GPUs, for example. I think this question is a little broad, but we will be covering different use cases in our course material. _x000D_
Ankush_x000D_
Yeah. I think there are a lot of things missing from this question. Just taking your example – it really depends on how you are ingesting it and how you are consuming it. If you are consuming it in real time, you will have a totally different architecture. If you're consuming it as a batch, then why do you care about consuming it every minute? So there are these kinds of questions. It would be way much easier to answer this if we know all of this upfront. I will actually encourage you to put this in our engineering channel in Slack. I hope somebody can pick this up or we can also pick this up. But I will also suggest that you think about the end usage. How are you ingesting it? How are you using it? Because that will also determine what the architecture will look like."
Data Engineering Zoomcamp;2022;How are restartability and incremental loads usually made in BigQuery? I'm familiar with Redshift, where it can use a stored procedure to chain steps.;"Alexey_x000D_
I must admit that I understand what each word by itself means, but collectively – maybe I’m too much of a data scientist to answer that. Too bad Ankush is not here. Maybe he would be able to answer that. What about you, Victoria?_x000D_
Victoria_x000D_
No, I would say I am also not familiar with BigQuery. I haven’t worked with BigQuery outside of this project, in particular. But I would imagine that it's the same as any data warehouse. You could just use a key and then you start to limit a loading timestamp or something like that."
Machine Learning Zoomcamp;2021;I'm using Jupyter on VSCode directly with plugins rather than running a Jupyter server or using a web browser. Any cons to that approach?;I don't think so. If it works for you, go ahead. I haven't really tried that. I know it works, but I haven't experimented. I think I'm too used to this screen. [image 5] I don't even use Jupyter lab. I used Jupyter notebooks. Maybe I'll try it, but I don't think there are any cons. In the end, it's the same Jupyter. It doesn't matter what exactly you use for that.
Machine Learning Zoomcamp;2021;What do we need to submit for the midterm project? Data, code, deployment files, etc.?;I think I just answered that in the previous question. I will make sure to document that as well in this file. We'll put deliverables here.
Machine Learning Zoomcamp;2021;In the train.py, we train the model and save to disk. Is it required to show the score on validation data, test data of the models and the script?;It's actually not. I think it's useful for you, but you don't have to do this. This train.py should produce a pickle file. Let's say you want to retrain this model using different parameters – for you, it's useful to see if it changed your score or not. In real life, like for projects I have at work, this train.py (it's not always just one file, sometimes it's a bunch of files) but all of them somehow log the performance. So I think it's useful to have. But if you don't want to for some reason, you don't have to do this.
Machine Learning Zoomcamp;2021;What do you expect as a submission in the project? Iâ€™m assuming notebooks showing EDA and data pre-processing and training script. Anything else?;"Yes. In addition to that, ideally, it will be a repo in Git or a folder in your Git repo that contains the notebook showing the EDA, data pre-processing. Then, in this notebook, I assume you would play with different models. You would play with, let's say, logistic regression, decision tree, random forest, XGBoost – maybe you want to play with some other model and you want to tune parameters. You should also keep this in the notebook – this exploration of trying different parameters. _x000D_
Then, from this notebook, you extract the training script with just the best model and you also need to deploy this model. You need to create the Python file that loads your pickle file or whatever way you use to save the model. It loads that and then it serves with a web service. So you need that. Then you need a way of managing dependencies. I recommend using Pipenv, but you can use something else like Poetry, or Virtualenv. But I think it’s better to stick to Pipenv. And then you also need to package this in Docker. _x000D_
You have to have a Docker file and you also need to have a readme that explains how you actually run this thing. Let's say, your peer (somebody who is reviewing this) wants to run your project – they need to have clear instructions on how to do this. In the readme you can just say, “First execute that (could be dockerbuild -tname) and then execute docker run -it –rm.” Something like this – so they know what exactly they're supposed to do to run this."
Machine Learning Zoomcamp;2021;Do we need to have features on the same scale for linear regression? (For example, if engine horsepower is between 1 and 100, but mileage is between 100 and 10 000 â€“ do we need to normalize both?);"Dmitry_x000D_
Usually, if you're working with linear models, for sure, you need to have the same scale. It’s a good question, but it wasn't covered in this homework due to us trying to make it a bit simpler at the initial stages. But for sure, this is very important to know if you're working with the linear model, you need to have the same scale. _x000D_
Alexey_x000D_
Here, we use normal equation, and for normal equation, maybe it matters less. Of course, if your features vary significantly, if in one case you have features that are in billions and another is between zero and one, then you probably need to do some normalization. But I think in cases like mileage and engine horsepower – yes, they are different, but for normal equation that we used here, it's not as important as for other types of finding solutions. We didn't talk about stochastic gradient descent or gradient descent in this course (in general). We will talk a bit about this in neural networks. But for this type of finding solution, there you need to scale features. Not for normal equation, though. Normal equation, you will be fine most of the time, I think, except if there is a huge discrepancy. _x000D_
Dmitry_x000D_
Yeah, for sure it will affect it. But, as you said, if there is a huge difference, then definitely. _x000D_
Alexey_x000D_
Yeah, I saw this question multiple times already, by the way, about stochastic gradient descent. This course from Andrew Ng – I don't remember whether he starts with normal equation or with gradient descent or the other way around. But today, Dmitry shared a good article with me. Maybe you can share it in the channels? Well, I think it's a nice one. If you're interested in other ways of finding solutions to linear regression, that's a good read. For that (for stochastic gradient descent) you need to normalize features."
Machine Learning Zoomcamp;2021;If we use GridSearchCV for hyperparameter tuning, do we still use the validation dataset or do we use the full train dataset?;"I think you use the full train dataset. Let me look at GridSearchCV. For those who don’t know, GridSearchCV is just a way of doing parameter tuning and selecting the best parameters. Internally, it is doing cross-validation to find the best parameters. I think there must be an example here somewhere. Here’s a good example – parameter estimating using cross-validation. You can just check out this example. _x000D_
They are tuning the support vector machine classifier and they’re tuning parameters. They are trying different values for C. They're trying different values of gamma. And then for a linear kernel – this is just internal details for SVM, doesn't matter. They're trying different sets of parameters. And then what they do is – they have this train/test dataset separation, and they are doing what we call a full train here. They are doing the fit on the full train dataset. They don't do the split again for train/validation. They're using the full train dataset and then GridSearchCV is doing the split internally. I think this is a cool thing."
Data Engineering Zoomcamp;2022;I think I read somewhere that there will be a MLOps Zoomcamp after this one. Am I right? Are those who took Data Engineering Zoomcamp in good condition to go to the MLOps Zoomcamp?;"Alexey_x000D_
I guess that you're right, there will be. This may be a bit of a spoiler. In the DataTalks GitHub, there is already a repo here, which I don't think contains anything. Not a lot of content. But, yeah there will be a course, eventually. _x000D_
Sejal_x000D_
Maybe once we take a break from data engineering? [Alexey agrees] I'm gonna go on a long vacation, and [inaudible]_x000D_
Alexey_x000D_
But these courses are not dependent on each other. Date Engineering Zoomcamp is not a prerequisite for MLOps Zoomcamp. These are two separate courses. But yeah, I think if you took this one and you understood everything and it wasn't too difficult for you and you were able to finish it, then I think you're in perfect condition for the MLOps Zoomcamp. What is probably more useful for that one is actually the Machine Learning Zoomcamp because that’s where we already talked about things like deployment, Kubernetes, and things like that. That one would be more useful."
Machine Learning Zoomcamp;2021;Is it necessary for junior data scientists to learn the model deployment stages and deep learning?;Model deployment stages, yes. Deep learning, no. I mean, it's good to know what deep learning is and what kind of problems you can solve with it. If I were to interview a junior data scientist and the person doesn't know how convolutional neural networks work, that's not a big deal. Actually, it’s the same for any level. If I were to interview a senior data scientist and they don't know how deep learning works – again, not a big deal, especially if this position doesn't require deep learning. If the position requires deep learning, that's a different question. But most positions don't require that, not only at OLX, but in general. Maybe there are some companies that specialize in computer vision, then deep learning is a must there. But I would say that the majority of companies do not require deep learning because they use other models as well, not just deep learning.
Machine Learning Zoomcamp;2021;Any tips to learn Linux?;"Yes. If you're running on Windows, what you can do is first install WSL (Windows Subsystem for Linux) and then use it. The second thing you can do is just altboot and install Ubuntu on your computer. This way, you can keep Windows and then for, let's say, development work, you just go and use Linux. Then every time you don't know how to do something, you Google it. At least this is how I learned Linux. I think you will be able to learn it this way as well. _x000D_
What you can do is – you have a command line, so just try to do as much as possible without leaving the command line. For example, say In this is Ls. If you want to look inside some file, you use “less”. Get familiar with tools like these simple Unix tools, like ls, less, CD, vd and so on – things like this. I think that should be sufficient for most of your work."
Data Engineering Zoomcamp;2022;For the project, do we need to orchestrate everything with Airflow? Or can we use Airflow in some parts and then do the rest â€œmanuallyâ€?;"Alexey_x000D_
You need to use some sort of orchestrator, whether it's Airflow or something else. But doing things manually – I don't think it's a good idea for the project. You can still finish the project. We haven't started working on the dimensions for assessing the project. But one of the criteria for evaluating the project will be whether you use a workflow orchestrator or not. So if you decide not to use a workflow orchestrator, you will not get points for that criterion. Maybe you'll lose two, three points. We still need to finalize that, but that's the idea. It’s better to use it, because in real life, you will need to use an orchestrator. It also depends on what exactly you mean by “the rest”. Maybe if you can give us a bit more details, let's say in Slack, we can answer it. But yeah – it's better to use it. _x000D_
Victoria_x000D_
It’s probably also worth mentioning something that we mentioned before that they could use. They could use another scheduler, if they don't like Airflow. Just make sure to clarify that in your project details, so that the person that will evaluate your project in the peer review will be able to understand what you've done. They may not have experience with that particular scheduler. But that could also be an alternate way if that's what you find easier."
Machine Learning Zoomcamp;2021;What is the scope of the two projects to be developed in the course?;"The project that we will have next week will be something very similar to what we did in the lectures and what we did in the homework. You will need to find a dataset – we will propose some datasets, but you can also find the dataset yourself. Then you will need to explain what kind of thing you want to predict. Should it be regression or should it be classification – what kind of problem you want to solve. And then you need to do some sort of exploratory data analysis, do some feature importance analysis. Then you will need to, of course, do some data preparation. And then you will need to train a model, actually multiple models – because now we're on week six, we will learn how to use tree based models. _x000D_
So, no matter if you're solving regression or classification problems, you can use a linear model, which is linear regression or logistic regression. Or you will be able to use a tree based model, like tree decision, tree classifier, or decision tree regressor, for example. So you will need to try multiple models and then deploy this at least locally. This is the rough outline of the project. That's the main idea. You learned a lot already. You learned how to do basically all that we covered so far. Now it's time for you to do this yourselves, without guidance – without just repeating the videos. For this project, I want to do peer reviewing. You will also need to review the answers of your peers. This way, you’ll also learn from them."
Data Engineering Zoomcamp;2022;Can you quickly run through the difference between an external table and a materialized table?;"Victoria_x000D_
Every table is a materialized table. Isn’t it?_x000D_
Ankush_x000D_
Exactly. We answered that in the previous question about external tables. I think that's what the meaning from this question was, basically, external tables versus internal tables._x000D_
Alexey_x000D_
Okay. There is also the concept of a materialized view, which is when you have a view and the view is nothing but a SQL query that is executed every time you want to do something with a table or with a view. You're kind of creating a query. The way to speed it up is – you materialize this view, meaning you create a table with all this data, and then instead of querying a query, you query the actual data. Did I get that right? [Ankush and Victoria agree]"
Data Engineering Zoomcamp;2022;Do you think it's necessary to have some basic knowledge in Kubernetes in data engineering?;"Alexey_x000D_
I think it's helpful. It's not necessary, but it is helpful. Because what happens is we have these DAGs that we run in Airflow. In DAGs, you have tasks and these tasks are often Kubernetes jobs or something like that. So you have some infra – in many companies, this infrastructure is managed by Kubernetes because it's quite simple to get a new container and execute a new job in Kubernetes. So I don't think it's necessary, but I think it's helpful. _x000D_
Then again, I'm not a data engineer – I'm a data scientist. But for me, as a data scientist, knowledge of Kubernetes was quite helpful because I could debug all the jobs that I'm running on Airflow (that run on Kubernetes) and I could see logs and do all that. I would say that if you want to learn it, it will not hurt. You can get hired without Kubernetes knowledge. With the materials that we have in this course, it should be sufficient to get hired and then you can pick up Kubernetes at work, for example. Do you agree, Victoria? _x000D_
Victoria_x000D_
Yeah. I think Kubernetes is getting more and more popular, so it can't hurt."
Machine Learning Zoomcamp;2021;Can we use PyTorch instead of TensorFlow?;Yes, you can.
Machine Learning Zoomcamp;2021;How can I practice writing codes from formulas better?;"I do not know that to be honest. The first thing you probably need to realize, especially when it comes to matrices, is that all these things (matrix multiplication, for example) everything can be expressed with Python code, or with any code. Once you see that, then you can just take a formula and try to decompose it one by one and try to see how it translates. I don't know if this actually answers your question, but there is no better way than just try and translate. _x000D_
What also helps is, when you have a reference implementation, let's say when we were implementing matrix multiplication in plain Python, we could compare the results with implementation from NumPy and see that the results are the same. So if you have some existing implementation, you can compare your implementation with the reference implementation. That probably helps."
Data Engineering Zoomcamp;2022;What would be the next biggest step for DBT, in your opinion, in terms of deployment?;"Victoria_x000D_
Around the tool, specifically? So where is DBT going, or…? How do you read this question?_x000D_
Alexey_x000D_
I think it's about the tool, not the company, right?_x000D_
Victoria_x000D_
Yep. So what will be in their roadmap or something like that? What would I expect to see for them? Or us in the project?_x000D_
Alexey_x000D_
Probably for them – where is the tool going? What is next for the tool?_x000D_
Victoria_x000D_
That's very interesting. DBT, in one of the first videos, they have two parts. Basically, one is the DBT Core and that part is open source. That's the part that does the whole magic. And then they have DBT Cloud. I think their next steps are going to be around developing DBT Cloud, because that's where they make the money. The thing would be in DBT Core open source, is that they literally provide their information to the competition. So then, they would try to DBT Cloud stronger to compete better. I would say that. _x000D_
Then, more specifically, things that they recently introduced and I would really like to see would be more around data cataloging or data governance. They recently introduced something called Metrics. You could actually define something like utilization rate or net revenue or stuff like that – how that is calculated and where that comes from. _x000D_
That's a big problem in companies. It's very, very common that everyone has their own spreadsheet. They're like, “No, no, in the calculation that revenues is this.” And numbers change. So treating more data as a product, in general, I would say would be a next step as well. That said, this is my opinion."
Data Engineering Zoomcamp;2022;Is there any way to collate all the frequently asked questions into a knowledge repository so that it's easier to self-help in troubleshooting issues?;"Alexey_x000D_
For that, we will ask for your help, because it's very difficult for us to prepare the materials and also create a curated list of frequently asked questions. So if any of you can help us – you're already doing this in Slack by answering questions – but if you can help us and do that, we'll much appreciate that. But I am not sure if it's physically possible for us to also invest time into that right now, at least for me. I'm not sure I'll be able to do that. _x000D_
So please, if you want to do this, we will be happy to support you. You can just create pull requests, for example, for our repo with a Frequently Asked Questions section. Or, for example, in the readme – if there is a place where many of you have errors, maybe you can create a pull request saying, “If you have an error like this, please do that.” It will be helpful for many people._x000D_
With Slack, I think it's a bit overwhelming right now with the amount of questions there. Many of them are repetitive, so I kind of wish we had a list with frequently asked questions as well. Then it could be easier to direct it. It's just that putting it together requires attention._x000D_
_x000D_
Sejal_x000D_
Another thing I was actually thinking about is that maybe we can create a GitHub readme page of FAQs and people can just create pull requests to add questions there. In cases where there are pull requests with repeated questions, we can just mark it as stale because they can already go and look over there. On the other end, as Alexey said, since we have a big time crunch and resource crunch – we are swamped with preparing the material as well as managing our full-time jobs. _x000D_
We're trying to do as much as we can, but if someone can volunteer to prepare maybe a Notion doc, for example, and organize all the questions that are asked in the Slack channel, and then put it all onto that in a Notion doc or Google Doc, whatever you like, that will be very, very helpful for us."
Data Engineering Zoomcamp;2022;What are the most important skills for a new analytics engineer to master other than SQL?;"Victoria_x000D_
SQL is definitely super important. It will depend, as in any other role, on what the company is going to do. I personally don't really use much Python, outside of maybe some API scripts. But SQL is definitely the one I use every day and a lot. It could be very good if you're at least familiar with Python and can write with it. Then come data modeling concepts, because keep in mind that you have to focus a lot on transforming that data to be lighter use. Then it's very important that you know how to transform the data. Then, you would need to know some BI – being able to expose that data later on to some BI tools. _x000D_
This doesn't mean that you need to focus on building dashboards or things like that, but at least know how that data could be used from a business perspective. Once you have that, then it’s also important to know concepts around data warehousing – at least know the parts of the ETL, ELT, all of these concepts. Know how the data gets to the data warehouse and be able to understand that to kind of cover both parts. It would be very important."
Machine Learning Zoomcamp;2021;If it's feasible, could you provide some insights on hypothesis testing and a use case that could be referred to in order to understand implementation?;That's out of the scope of this course. But there is a good video on our channel, called A/B Testing from my former colleague, Agnes. Agnes talked about how we use A/B Testing at OLX. It also goes into hypothesis testing a little bit. But we will not cover it in this course.
Data Engineering Zoomcamp;2022;I got to know about this course today. Is it possible for me to begin now?;"Alexey_x000D_
I think it is. We still have two days to finish your first homework, although maybe it could be a bit challenging, considering that some people already spent 10 hours last week. If you need to catch up, maybe take it a bit easier. You can join now, but maybe don't rush and try to do homework 1 and homework 2 now. Again, I want to say that, at the end, when we say that you have finished the course, we look at the project. If you completed the project, then we considered that you completed the course. So don't stress too much about the homework._x000D_
Ankush_x000D_
I think it's definitely doable even if you start now. It's still doable. I would also encourage you to look into the topics and if you see that, “Hey, you know what? Terraform is not something I want to learn right now.” Maybe you want to skip that. It’s the same for Docker and the same for any particular technology you think you are comfortable with – you can skip those videos for now if you are already running late on those topics."
Data Engineering Zoomcamp;2022;Do we have to use infrastructure as code tools or can we set up a data warehouse with UI?;"Alexey_x000D_
Let me go see the criteria. Here, we have one of the criteria, which is cloud. You get zero points if you do not use cloud at all. You get two points if you use Cloud and use UI for setting up things. And then you get four points if you use Cloud and you use infrastructures as code tools. So yes, you can use UI. You don't have to use Terraform or anything similar. But if you do use that, you will just get two more points."
Machine Learning Zoomcamp;2021;Can test-driven development be applied to machine learning?;"It probably can. To be honest, I'm not a big fan of test-driven development, in a way. I mean, tests are important. But the way TDD works is that you first come up with a test that fails, then you implement things and then you fix it (make the test green) and then you have this iteration. I don't find it super useful, nor do I like it. That's just my personal preference. I usually implement something and then I test it. But those are more like general software engineering practices. If you’re talking about test driven development for machine learning in the way that you first write a test and then implement something – I don’t know. If you’re talking about the model and then you say, “Okay, this model has to have this kind of accuracy,” you train a model, and then it's not the way you expected, maybe it doesn't make much sense. _x000D_
But on the other hand, there is a lot of code around the model that needs testing. For example, all these things that I showed today – all of them may fail or behave not in a way that you expect. Having tests around that, I think is quite important, if you want to make sure that you have things under control. For example, this could actually be a good test. You have an example, you transform it, and then you want to make sure that this neighborhood (Fordham) doesn't appear in your categories. This could be a good test, maybe. Actually, it makes sense to write tests – not for the actual machine learning thing, not for the model – but for all the things around the model. There is a data preparation pipeline, so you probably want to test that. If you get the results and you do some post-processing of these results, you want to test them. But testing the model itself could be tricky."
Data Engineering Zoomcamp;2022;How can I deploy Airflow on Kubernetes? Is there a Kubernetes operator for Airflow?;"Sejal_x000D_
Yes, there is but, again, I am not really a Kubernetes expert. Maybe Alexey can help._x000D_
Alexey_x000D_
If we do a bit of Googling, we can see that there is a Kubernetes operator. That probably answers that. But yeah, there is an operator and you can use Kubernetes for that. It will internally, I think, create a Kubernetes job and it will just execute. What we typically use is not Kubernetes operators, but we use AWS Batch operators, which is essentially the same thing, except you're not running Kubernetes on AWS Batch. From what I see, this is usually the pattern of how exactly you use Airflow. So you do not use Python operators or bash operators – you do not execute these things on the workers – you usually make workers pretty dumb (they don't have a lot of resources) and they instead delegate to some external compute environment like Kubernetes, AWS batch, maybe ECS, or alternatives on Google Cloud, or Azure. _x000D_
You state how much resources you need for this specific job and where you want to run it, what the Docker image is, what the parameters are, and they are executed somewhere – not on the worker. This is the pattern I see. Or, usually, it's either SQL operators that you execute on BigQuery, or Athena, or Presto, or something else, or these kinds of AWS batch Kubernetes jobs, or Spark, for example."
Machine Learning Zoomcamp;2021;What should we do if certain model results are not as great as we think? Is there a requirement for model outcome data?;No. Like, as I said, it's always problem- and data-specific. For some datasets, you just cannot get very good performance
Data Engineering Zoomcamp;2022;How do we know if the homework was accepted?;"Alexey_x000D_
Google form works fine. Don't worry about that. If you clicked “submit,” then it worked and it was accepted. But I guess your question is about your scores and you want to see if your answers were correct. For that, we will show the solution on Wednesday. We'll just upload the video with the solution and there will also be some sort of leaderboard. Maybe I can actually show you because we already did something similar for the other course. There is a leaderboard that shows how many points you get for each homework. We don't show your email here, of course, it's a hash of your email and then you see how many points you got for each question. As a result, you get a leaderboard with the total amount of points. You will know soon which of your answers are correct or not."
Machine Learning Zoomcamp;2021;How many hours do you recommend to invest in the course?;I don't know if it's easy or not for you. It's up to you. If you have a job, then I think it's difficult for you to invest time. You know the minimum requirements and just work from these minimum requirements. The minimum requirement is that at the end, you have a web service that is deployed in Docker, that you try multiple models and you also have a description of your project. Basically, if you do all that, you're good. If you want to spend more time – do it, by all means. If you satisfy the requirements, you will get a good enough score, don’t worry. I will also ask how many hours you actually invested, because it will be quite interesting for me to see how much time people spent. I see that some of the students spend like 20 hours on some of the materials. When we do other iterations of the course, I will already know how much time people will need to invest. So when you submit your homework, please tell me how much time you spent on this.
Machine Learning Zoomcamp;2021;Can you please clarify where to use from_logits=True?;"This is when we have classification. In classification, let's say if this here, we weren't predicting the price, but let's say we were predicting if it's above average number of hours. [image 1] In this case, we would need to have another layer here – or rather not a layer, but it's called activation. If we don't have this activation, we're still… I’m trying to formulate it in a way that it's not confusing. But I think it's better if you just go to the lecture and rewatch it, because I’m essentially just retelling the same thing. Here, we do not need from_logits, because we have the price variable. Our loss is a mean squared error. There are no logits. But if our loss is categorical cross-entropy, and categorical cross entropy is usually used when we have multiple categories, then this is when we need to use logits, because it's numerically stable. _x000D_
We don't have to use it – we can stay activation softmax and then we don't use legit. I hope it's clear. For regression, we don't need to use it. For homework, you will not need to use from_logits either, because in the homework, we have just two classes. It's a binary classification problem. But when we have multiple classes – let's say three classes, or four classes, or five classes – this is when we need to use logits, because then the loss we have is categorical cross-entropy. When the loss is categorical cross-entropy, then it's recommended to use from_logits=True because it's numerically more stable."
Data Engineering Zoomcamp;2022;What is a landing zone?;"Ankush_x000D_
For example, if you're doing a data pipeline for the first time, you might want to consider something like a data lake or a data warehouse solution. That would be your landing zone. But in a lot of cases, what I see is people already start to think in a very wide range and say, “Okay, this is my raw landing zone and then there is my first transformation landing zone and this is my main transformation landing zone.” They also name it “bronze” and “gold”. Yes, you can keep maybe a couple of them. But don't overcomplicate it. Don't keep like seven of them because that will just be overkill for your first pipeline."
Data Engineering Zoomcamp;2022;For someone who wants to now apply for data engineering jobs after the course, what else do you recommend? SQL, Python coding, AWS/GCP in depth, etc.?;"Alexey_x000D_
Maybe I would suggest just to start applying and then see what they ask you. What we’ve covered is already sufficient to start working, in my opinion. Of course, we already expected that you know some SQL. Becoming better at SQL is always a good idea. Python coding is also helpful. I think that the best thing you can do now is just to start applying and then you will either get a job immediately or you will get some feedback from companies. You will see, “Okay, it seems like they need this thing, but I don't know this. Let me improve this.” _x000D_
Victoria_x000D_
I feel the same way. If you start applying, just know that you're going to get rejected, but you'll get a good idea of what you're missing. Have in consideration that every company has their own expectations and different things, but you get a sense of where you stand. Try to ask for feedback, as well. Especially if you get to the home assignment challenge – if you get to the technical interview stage, that would be super helpful to get you oriented. People usually are nice, I would say. You can get to the technical interviews. I think I had people that were nice in the technical interviews. You actually can have a discussion on the technology and they can even orient you a little bit. So it could be a good resource, I guess._x000D_
Alexey_x000D_
Also, maybe do projects. Instead of focusing on, “Should I improve coding? Should I learn AWS?” Let's say instead of just learning AWS, maybe come up with a project and then use AWS for this project. In my opinion, this is much more productive rather than just preparing for an AWS certification. This is a good thing, of course, but when you focus on the project, you focus on the things that you really need. Then you have something at the end rather than just AWS knowledge. You get something that you can put on the CV and then people can go to your GitHub and check it out. I would recommend doing projects instead of just learning things._x000D_
Victoria_x000D_
It's also more fun. You have a lot of troubleshooting to do when you do a project, plus you have motivation – you have to analyze the stream, or prepare data or stuff like that. It's definitely more entertaining as well. You get to know a bit of everything, which are things you've got to do as well when you're working._x000D_
Alexey_x000D_
There is a thing called “just in time learning” where you have a problem and you need to focus on solving this particular problem. Then you start learning what exactly you need for that. Projects are cool."
Machine Learning Zoomcamp;2021;Are there any datasets that are not allowed?;"It's actually a good question. There are datasets like Titanic or IRIS or Wine Quality dataset, Boston Housing is also a good example because of the ethical issues. There are datasets that are just so common that I don't think if you use them, you will learn much because there are so many tutorials out there. I will have a list of datasets that I do not suggest to use. If you really want to predict if somebody will survive the Titanic catastrophe, go for it… I'm trying to say it’s a boring dataset but besides that… people actually died. _x000D_
Anyways, just go to Kaggle and pick a dataset and you'll be fine."
Machine Learning Zoomcamp;2021;Will the project involve deployment using Flask or Django?;Please use Flask. But if you want to use Django, you can use that as well. Just be clear when explaining what exactly you're doing so that others can use and test it. Ideally, as I said, everything you do should be a Docker image at the end that people can just run and test it. This way, they don't really have to know if there is Django or if there is FastAPI or if there’s Flask. It shouldn't be a Docker image that is runnable.
Machine Learning Zoomcamp;2021;I want to know the theory behind machine learning. Could you recommend a book or course that covers that part?;I think I did recommend a course once. You can watch a course on Coursera by Andrew Ng. It's called machine learning. It covers theory quite well. The theory is also on an intuitive level – the math there is comparatively less difficult when compared to textbooks. Then, of course, you can get textbooks. A good textbook is Elements of Statistical Learning. It’s really good – there is a lot of math. I think there is a book called Pattern Recognition, or Machine Learning Probabilistic Approach – that's another book. One is Bishop, and for the other one, the author is Murphy. They're very mathematical. There is a lot of math. You really need to have a couple of years of calculus in your background to be able to understand that. But if you don't have that background, the course by Andrew Ng is good.
Data Engineering Zoomcamp;2022;How important are certificates? I am unable to complete the course the course due to work pressure. Can I do it on my own pace?;"Alexey_x000D_
They are not important. I hire people and I have never asked anyone to show me their certificates because I don't care. Usually, you have an interview and if you see that the person you're interviewing knows something, then they know something and you don't care if they have certificates or not. I wouldn't say it's important but maybe it's also nice-to-have thing. It gives you some sort of achievement, but I don't think you should stress too much about that. Just take the course at your own pace. The most important thing here is learning and not whether you get a certificate at the end or not._x000D_
Ankush_x000D_
I would like to add to that. Whatever Alexey said is absolutely correct. It's more about learning, it's not about the certificate. But in case you are a fresher and you are starting your first job, then I don't know if it would be helpful, but it might be helpful. Think of it like this. If you do get a certificate, put it on LinkedIn, so that people who review your profile can actually see that you have it._x000D_
Alexey_x000D_
It will also serve as an advertisement for our course, right? They will see “Okay, Data Engineering Zoomcamp? What is that?” Then they will click on this thing and they will find this amazing course – they will know that the course is amazing and they will immediately trust that you learned something and they will maybe hire you._x000D_
Ankush_x000D_
That was not the direction I was going into, but yeah – why not? [chuckles]"
Machine Learning Zoomcamp;2021;Is randomizing initial weights a good strategy to get better results and reproducibility?;"Alexey_x000D_
I think the question is how we can actually make the results reproducible in neural networks. Every time we actually create a layer, it gets initialized with random weights. This actually does happen._x000D_
Dmitry_x000D_
We cannot talk about full reproducibility here because of the stochastic nature, and there will always be some fluctuations. The question is how severe they will be._x000D_
Alexey_x000D_
For neural networks, it's very tricky to make sure it’s reproducible. That's why the homework for this module was also a bit tricky._x000D_
Dmitry _x000D_
Regarding the answer 0.45. It was a bit strange. You mentioned that there was the answer 0.45. _x000D_
Alexey_x000D_
It is strange indeed. I think the main reason for that was because there was no shuffle. [Dmitry agrees] When I added shuffling maybe some people submitted it before I explicitly wrote it in the instructions that you need to shuffle. That could be the reason as well. Let's say you're evaluating it, and then you just get a bad batch with only dogs. Then maybe for this particular batch the accuracy is not great. It can happen."
Data Engineering Zoomcamp;2022;How do you override the schema of parquet files in BigQuery once stored in cloud storage? It wasnâ€™t working for me, but CSV was easy.;"Ankush_x000D_
I don't know the answer to this because I only tried it with CSV. I didn't try it with parquet. The thing is, with CSV, it automatically detects it as date time. And with parquet, the date fields are converted into byte arrays. I think I've seen this issue multiple times on Slack. I have not sorted it right now. What I can suggest is that, I can try it, I can maybe then put the solution on Slack._x000D_
Alexey_x000D_
I think the problem with this one is because you already have schema embedded in your parquet files, right? Then if BigQuery sees that in one parquet file, you have one schema, and then in the other one, you have a different type, then it doesn't know what to do with this. Right?_x000D_
Ankush_x000D_
Yeah, that can definitely be the case."
Data Engineering Zoomcamp;2022;I understand that there is a growing importance on data cataloging. Do you think it's beneficial to put something together for data cataloging tools, OpenMetadata, etc.?;"Alexey_x000D_
I think this is one of the topics that relates to the very first question regarding what is important, but outside of the scope of the course. These data governance tools could be one of those things. I don't know if they're data engineering per se. They're related to data engineering, but I think they are more about general data governance – how you organize data and how you manage data in your company. This is definitely an important thing, but I don't know which of us can actually put something together for that. Does any one of you know anything about OpenMetadata, etc.?_x000D_
Victoria_x000D_
I work a lot on trying to select a data catalog but haven't used OpenMetadata or anything like that. I would suggest, if you have the time, yes, try it. It's also something, as it is quite new, maybe not as advanced as it should be. But judging by the size of the modern data stack and all of that, it's probably not something that any company will ask you, so don't feel pressured. What I'm trying to say is, yes, it's very important – it would be quite nice if you can read about it, understand the concepts and all that. If you have the time to try OpenMetadata or any other open source data catalogs that are out there, that's great. Do not feel the pressure to do so. You'll probably get to do it. There's also more on the data engineering side, I would say. Probably like a Glue crawler or something like that. That's also maybe what the data engineer would participate in regarding the data catalog. We also kind of saw a data catalog from the DBT part, but that only covers that part. It's a bit more than that, if you think about the whole pipeline that everyone made for the project. Maybe we'll do something for the passwords, if we have the time._x000D_
Alexey_x000D_
We just use an internal data catalog. It's pretty convenient. I don't think I'll be able to record anything about that. But actually, maybe it's a good idea to invite somebody to do a webinar about that. If any of you knows someone who did talk about any of these tools, maybe you can connect us, and then we can have a webinar about that._x000D_
Victoria_x000D_
You've had an episode about data governance, right? A long time ago?_x000D_
Alexey_x000D_
Yes. _x000D_
Victoria_x000D_
I can't remember the name, but they had a book._x000D_
Alexey_x000D_
Yes, they do. They have a book._x000D_
Victoria _x000D_
And I have this book. I read it during my project around selecting a data catalog._x000D_
Alexey_x000D_
But this one is more like a discussion about why we need data governance and a data catalog. I think one of the points here is that data governance is just about having a data catalog. This is a nice episode, where it's more theoretical. If you're looking for hands-on practice, this is not the right place. But this is definitely an interesting place to at least start to understand what data governance is. Yes, they have a book, so check it out."
Machine Learning Zoomcamp;2021;I just came across this course. What is the best approach to take this course? I'm late to join the course and it seems like there are some submission deadlines.;You can take it at your own pace. You can just follow it from the very beginning. And if you have any questions, just ask us in Slack. I guess that's the best approach. Or if you're a just-in-time learner, you can just go ahead and start working on the projects. The next project we'll have will be a capstone project. You can start working on the capstone project right now and just figure out what you need to learn to actually do the project. It’s up to you. The way I imagined it in my head – now it's, we're doing this together, but all the videos stay there on YouTube forever and anyone can join it at any time and follow the videos, do the homework, and ask questions.
Machine Learning Zoomcamp;2021;I was trying to use Conda as an environment manager. I was able to create an environment in the container, but I was not able to activate it directly on startup of the container. Gunicorn build.;Yeah, this is tricky. I know. I saw a link in Slack that somebody posted about best practices of how you can actually do that. That's one of the reasons I usually don't use Conda in Docker. I like Conda – I love Conda. I love Anaconda. I have them installed on my laptops for local development. But when it comes to deploying things, then yeah… There are advantages and disadvantages. But I usually use something like Pipenv to manage the environment, and I use Conda for local development. Because I can just install Anaconda and have all the libraries I want on my local computer and then I just train a model there. When it comes to productionizing, I create the environment file and put all the scripts there. That was a long answer. The short one is just go to Slack and look for the link with the best practices for putting Conda in Docker.
Machine Learning Zoomcamp;2021;Is clustering problem allowed for the midterm project (multi-classification)?;Multi-classification – yes. Clustering – I'm not sure, probably not a good idea because we don't cover it here. It will be hard – you will not know how to evaluate it and your peers will not know what to do with it. Better not to not do clustering.
Machine Learning Zoomcamp;2021;Are we going to tackle Kubernetes and TensorFlow Serving and Kubeflow and Kubeflow Serving?;Yes, we are.
Data Engineering Zoomcamp;2022;This is somewhat unrelated to the lecture videos, but what would be the pros and cons of AWS and GCP? It seems like AWS is more popular.;"Victoria_x000D_
I would say it’s also because there's a different set of products. For example, for a data warehouse, if I were to start a data tech stack from scratch, then I would definitely use Redshift because the cost is lower. But then, in the end, when you need more, you probably would need to migrate to something like BigQuery or Snowflake. For that, they have a lot of documentation, it's pretty easy to set up, it's been around for longer – I think the reason why it has a broader adoption is mainly around those things._x000D_
Alexey_x000D_
It seems to me that AWS is more popular as well, at least in Berlin. If I look at companies that use some cloud, maybe 70% of people use AWS, 20% may use GCP, and then the remaining 10% use Azure. That's roughly how I see it. Maybe I'm wrong. In this course, I also got to use GCP a bit and it seems to me that the UI is nicer. When you go to the web console, it's a little bit nicer. But I think when it comes to tools, it's a bit more difficult. _x000D_
For example, in AWS, you just do a pip install for AWS CLI and you have it. With Google Cloud, it's a bit more difficult. Maybe that's one reason that AWS is more popular – it’s more mature, maybe. But I think GCP is catching up because they have a nice interface and sometimes some things are cheaper. BigQuery is also, I think, a big advantage that maybe other clouds don't really have._x000D_
Victoria_x000D_
Plus AWS has been around for 10 years. I did my thesis using AWS and there was no such thing as GCP yet. There's a lot of people that go with what they're familiar with, outside of the stability._x000D_
Alexey_x000D_
But I think in most of the cloud services, the services are quite similar. For me, I’m more used to AWS. For me, many things in GCP were straightforward and some of them weren't. Some of them were typical, but more or less, I think many concepts can map from one to another."
Machine Learning Zoomcamp;2021;Can you please share the code for model2.bin? I tried to unpickle it but couldn't.;Yes, I can. I think I just wrote you the code. [image 2] This is the code I used for creating model2. You can take a screenshot and copy it and then just run – then you will have it. I can also send it in Slack if you want. Just tell me if you need it.
Machine Learning Zoomcamp;2021;What changes are required to host and port settings in Python script (both Flask and requests script) if we deploy our container to cloud (i.e. Heroku)?;It really depends on the cloud. I showed you how to do this with Elastic Beanstalk. I don't think I needed to change much there, if anything. I don't remember changing anything there. So the only thing I changed was the URL for the service. In this test.py script, that was the only thing I needed to change. I think for Heroku, it's similar. Actually, there are a couple of tutorials about using PythonAnywhere and Heroku. They will probably explain what you need to change here. Just go through this and see if you need to change anything or not. Probably not much.
Machine Learning Zoomcamp;2021;Is there any easier way to do one-hot encoding without using DictVectorizer?;I think DictVectorizer is the easiest one. This is the reason I thought it would be good to use for this course. Let me show you. This is how you can use a one-hot encoder. I also showed you that you need to do “column stack,” which makes it a bit more difficult than using DictVectorizer. With DictVectorizer, you just throw everything in and you get a matrix out, which I find easier than a one-hot encoder. I'll share this code as well. You can take a look at this and decide which is easier for you – one-hot encoder or DictVectorizer.
Machine Learning Zoomcamp;2021;Can you talk a little bit more about ridge regression? Are there other regression types that are popular in data science?;Ridge regression is an implementation of regression. We have linear regression, which is regression without regularization, and then we have regression, which is regression that has regularization. Ridge regression is very similar to what we've had in the previous lesson. It's more optimized. Other regression types (or regression models, let's say) that you can have are tree-based models that we will cover soon. Then there are neural networks, which can also solve regression problems. There are definitely many other models that you can use for solving regression. I think maybe for the week when we will train tree-based models, we will also solve a regression problem using trees.
Machine Learning Zoomcamp;2021;Will you help us coordinate for peer reviews? How will I decide who to review and who will review me?;For that, the way I want to organize it is: now, everyone is working on a project and then will submit them by the first of November. On the first of November, I will see who submitted the project and then from these people, for each of the projects, I randomly select three people to review it. This means that each one of you will get to review three projects. I will send an email (I hope it will work, I still haven't written a script for sending the emails) saying, “Hey. You will need to review these three projects. Here's the form. You will need to put this hash there and you will need to put your assessment there.” You will have one week to do that.
Machine Learning Zoomcamp;2021;How many hours of daily study are recommended? And how to study continuously without a long break?;"Well, to be honest, I don't know how many hours of daily study are recommended. That's why I added a question about that in the form. I will share the results later with you about how many hours it usually takes on average. But again, the answer depends on your background. If you're comfortable with coding, then maybe you don't need to spend that much time compared to if you're new to Python, for example, because then you would need to put some more effort in. _x000D_
As for the numbers, at the very least, maybe you want to watch lectures first and then spend one or two hours on the exercises. Maybe in total – three, four hours. Again, it depends. I don't really know. As for “How to study continuously without a long break?” Maybe you shouldn't study continuously without breaks. I don't think it's good for you. So it’s better to have breaks."
Data Engineering Zoomcamp;2022;In some resources, there is a .collect() method applied after a query on PySpark. Could you please explain in simple words what the purpose of this method is?;"Alexey_x000D_
The purpose of this method is, let's say you have this… Well, I'm going to say RDD, but they haven't released videos about RDD so I'll try to explain briefly. RDD is a distributed dataset. Let's say you have a usual python list, this is the same, but this is not a usual Python – this is a list that is distributed across many machines or data frames. Data frames are also a sort of distributed collection that we have, which consists of many partitions. Then you do some operations on this data frame or on this RDD, you invoke this .collect() to get everything that this data frame or this RDD contains in a memory object. You kind of move to think you have (a distributed collection) and you turn this into a collection that you have in your Python driver node. Maybe that was a long explanation. Maybe you have a shorter one, Ankush?_x000D_
Ankush_x000D_
If you want to count all the elements, you need to put it in to one node to count it. So that's where you will use .collect(). But do remember, if you use .collect() on large datasets, you will also run out of memory. Be careful of using it and be aware of what you're doing. It's really helpful if you want to collect and then write to one file. Then that can be helpful. But be careful while using it._x000D_
Alexey_x000D_
So this is a transformation. When I was talking about actions and transformations, most of the operations in Spark are transformations, but some of them are actions. As an example of actions, I talked about “show,” I talked about “take,” I talked about “head,” and I also talked about “write”. But “collect” is also an action. It triggers the entire execution, but then instead of saving it somewhere, you get it as an object in your driver node."
Machine Learning Zoomcamp;2021;How to document files, write GitHub readme files? Do you have any resources?;There are no guidelines. I do not have any resources. Just write something. It's the same way you write a document – just use markdown for that.
Data Engineering Zoomcamp;2022;Do you see data engineering being outsourced, like coding, for example? I tend to see people sourced out of Russia, Ukraine, and Poland? Is data engineering also affected by similar moves in 2022?;"Victoria_x000D_
Yes. Don't you think so? I received a bunch of LinkedIn messages from people offering me to outsource engineers. I don't know where from, though. _x000D_
Alexey_x000D_
They approach you, but it doesn't mean that companies actually agree._x000D_
Victoria_x000D_
Oh, yeah. I don't know who agrees. I've never agreed so far. I’ve never even talked to them. There must be someone if an agency is providing that service, right?_x000D_
Alexey_x000D_
I think that when it comes to data, it's a little bit trickier than with basic coding. The data can be sensitive, then you need to have all the right things in place. First of all, you need to have a lawyer that would put a proper data regulation contract. Then you also need to have an infrastructure that allows you to maybe not send sensitive data to these outsourcing partners. _x000D_
I think it becomes a bit trickier than with, let's say, traditional software engineering. On the other hand, with traditional software engineering, you also get to see things like phone numbers, emails, etc. Maybe we will see something like this. I see that, for example, data science is often kept in-house rather than outsourced. Something similar probably still happens with data engineering. I don't know. _x000D_
Maybe we'll see this in one year. It changes. But people also reach out to me saying, “We have this great team in Minsk, Belarus. We want to start working on your projects.” And then I have to answer them saying, “Sorry, we don't need your services right now.” _x000D_
Victoria_x000D_
Yeah, same. _x000D_
Alexey_x000D_
I was probably assuming too much about the distinction between data engineers and software engineers. I remember that when I was in Poland, I worked at a bank and it was actually an outsourced sort of team. We didn't see personal data – we had some sort of masked data – but we still needed to write code that would deal with all this sensitive data. So in this aspect I think there are ways to ensure that data is not leaked. Maybe we just need to wait a bit until this area gets more mature and then we'll see more data engineer outsourced work._x000D_
Victoria_x000D_
I’ve also seen that in a company that came from Ukraine. They were in Ukraine and they used to come to the office once a year or something like that. They had complete access, but they had to sign something._x000D_
Alexey_x000D_
And they also needed to come to the office, right?_x000D_
Victoria_x000D_
Yeah, once a year for a week. I guess it was to have meetings."
Data Engineering Zoomcamp;2022;Are data engineering skills needed for a machine learning engineer?;"Alexey_x000D_
Yeah, I think they're helpful. Not all of them. For example, machine learning engineering often requires building some data pipelines for the model. Sometimes it's done by data engineers and sometimes it's done by data scientists. Sometimes it's done by… basically everyone can take part there. _x000D_
If a team doesn't have a dedicated data engineer, somebody still needs to do this – a machine learning engineer or a data scientist –knowing tools like Airflow, Spark, maybe DBT, is quite helpful to prepare the data in the form that is useful for machine learning models."
Data Engineering Zoomcamp;2022;Can you give us a tour of the Slack channel?;"Alexey_x000D_
This is our Slack. You can see the default channels that we have here. For any course-related questions, you should go to the #course-data-engineering channel. You need to click on the plus icon, then “browse channel” write “course” in the search and look for the relevant course channel (#course-data-engineering). You can just click on “join” and you will have it here. This is the channel that you should use for asking course-related questions. _x000D_
We have a lot of other stuff happening in the community. For example, #welcome is where you introduce yourself. I think some of you just did that. This (#course-ml-zoomcamp) is the channel for the other course we had. You just click on this, browse channels and you can see what we have here. For example, we have a channel about engineering, which is about discussing engineering things. We have a channel about data science. We have another cool channel that is called #book-of-the-week. We invite book authors and they come and answer our questions. You can check this out as well. _x000D_
To register in Slack, if you still haven't done so, you can go to DataTalks.Club and just leave your email here on the main page where there is a prompt to do so and click on “join”. You will receive a link. If you don't receive a link, reach out to me through Twitter or LinkedIn – whichever way you prefer, and give me your email. I will use a different way of adding you to Slack."
Machine Learning Zoomcamp;2021;Is there any other way to transform the y variable apart from log1p (and expm1) and if so, when and how should we transform it?;"Dmitry_x000D_
Usually I use the NumPy function. _x000D_
Alexey_x000D_
I think also the question is, “Do we need to transform the y variable in any way apart from this logarithmic transformation? Are there other transformations that we need to know?”_x000D_
Dmitry_x000D_
I can think of the Box Cox transformation. There are certain types of transformations of the target variable, especially in the regression tasks. You have to make it more randomly distributed, for example. For sure, you can leave it like that, but the results, especially in a linear model because linear models really depend on the linearity, or the normal distribution of the target variable, so the results can be not very good. _x000D_
For sure, throughout the course of the Zoomcamp, we will talk about the nonlinear methods such as end symbols, for example, end there. You can see that, for example, you can use it without transformation – using the trees._x000D_
Alexey_x000D_
Yeah, thanks. “Cox Box transformation” you said? [Dmitry agrees] I think this one turns any variable into a normal, right? [Dmitry agrees]"
Data Engineering Zoomcamp;2022;What would be the advantage of using Airflow over Azure DataFactory as an ETL tool?;"Alexey_x000D_
I’m guessing this is some sort of managed solution for doing ETL. If it is, then the advantage is that you don't need to manage Airflow. For Airflow, you need to host it somewhere, somebody needs to look after it, if your DAGs don't run, then somebody needs to go there and check why they don't run. It could be you. Do you want to do this or not? If it's a managed solution, then all these headaches go away. But I don't know how flexible it is. So that's the advantage. But there’s also the disadvantage that Airflow can do a lot, while Azure DataFactory might be quite specific to Azure. Maybe you cannot do things that are outside of Azure. But I'm just speculating. I've never used it._x000D_
Victoria_x000D_
Actually, it turns out that it's the cloud version of the information service. I think it is from SQL. I've used it in the past and I would say I'm also not a super-expert Airflow, but it's definitely flexible. I'm guessing it's better now in Azure and all of that, but the problem there is that you need to have the SQL Server and all of this. You're limited in the sorts of things you can use, you're limited in the things that you can do. It wasn't super easy. You don't have a DAG – you're gonna generate it yourself. You can generate all these steps but it's not that clear, at least from my experience in Airflow and then using SSIS._x000D_
Alexey_x000D_
Azure DataFactory is something similar to SSIS, right?_x000D_
Victoria_x000D_
Yeah. It's like the cloud version, kind of. It says that it's looking to modernize SSIS._x000D_
Alexey_x000D_
I remember the first time when I saw this SSIS, I was like, “Why did I spend all this time coding things in Java if I could just drag and drop things and connect them with arrows and it just works?” The answer is that it just doesn't always work. Sometimes you need some flexibility and that's why you need Java developers who would go there and fix it."
Machine Learning Zoomcamp;2021;Can we check for correlation/feature importance of categorical variables for numeric target?;Yes, I think this is what they showed you. I'm not sure what you mean. But yeah  I think it's a good idea to do this.
Machine Learning Zoomcamp;2021;Is there anything else that we have to be aware of for the midterm?;Nothing comes to mind, but maybe if you're uncertain about something, please ask.
Machine Learning Zoomcamp;2021;That mean proof screen â€“ how did you get the plus?;Maybe you're referring to this plus here. [image for reference] Let me maybe say it once again. First, when we compute the mean without missing values, we use only that part of the array (the number values). The formula for that is Σxi. But now, let's say we include the missing values, and we fill these missing values with the mean x̄. Now, instead of just having n elements, we have k more elements. In total, we have k+n. So that's why you have k+n here. That's how you have the plus here. Because first, you need to sum these elements and then you need to sum these elements. So that's why you have this sum here. I hope I understood the question.
Machine Learning Zoomcamp;2021;Is there some function or library to recode the categorical variables automatically, instead of the map method()? Just imagining doing this for 100 variables?;Maybe? I don't know. You still need to have some names. I think the question is referring to this part here. [image 1] So you need to know that 1 means “ok” and 2 means default. You need to know that 1 means “rent” and 2 means “owner,” “private,” and so on. So if you have this information somewhere, it can be JSON, or CSV file, whatever. So if you have some information somewhere, then you can just load this. This could be stored in a JSON file. And if you have that, then you can just use that. But again, you need to know that 1 stands for “rent,” 2 stands for “owner,” etc. If you don't care about this, machine learning. models don't care – they don't care if it's “rent,” “owner,” or if it's 1 or 2 – for this case, you can just turn this home variable into a string by using something like that. [image 2] Then it will stay encoded as a number, but it will be a string, and when you use a Dictionary Vectorizer it will treat it as a string, not as a number. I hope that answers the question.
Machine Learning Zoomcamp;2021;What are some of the most popular applications of logistic regression in data science?;One thing that immediately comes to mind for me is click prediction. Let's say you go to some website and there is a banner there. What happens behind this banner is that there is a model that predicts “What is the probability that this particular user that comes to this webpage (meaning you) will click on this banner?” Usually, there is some sort of linear logistic regression under the hood because they are pretty fast. For advertisements, they're used quite a lot, especially when we talk about real-time advertisements. When you enter a website and you immediately get an ad, this is where linear models such as logistic regression are used.
Data Engineering Zoomcamp;2022;Will the project be group or individual work?;"Alexey_x000D_
Individual. With groups, I think it becomes trickier._x000D_
Victoria_x000D_
Yes, but I guess you can always discuss it in Slack. Someone else could be using the same dataset._x000D_
Alexey_x000D_
Yeah, I think that becomes a group project in this case, right? If the dataset is the same, but you have different use cases at the end."
Data Engineering Zoomcamp;2022;What is the scenario for data engineering jobs in terms of backend development knowledge? How much backend development knowledge do data engineers need?;"Ankush_x000D_
This would, again, really depend upon the kind of job as a data engineer you will be doing. If you're doing an analytical data engineering job, or an MLOps machine learning data engineering job, backend development might not be a very interesting topic or the knowledge of backend development might not really help you. But if you're doing data engineering in terms of data products, maybe doing a lot of real-time streaming and working with a lot of microservices, then this becomes really important and really interesting, because now you can use or combine your knowledge of backend development and can now maybe use NoSQL technology to build a data product. You can use Kafka to do stream streaming into your data product. In those cases, the backend developer knowledge becomes really helpful and you can really excel in your job because of your background. On the other hand, I would also say that backend development knowledge, if you are learning in terms of SQL, and connecting sources or microservices, will help you in the future anyway, because that's the future we are going towards as a software development group. I think there's a saying, “No knowledge is lost.” So there will never be the case that you cannot use your backend development knowledge, but you would use it more in terms of if you are building a data product. _x000D_
Alexey_x000D_
Maybe then we should ask ourselves, “What actually is backend development knowledge?” Are we talking about general software engineering principles, or are we talking about how to use a specific microservice framework? Maybe you will not need this microservice framework for data engineering, maybe you will – who knows? You never know. But the general software engineering principles and practices apply for any engineering job. That will be useful, for sure. I think the skills of backend engineers will translate to data engineers, machine learning engineers – any sort of engineers."
Data Engineering Zoomcamp;2022;Can you explain how to apply/organize version control over all stored procedures, functions, views in existing databases, coupled with other code?;"Alexey_x000D_
I feel very sorry for you if you need to deal with that at work. [chuckles] I don't really have experience with that. _x000D_
Sejal  _x000D_
I used to when I was working with Oracle but it was like a long time ago. I’m assuming the person that wrote this question is still working with some legacy databases like Oracle. I really don't have an answer to this, either. [chuckles] I guess I'll just talk in more generic terms about CI/CD pipelines and wherever you're deploying your infrastructure, basically, or wherever you're deploying your schemas. I think that would be a better place to keep these things in._x000D_
Ankush_x000D_
Maybe using Git?_x000D_
Alexey_x000D_
I think the issue here is because the code is stored on the database itself, so you cannot easily version control it. Somebody, at some point, thought it was a good idea to let databases run some arbitrary code, like triggers?_x000D_
Sejal_x000D_
Possibly Victoria would be able to answer that in case there is any coupling of CI/CD pipelines with DBT pipelines? I don't know._x000D_
Alexey_x000D_
I think, usually what people do these days is try to extract this logic and put it into your backend – move this from the database and put it to the backend, where it can be version controlled."
Machine Learning Zoomcamp;2021;What is the reasoning of using AUC for comparing features rather than feature and target? Is there any mathematical reason/intuitive notion behind it?;"For this one, I think it's just a good way of checking feature importance. There is no mathematical/intuitive reason behind that. Let's say, before you even train your first model, you can do that and see if there are any features that already give you good separation between positive and negative classes. If you remember about the attrition of AUC interpretation, that AUC is the probability that a randomly selected positive example has a higher score than a randomly selected negative example. _x000D_
Here, the score can also mean a feature. Here, it’s if a randomly selected positive example has a value of this feature higher than the randomly selected negative example. If you think about this, then it probably makes sense to try this. Let's say, for seniority – it was a negative correlation, so maybe it's a little bit more complex. But you can already see what the important features are, and maybe some of these features are already good enough, so maybe you don't even need a model. You can just use this feature as your prediction, roll it out, and then in your second iteration, do a proper model. So that's why I think it's a good idea. And that's why I added this exercise."
Data Engineering Zoomcamp;2022;Is there any tutorial to install DBT locally with Anaconda? I found someone explained it in Slack, but it is still not working when I run DBT in it.;"Victoria_x000D_
I haven't used it with Anaconda. I think it's also in the documentation, but just install DBT locally in general, and if you're using Anaconda and you're using it in your environment, you can install it there. You can just use Homebrew if you're using a Mac, or you can also use pip install. Then you just do pip install DBT Postgres, for example, if you need that adapter or DBT Snowflake or DBT Big Query – whatever you want to use locally. And then that's it, you're installed. Then you have to set up the profiles.yml that show in the videos, create your project, or clone it or whatever, and that's it. That's all you need to use it locally. I never install it using Anaconda, but I'm guessing it's probably something similar. You probably do Conda Forge and install it like that, if it's possible. Otherwise, I would just go with the pip install._x000D_
Alexey_x000D_
In Anaconda, you usually have pip._x000D_
Victoria _x000D_
Yeah. So if you're in that environment you just install it like that and then that's it."
Data Engineering Zoomcamp;2022;I am trying to catch up and have almost finished the first week. What should I do?;"Alexey_x000D_
Well, good job. You can just keep working on that. You almost finished the first week, so you can finish it and then start with week two and then finish it. Then there will be week three. [chuckles] Just follow the sequence of videos in the playlist and in GitHub and that should be sufficient, I think. And if you have any questions, go to Slack_x000D_
Ankush  _x000D_
Just keep going. _x000D_
Alexey_x000D_
[chuckles] Yes, exactly. You’ve almost finished the first week, which is a good sign, because the first week in this year was pretty tough for many people. We had a lot of problems with Docker and whatnot. So if you're almost finished, this is a good sign. You’re on a good track. Keep working on this."
Data Engineering Zoomcamp;2022;Regarding topics not being covered in the course, what about Data Mesh?;"Victoria_x000D_
Data Mesh is one of the topics that we are going to cover. Those are going to be a smaller video. There's no practice around it or anything like that, like in the Weeks. It's more around explaining the concept, so you have a familiarity with it. Then you can deepen the concept if you need to, once you start working, let's say. I have Data Mesh, specifically, and I'll be recording that at the end of this week, once I'm back in Berlin. I'm not sure when we plan to upload all of them, but it should be soon enough – before the deadline._x000D_
Alexey_x000D_
I actually want to actually record one or more videos about Spark, which is something I planned like a month ago. I still haven't done that, so I want to do that this week probably, because I see that some of you have problems with reading from Google Cloud, or running things on Dataproc. So I want to do that first. After that, I will take care of the MLOps video. We will at least have two videos – Data Mesh and MLOps. I don't know what the rest are."
Machine Learning Zoomcamp;2021;Do deployed models work through APIs?;Yes they do.
Machine Learning Zoomcamp;2021;What algorithms/models are we allowed to use? For example, for gradient boosting, can we use LightGBM?;Yes, you can use whatever you want. Just be sure you document this.
Data Engineering Zoomcamp;2022;General architecture was shown last week. Could you go a bit into details about the technologies we are learning â€“ Docker and Terraform first of all?;"Alexey_x000D_
I'm not sure if the question is about the internals of these technologies. _x000D_
Sejal_x000D_
Maybe I can answer that. If I understand correctly, what they are talking about is the architecture that we have on the GitHub repository. I'm not sure if it represents Docker or Terraform. That is because Docker and Terraform are tools used in order to create your data engineering framework. But the actual data engineering framework – the architecture that you'll see on the GitHub repository – looking at that framework, the main technologies would be starting from this week (week 2) onwards, which is Airflow and Airflow for orchestration. Then there will also be DBT for transformations, there will be Spark for another kind of batch processing transformation-related job, and Kafka streaming. This is the core set of technologies generally used in data engineering and also what we will be covering in terms of the architecture that you’re talking about, if I understood your question correctly._x000D_
Alexey_x000D_
So what you're saying is, the foundational technology is there, but it's not visible? [Sejal agrees]."
Data Engineering Zoomcamp;2022;What is your decision on the idea of â€œTOP N Public Leaderboard?;"Alexey_x000D_
Let me show you what that is. Again, I will use another course as an example. So we will have something like that as well. We have this public leaderboard. Maybe we'll design it better, because right now it's just very boring – very textural. It's less than 100 people, but these are the people who wanted to share some information about them. So we'll have something like that. I don't know if the question was actually about that. I don't know what the question is actually asking, but this is the idea behind the public leaderboard and I think it will look something like that._x000D_
Victoria_x000D_
I also want to say something with the points – with homework or something like that?_x000D_
Alexey_x000D_
The one with the points and homework is anonymized. _x000D_
Victoria_x000D_
I also don't think we could disclose those._x000D_
Alexey_x000D_
Yes, exactly. So it will stay like that._x000D_
Victoria_x000D_
I mean, at least you kind of know, because you can see the hash. So you know who you are. You'll know if you're in first place._x000D_
Alexey_x000D_
But then it's up to you, if you want to go from this thing here, which is a hash, to this thing here, which is your name and your contact and whatever link you want to share. Because when I sent this email to 100 people, not everyone replied. Not everyone wants to be on this page, which I can totally understand._x000D_
Sejal_x000D_
Towards the end, are we going to, at least reveal the names of the top three people on the leaderboard or we are not going to declare that for the same reasons?_x000D_
Victoria_x000D_
It's just that we can't say any names, unless there's an approval from them somehow. Otherwise, it goes against GDPR._x000D_
Alexey_x000D_
We didn't ask for permission to share the names when we were collecting this data._x000D_
Sejal_x000D_
Okay, fair enough."
Data Engineering Zoomcamp;2022;What are your favorite non-common tools that you use for data engineering/analytics?;"Victoria_x000D_
I use all common tools, I guess. [chuckles] But we also use Etleap for the pipeline. Instead of doing Airflow and all that, we load the data basically into Snowflake from the S3 buckets with Etleap. It is not well-known. It's very, very small. It's not I wouldn't say it's my favorite at like ETL_x000D_
Alexey_x000D_
I think when I interviewed you about analytics engineering, you mentioned this tool. I remember trying to understand what you were saying and then map it to a tool. I think I spent, like, five minutes doing that. [chuckles]_x000D_
Victoria_x000D_
Yeah. In the end, it’s just an integration. It's not well known. I wouldn't say it's my favorite, but it's very easy to use. It does what it needs to do. In five minutes, you have a pipeline. It works. I would say that’s the most non-common tool that I've used. And I really liked MetaBase. I get to know [inaudible] to provide the local version too. I really liked that. I only get to know it for that. But I really liked the idea that you select something like a table and it gives you a lot of graphs and all of the things that I show a little bit in the videos. It was very easy to use. It has, of course, some limitations if you compare it with bigger ones like Looker, for example. It's clear that it's newer and all that, but it works very well. At least that was my small experience – not using it for work._x000D_
Alexey_x000D_
I’m trying to think of what would have been my favorite non-common tools that I use for data engineering and analytics. Frankly, I don’t know. [chuckles] The tools I use are pretty common. I don't think I have any favorite ones. We use Airflow. We use Spark. Pretty boring stuff._x000D_
Victoria_x000D_
Data quality, for example, I guess you get there when your team is more mature to actually have a tool outside of that. Data catalogs. There are some very good ones._x000D_
Alexey_x000D_
We use some in-house data catalog, so I haven't really checked. But having a data catalog is a good idea. The purpose for this is – for all the data sources, for all the data that flows through your data pipelines, you need some sort of recommendation. You need to know who is responsible for this dataset/data source or if there is any personal data there. We have an in-house tool, but I think there are many open source tools. There are many non-open source tools for that._x000D_
Victoria_x000D_
If you're interested in that, I would recommend checking out Amundsen. That's probably one of the first ones to explore. They have been in a bunch of conferences, in podcasts, and everything. So there are a lot of resources. That one is open source from Leap, if I remember correctly. But now, all the big ones have it. LinkedIn has a data catalog and all that. I think most of those are open source, so you can check them out as well. Now they even build data catalogs built on top of those._x000D_
Alexey_x000D_
You mean proprietary tools that are based on open source tools?_x000D_
Victoria_x000D_
Yeah. [chuckles]"
Data Engineering Zoomcamp;2022;What is the difference between the resource manager and master?;"Alexey_x000D_
A resource manager – let's say we have this YARN thing. YARN is yet another resource negotiator. Remember the picture [image] where we had the driver, and then we had a master, and then we had executors. What this resource manager does – and you can also think about Kubernetes, because Kubernetes also does that –what it's doing is locating new sources. You submit a Spark job and you say, “For this Spark job, I need executors with this amount of memory, with this amount of CPU, and I need 10 of them.” You submit this to your master and then they communicate with the resource manager and the resource manager provisions these executors. Before, YARN was mostly used for that. There is a thing called Mesos, I think, that works with Spark. Lately, I think Kubernetes is usually used for that. _x000D_
Ankush_x000D_
Let's assume you say “I want four worker nodes.” This resource manager would be responsible for spinning up these four worker nodes._x000D_
Alexey_x000D_
You have a bunch of machines, and then it will look at it like, “Okay, I have these machines. Let me allocate an executor node there. I’ll take a worker node there.” This is what YARN will do. I think with Spark running on YARN, your master also runs on YARN. A master is kind of doing both things. At the same time, a master is a thing to which you sent requests, and then it's closely connected to this resource manager, because this is the thing that provisions resources. Right? [Ankush agrees] I remember days when I would Google “YARN” and I would find things about Hadoop. _x000D_
Ankush_x000D_
You still find things about Hadoop. _x000D_
Alexey_x000D_
But where? It's almost at the end. And I guess it’s because I clicked on it a couple of times._x000D_
Sejal_x000D_
Yeah, I think the search is curated by user. So if you're a techie, then you would get technical definitions. _x000D_
Alexey_x000D_
Yeah, but this is a JavaScript package manager, which is more popular. “YARN is software packaging system.” You don't get the Hadoop YARN. I still don't know what the difference between NPM and YARN is. Maybe this is a topic for a different discussion. I think these days Kubernetes is getting more and more popular for this. At OLX we have YARN as well, but most of our workloads now run on Spark and Kubernetes. Same for you Ankush?_x000D_
Ankush_x000D_
Yeah. We use Flink, but we are also running it on Kubernetes. Kubernetes as a resource manager is pretty cool._x000D_
Alexey_x000D_
I remember attending a talk like three four years ago at Berlin Buzzwords. It was about running Spark on Kubernetes. It was so difficult. There were so many details. They ran into so many problems. Then they managed to run it at the end and I thought “Okay, it's not worth it.” I can just go to AWS, click on AWS EMR, and then I'll have a YARN cluster without worrying about all that. But now Kubernetes is becoming more and more popular._x000D_
Ankush_x000D_
Yeah, because everybody's using Kubernetes. Who wants to spend more hardware on running Spark jobs? _x000D_
Alexey_x000D_
Yeah. Because you already have Kubernetes, you don't want to have another resource manager nearby. With YARN, I think you also usually install the whole Hadoop thing – you will need HDFS there and all the stuff that you don't typically use these days. You keep your data in Cloud Storage, so you don't need all this HDFS kind of stuff. Do you still use HDFS, Ankush, Sejal?_x000D_
Ankush_x000D_
There was one company where I used it._x000D_
Sejal_x000D_
Yeah, me too. And that a long time ago._x000D_
Ankush_x000D_
Exactly._x000D_
Sejal_x000D_
Ever since AWS and S3 has come up, I think HDFS is more or less forgotten._x000D_
Alexey_x000D_
I think today, there was a talk from Indeed, which is a company for job searches. They have an on-premise Hadoop cluster and they run Spark on that. But I think this has becomes more and more rare these days. I think Criteo, the company we talked about it has this huge dataset, at some point were bragging that they have the largest Hadoop cluster in Europe. But I thought, “Okay, but is that really a good thing or not?” [chuckles]_x000D_
Sejal_x000D_
Must be an old enterprise company. _x000D_
Ankush_x000D_
Legacy. _x000D_
Alexey_x000D_
Yeah, probably._x000D_
Ankush_x000D_
I think SoundCloud also has that. I don’t know. I’ve heard that."
Data Engineering Zoomcamp;2022;Please recommend some books that we can read along with the course.;"Ankush_x000D_
I have one recommendation. I think Designing Data-Intensive Applications by Martin Kleppmann is definitely one of the books that every data engineer should read. Another one, which is my favorite, is Database Internals. That one is also really nice. But the first recommendation is definitely the top priority. _x000D_
Alexey_x000D_
I would like to mention that this Designing Data-Intensive Applications is a really great book, but it might look intimidating simply by looking at the sheer volume of the book. We do not expect you to read the book cover to cover, but this is something that you can buy and have as reference that you will go to throughout your data engineering career. Even if you're not a data engineer – I'm a data scientist and I find this book very useful. I keep it as a reference."
Data Engineering Zoomcamp;2022;Will the future iterations of the ML and DE Zoomcamps pick up the same videos, and the only difference being the homework?;"Alexey_x000D_
It's too early to talk about future iterations of Data Engineering Zoomcamp, I think. [chuckles] But for ML camp, yes – the videos will be the same. The only difference being the homework. That's still up to discussion, so I don't know. We'll see._x000D_
Ankush_x000D_
Yeah, I think we had a slight discussion about this when we met. We are not sure if we are going to repeat the ML and DE Zoomcamp. But let's see the response and the participation. If we feel that it was definitely a success, we will definitely do that. Feel free to pass around the course and give us some feedback so that we can maybe make a better decision._x000D_
Alexey_x000D_
Re-recording the videos… It's just too much effort. So I don't think I'll want to do this, let's say, for the ML Zoomcamp, for example. It just was too much effort and I'd rather reuse the same videos instead of making new ones. I hope the videos are good, so that if we decide to rerun it, we will not need to re-record them. But yeah, we'll see."
Machine Learning Zoomcamp;2021;What does it mean that using powers of two for batch size is easy for parallelization?;"Alexey_x000D_
I have no idea. Do you have any idea, Dmitry? You said that you split the data into two parts, right?_x000D_
Dmitry_x000D_
Yeah. As far as I understand, it can be used so that we can basically separate the set and run it on different machines. For example, if we have quite a big dataset, we can split it between the machines and run."
Data Engineering Zoomcamp;2022;Will we be randomly assigned to a few projects to review? How many points do we need to get in order to pass the project and get the certificate?;"Alexey_x000D_
Yes. For the previous question, I just showed the review assignment process. As for the passing grade, we will see about that. We'll take a look at this course. Because if we say right now, “Okay, in order to pass the course, you need to get 30 points.” We say that and only two or three people pass. This is not the situation that we want to have. [chuckles] We want to first look at the projects and then come up with a fair threshold to see that most of you actually pass."
Data Engineering Zoomcamp;2022;How does Data Vault differ from â€œclassicâ€ data warehouse concepts, like cubes, or star/Snowflake schemas?;"Victoria_x000D_
I bought a book about Data Vaults, and it's called The Elephant in the Fridge. I found it quite nice to read, but I didn't read it all. The reason for that is because what I saw with Data Vault is that there is no practical use, from my point of view, and for the work I do and all that. My understanding is that it generates an extra layer and then you have something else like a data mart. Right now, we have a warehouse, where we load the data, and then we transform it there, and then we build something like data marts or the star schema or something like that, depending on what you want to build, or on the data modeling concepts that you want to apply. All of this lies in our data warehouse. From what I understand from the Data Vault – and sorry, if I'm understanding this the wrong way – it generates an extra layer before and then you have something else, like another database or something, which is like the data marts. It tries to denormalize the data similar to what happens with the star schema. But the reason, or the solution they're looking for, is that the star schema is too difficult to understand. They first define the business need, and then they accommodate that. They have similar concepts to the star schema, but they accommodate that, and then they generate this extra layer. This is the main difference from what I understood. But probably go Google a little bit more. I also don't know companies that use it, to be honest. I've never met anyone that I could have a conversation on how this looks like in real life._x000D_
Alexey_x000D_
What exactly does vault mean? This is a place where you keep things, right?_x000D_
Victoria_x000D_
Yeah. It’s technically a data warehouse, but you cannot get out. [chuckles] You have bolted this extra layer where you have this data, and then you don't expose that data – it doesn't get out of the vault – except for those marts. Similar to the concept of data marts, they have it more on the business process group, but there’s this process. I can share the book that I read, but not much more than that._x000D_
Alexey_x000D_
It was something with elephants, right?_x000D_
Victoria _x000D_
The Elephant in the Fridge is what it’s called. The reason for that is because the fridge can be also called a vault. And it's quite nice. It's not from the creators of Data Vault, it’s from someone that actually explains what the creator did. I found it very useful. It’s very clearly written. But that's all."
Data Engineering Zoomcamp;2022;A compilation of useful and interesting links discussed in Slack and used personally would be insanely helpful.;"Alexey_x000D_
I wonder what we can do. Maybe we can just start a Google document and if you can help us actually come up with this list, that would be really helpful. Maybe we can then turn this Google document into a Wiki page on GitHub or something like this. I guess we can just start a Google document and then see if it gets any traction – if people start putting links there. If it happens, then you will have it."
Data Engineering Zoomcamp;2022;Do you provide a standard solution for the final project like you're doing for weekly homeworks?;"Alexey_x000D_
No, I don't think so. Some of these projects that you will work on will become standard examples for the future generations. You have the honor of being the first one and making it easier for the future generations. We still haven't decided if we want to do it one more time. But if we decide that, then it will be simpler for them._x000D_
Ankush_x000D_
Even if we don't decide to do that, the videos are out there."
Machine Learning Zoomcamp;2021;I submitted the homework and sent the link to my GitHub. Is that okay?;Yeah, I guess it is okay.
Machine Learning Zoomcamp;2021;Is it too late to start with the project today?;I think it is. You have one day and a half. You can try. Actually, I just want to remind everyone that we will have a third project, which is optional. If you're behind with this midterm project, what you can do is keep working on this project now, at your own pace, without worrying about deadlines, and when the time comes to submit the third project, you just take the project and submit it as your third project. This way, you will be able to still submit something even if you had to skip a week for some reason. Keep working on your project. You can start today. You can keep working on this and then you can submit it as your third project. That will be fine.
Data Engineering Zoomcamp;2022;Iâ€™m really not able to understand the week 2 homework. Please explain in detail. Thank you.;"Alexey_x000D_
I'm afraid if I go over it again, it will be the same words. I'm not sure what else I can add here. If you want specific instructions, let me do this. So this in the week 2 folder, in Airflow, there is this data ingestion DAG, which you saw in the video. You will actually need to take that and modify it in order to turn this into a DAG that runs for this yellow taxi data and puts this data to Google Cloud Storage. _x000D_
You will use this as your base and then on top of that you will work on this, change and modify this code, which will copy all this data to Google Cloud Storage. Another thing that will also be quite helpful for you is this local DAGs folder. This data ingestion local DAG is the code from that long video that actually shows you how you can parameterize your jobs. _x000D_
When I was solving the homework, what I did was take that script for GCP and then I took some things from here and then I merged them. As a result, I got the solution for the first two questions. I think once you do this, you will figure out how exactly you should approach the remaining ones. I hope I didn't give away too much and didn't spoil the fun of figuring this out yourself._x000D_
Ankush_x000D_
I think that was a really good explanation. Again, but if you still have questions, feel free to use the Slack channel. I guess that would be the best place because that’s where you can actually put specific examples you're struggling with, or specific errors that you are seeing and I'm pretty sure the community can help you there."
Machine Learning Zoomcamp;2021;Can you explain functional-style coding and how it's different from object-oriented programming? Why not use a sequential object for building the network?;"I think I covered that more or less. Here, “functional-style coding and how it's different from object-oriented programming” – I'm not sure if it's related to object-oriented programming or not, but the functional-style here, in this sense, is that you think in terms of functions. Let's say you have a function and the function has some input and then the function produces some output. In this case, the function can be something like a Keras layer dense of size 10 “DENSE(10)”. This is our function. The input of this function could be an array of dimensionality 183 and the output would be like something of size 10. It kind of forces you to think about “What is the input? What is the output?” And then each time you put together multiple functions, you have sort of a chain of transformations. _x000D_
You have one thing, then another thing, then even a smaller thing. You have this chain. Of course, you can have this chain with the sequential way of building models. They're quite similar. I don't think the “sequential” has anything to do with object-oriented programming in this particular case. You can also think of sequential as – you take something, this is the first operation you apply, this is the second operation you apply on the first function – F1, F2. You just don't explicitly say what the input is or what the output is. It's happening under the hood in the sequential model. [image 3] You don't need to define it, but it's defined internally."
Machine Learning Zoomcamp;2021;Will there be another session starting soon?;"You mean another iteration of the course? Yes. If you go to our course repository, you will see the answer there. First of all, you can take the course in the self-paced mode. All the videos are available, except for the last few videos from Kserve. You can start watching them. It's important that you first attempt the homework without looking at the answer. So do that and then check the solutions. And then do at least one project. That's also important, because if you don't do this (if you just follow the videos) I am sure you will learn something, but it's better if you solidify your learning by also practicing. Do at least one project if you're taking this course in self-paced mode. If you want to get some feedback on your project, you can also share a link in Slack and we will be happy to give you feedback on that project. _x000D_
To answer your actual question regarding the next cohort – it will start in September. This cohort started in September. I will probably start it a bit earlier. We started in the middle of September this time, so we will probably start maybe one week earlier. If you want to be notified about this, to be informed about this, there is a link to the form. Click on this, put your email here, and I will send you an email when the course starts."
Data Engineering Zoomcamp;2022;Even with extra jars needed, I could not write into tables, but only read and process.;"Alexey_x000D_
I think I saw this thing a while back when we were talking about Dataproc and the BigQuery Spark connector, probably. Unfortunately, I haven't used this tool. If you use it from Dataproc, I don't know what the reason could be. But if you're using it from your VM, the reason could be that your VM maybe does not have permissions to write to BigQuery, so you will need to check how to configure this connector in order to be able to use the key. I suspect that this could be the problem. Also make sure that the role you use for accessing Google Cloud Storage and so on, can actually write to BigQuery. I think it's “BigQuery admin” or something like this. So make sure it has that permission. _x000D_
It's called a “service account,” I think. So the service account should have the right permissions. In case you created a separate service account, make sure the permission is there. Unfortunately, I do not have a working example that I can share right now. But this is something I want to check this weekend and maybe record a video, if I have time. Please feel free to share this in Slack. I think some of the other students can help with that as well, because I think some of you managed to make it work. Some of you used Spark on GCP, so please share this problem in Slack and then, perhaps somebody who solved this problem already can help you."
Data Engineering Zoomcamp;2022;Can only the 2019 and 2020 files be transferred using transfer service or should the entire bucket be transferred?;"Alexey_x000D_
Ankush, the way you did it, is you just stopped it. You started the transfer using a transfer service and then you stopped it when it transferred this data, right?_x000D_
Ankush_x000D_
It will automatically stop. But you can also use prefixes. There are some options to use prefixes and filters. And you can definitely try to use them."
Machine Learning Zoomcamp;2021;Could you update the current leader board results?;Yes, I could. As soon as I can, I will do this and I will post a message in Slack. I just need to find time to do a couple of tweaks in the script.
Data Engineering Zoomcamp;2022;I'm storing data from a database into a data warehouse. How do I put it into a pipeline if the data in the database may change in the future?;"Victoria_x000D_
Not sure if I understand them, because you're taking it from a CSV, not really a database, right?_x000D_
Alexey_x000D_
Yeah, I think this question is more general and not related to homework, if I understand correctly. Let's say you have some database in ecommerce (it could be about orders or something like this) and this data is constantly changing. For example, today, the price is this – tomorrow, the price is that. It changes. The question is, how do you put this in your data warehouse if this can change? Do you know how to do this?_x000D_
Victoria_x000D_
I don't know if the question is targeting a schema evolution, because your example was more about the same data changes, right? In that case, you would have a primary key and then you would have to do some kind of deduplication, because I assume you'll have the original record and then a modified record. _x000D_
When you're modeling the data, you will have to do some kind of deduplication to always take the last record on a primary key, let's say “order number” in that case. As for a schema evolution, if that's where the question is going, then that will be a little bit more complex, because you'll have to somehow adapt your pipeline. Or if you're using ETL as a service tools like Fivetran (there were some questions about that as well) some of those tools also adapt to schema evolution._x000D_
Alexey_x000D_
Another thing to add to that is – let's say you have a database with some records. This can be item ID, item name, price, and let's say the price changes. What you capture is this change. You have a new version, so you know that some record in a database was updated, and you save this change to the data warehouse. Therefore, every time a record changes, you save it to your data warehouse, and you have a bunch of rows that change. _x000D_
Basically, you capture every time something changes, you save it in a data warehouse, and then you can sort of travel back in time – you can see what the state was for this record on this particular date. I'm not sure though, if it's specific for a data warehouse – but at least this is how we do this. Every time the record changes, it will capture this change somehow and in our SQL query, we can say that “We want to see this date at this timestamp.” And we will see the latest price, for example. _x000D_
As for how to implement this, I actually have no idea. Maybe Fivetran can do this. For all the databases – let's say if you use MySQL – there is something that can send these data source changes or something like this. For Postgres, there is probably something similar._x000D_
Victoria_x000D_
Yeah, you'll implement a pipeline that is basically logging what's going on. Even though in the original database, maybe if it's an order (like in this case) it will update, but that will generate a second entry, like Alexey mentioned, because you're logging. With Fivetran, you could do it as well, of course._x000D_
Alexey_x000D_
Fivetran is a low-code thing. You just say, “Okay. This is my source, this is my destination,” and then it does the thing, right? It just moves data from one location to another. Is this how it works?_x000D_
Victoria_x000D_
Yeah, more or less. [chuckles] You set up a connection to a S3 bucket or whatever you want – they have several adapters. You can set run rule scripts and then just set a destination."
Data Engineering Zoomcamp;2022;What do you think about Scala? Is it important for data engineers to know this language?;"Alexey_x000D_
Ankush, you like Scala, right? [chuckles] _x000D_
Ankush_x000D_
I think Scala is definitely one of the best languages out there for data engineers. [chuckles] The biggest part is that it's typesafe and it's not as heavy or as verbose as Java. Scala has a perfect combination, at least for me, coming from Java background, I love Scala. I would say, yes, it's kind of an important language to know as a data engineer, especially because Spark is written in Scala. Sometimes when you see errors, which are very Spark- or Java-native, you might get confused if you do not have any knowledge. So I like Scala, personally. I use it for developing some pipelines. I also use it to write code in Spark and sometimes also in Beam. But do you actually need it? No, not really. If you know Python well enough and you are very, very proficient in Python, you can get away with minimum knowledge of Java and Scala, I would say. Unfortunately. I would say I like Scala personally. So if you like it, definitely go ahead and learn it. It's still a good language._x000D_
Alexey_x000D_
But I think you get some performance benefits because Python is an extra layer of abstraction on top of Spark, which adds a bit of overhead. There are some cases when you want to have more control over things. Right?_x000D_
Ankush _x000D_
In my personal experience, whenever we have written batch jobs – I'm not talking about real-time, I'm talking about batch jobs, where it doesn't matter how much time it takes – it was always more than half an hour, 10-15 minutes minimum. So adding another 30 seconds or one minute because of the Python performance doesn't really matter. In real-time streaming, yes, this would be a huge loss. I don't think that's the case with PySpark in real-time, but in batch, it doesn't really matter. _x000D_
Alexey_x000D_
I think serialization is better when you use Scala, because you can use these “case classes” or whatever they are called. Then you can easily turn your RDD to this case class. I think this uses some very efficient serialization mechanism, which you do not have in Python. Sometimes it can be just a difference of minutes, but I remember we had a case when the difference was like 20 minutes. We took a pipeline and rewrote it in Scala, the difference was big._x000D_
Ankush_x000D_
I would say that was Spark 1.0-something because after all data frames, that's also not really useful anymore._x000D_
Alexey_x000D_
It was with data frames. Yeah, it was 1.2. I remember that. We used Python in every step of our pipeline and when we found this one job that we could optimize with Scala, the whole pipeline was a lot faster. But it was something where I needed Scala only once in a year of using Spark. [chuckles] What do you think is more important – to know Java or Scala for a data engineer?_x000D_
Ankush_x000D_
If you are very good at Python, then you would use it once. If you already know Java, you don’t need Scala. If you don't know either, then learn Scala, because Scala would be much easier to learn in this application, especially for Spark._x000D_
Alexey_x000D_
I will not argue with that because we don't have a lot of time. [chuckles] [cross-talk] _x000D_
Ankush_x000D_
I have my own preferences. I am biased here, because I really like Scala. My answers are not neutral. They are definitely biased towards Scala._x000D_
Alexey_x000D_
Victoria, you didn't want to add anything, did you?_x000D_
Victoria_x000D_
No. I don't know. I haven't used Scala in several years. For some reason, people offer me jobs in Java development on LinkedIn. [chuckles] _x000D_
Alexey_x000D_
Do you use it?_x000D_
Victoria_x000D_
No, no. I used it at university because I had to prove it to get the title. I would not use it again."
Machine Learning Zoomcamp;2021;I am facing problems in data cleaning. What techniques are good for data cleaning if we have large amounts of data?;"Lisa_x000D_
There are all kinds of things. Do you have missing data? Is it text data? Sometimes you need to do some cleaning, like if street is written as “st” and “street,” so you might want to combine them. If there's missing data, you might want to take an average or median. If there’s outliers, depending if it makes sense or not, you might want to throw that out. Say Elon Musk's salary is in your dataset, that might not be a good model to go on. [chuckles]_x000D_
Alexey_x000D_
I think his salary is zero, right? He wrote recently in his tweet that he doesn't have any salary. He only has shares. [Lisa agrees, laughs] He would be an outlier, but in a different way._x000D_
Lisa_x000D_
In the other direction, actually. [laughs] So things like that, you would want to look out for. You might use inner core tiles and cut off the tops and the bottoms. There are a lot of different techniques you can use._x000D_
Alexey_x000D_
Data cleaning is very broad and also very abstract. Unfortunately, it's very manual. There is no magic button like in DataRobot. It would be nice if you give it a bunch of dirty data and there is this magic wand button, you click on this, and then your data becomes clean. That would be a nice DataRobot. Maybe you can have two buttons – “Make my data nice, "" and “Make my data clean.” And then “Okay, now find the best model with this data.” Maybe there is some research in automated data cleaning as well. There must be something._x000D_
Lisa_x000D_
There are some companies out there. Some of the companies have been buying each other up and consolidating, so that the trend seems to be end-to-end. Also deploying the models and machine learning ops to make sure you don't have data drift and things like that. But it's not always as robust as you might want for your particular situation, because they try to generalize when they build the tools."
Machine Learning Zoomcamp;2021;Do you know any free ways to deploy models?;I just showed you PythonAnywhere. And Heroku is also free. For AWS, you can just create a new account and get another free year. For that maybe you’ll need a different credit card. But I don't know.
Data Engineering Zoomcamp;2022;Did you all know how awesome you are for hosting this course?;"Alexey_x000D_
I did know. Did you know, Ankush?_x000D_
Ankush_x000D_
No, I didn't. But I definitely know now. Thank you so much for this comment. _x000D_
Alexey_x000D_
Yeah, thanks. We also appreciate that you're taking this course – that you're spending time on taking this course and for all your feedback that you give us. Thanks a lot. _x000D_
Ankush_x000D_
Yeah, definitely. I think the feedback is really important. Please keep it coming so that we can improve the course. Maybe in the future, we can have even better videos and courses."
Machine Learning Zoomcamp;2021;In my project, I got 0.9963 roc_auc_score for train, and 0.9303 for validation. Is that overfitting a bit much?;I wouldn't look at train at all, to be honest. Sometimes I do look, but I think the most important thing is the validation result. 93% for validation seems high, but the only way to check if you're overfitting or not is to use the test dataset that you have. After tuning the model, after selecting the best one, use the test dataset. And if you see that the number is close to that, then no, you're not overfitting. If you see 70% there, then you probably accidentally overfit for some reason. I don't know why. Maybe for you, it will make sense to choose a different split or try to figure out what the problem is.
Machine Learning Zoomcamp;2021;Andrew Ngâ€™s course does not use Python. Do you still recommend taking it if ML mainly works with Python?;I took this course a while ago. Back then it was useful for me. I do think that doing something where you use Python immediately could be more helpful. I'm just not sure what would be a good recommendation for that. The current course, of course, is good. But I wouldn't compare it with the Stanford course from Andrew Ng, simply because they are very different. In that course, you'll get a lot more theory. This one is very practical, very hands-on – it's mostly writing code together. Maybe you can just take both.
Machine Learning Zoomcamp;2021;If features correlate highly with labels, is it recommended to drop them in order to avoid leakage? If dropped, is there any risk of losing relevant features?;It's really case-dependent. I saw in Slack, if I remember correctly, Carlos asked if he needs to drop features or not – there it seemed okay not to drop. The highly correlated feature was the assistant name and the target variable was churn. Basically, people who work with one particular person tend to turn less than people who work with somebody else. I think in this case, it seems like a relevant feature to include. I don't know – it's really case-dependent. Usually, you need to have some sort of domain expertise to decide if this feature introduces leakage or not. In Carlos’ case, I think it's safe to leave it. If you drop, you can just see how your performance and validation dataset changes and this gives you some idea how important this feature is.
Data Engineering Zoomcamp;2022;In this weekâ€™s prerequisites, you recommended â€œsetting your memory for your Docker Engine to minimum 5 GB.â€ So where do we do this? My Airflow webserver is not going up.;"Sejal_x000D_
I'm not sure if you've seen the video, but I've already explained this. If you use a Docker setup, then you can actually change your configuration to increase your memory. If you're not using a Docker setup, then there are special instructions, especially for Linux VMs, that we can update in the FAQ in order to be able to do that. _x000D_
Alternatively, if you maxed out your memory usage, that is, let's say if your machine only provides up to 4 GB and you cannot extend it further, then I suggest using the no frills version or using a cloud VM to be able to use this. Then you won't have to deal with manually configuring these things. _x000D_
Alexey_x000D_
It's controlled by how much resources this WSL 2 resources the backend has. It says “resources limits are managed by Windows” and you can configure that. On Windows, I didn't have problems with that. I think the way it works on Ubuntu is that Docker can get everything that you have on your host machine. I think on Windows WSL 2 it’s similar. And in MacOS, I saw how to do this in the videos. Also my computer doesn't like when I do Docker compose up. It’s a bit heavy._x000D_
Sejal_x000D_
I read somewhere that Docker Compose does not have that level of maturity and compatibility with Windows or WSL. I think the GitHub issues link that I sent you yesterday also had explicitly stated that._x000D_
Alexey_x000D_
It’s those Windows haters that write that. [Sejal laughs]They made fake accounts. [laughs] _x000D_
Ankush_x000D_
I am definitely one. _x000D_
Alexey_x000D_
[laughs] Well, maybe those aren’t fake accounts."
Data Engineering Zoomcamp;2022;What will be the format of submissions for the week 2 homework?;"Alexey_x000D_
We haven't created a form yet, but it will be the same as week one. There will be a Google Form, we will choose questions and we will have to trust you that you actually do the homework you code, because you can guess some of the answers without doing the work. But as we said, if you do this, you will not be able to work in week 3. So you better do that properly and maybe I'll include a checkbox in the form that you pinky-swear that you did the code. Of course, you have to include the link to the code. So if we have time, we may randomly pick a few submissions and check it. So you should do the code."
Data Engineering Zoomcamp;2022;What is the main challenge for data pipelines and what do you advise for companies that want to build a first-time data architecture/infrastructure?;"Ankush_x000D_
If you're starting, I would suggest you keep your basics correct. But try to keep it as simple as possible. The more you complicate the data pipeline, the harder it will be. Let's rephrase, and maybe talk about the basics, first of all. Use Terraform, use Docker, use Spark, for example, or any other technology that can scale. Don't use Hadoop – use S3 or Google Cloud Storage, if you're already on the cloud services. Try to use the services already provided by your cloud provider. For example, in AWS, there can be EMR (Elastic MapReduce) which can deploy your Spark jobs. In Google Cloud services, there is Dataproc, Dataflow. There is also an Airflow cloud composer, which is called Cloud Composer | Google Cloud. There is BigQuery. So try to use them as much as possible, but try to keep everything (the whole data pipeline) as simple as possible. The less steps you're going to have, the easier it's going to be. Only introduce another step or another landing zone or something like that if you really need to. The more simple you keep – it's easier to maintain, it's easier to change, and obviously we'll change it. Obviously, it will get complicated when you grow and have other use cases. But try to keep the basics right and in the beginning, just start with the simple use cases._x000D_
Sejal_x000D_
I would say the crux of a data pipeline is in the end, and at an automatic level and ETL pipeline. The basic steps are: extract, transform, load, or if you're using the modern version, it will be ELT, which is: extract, load, and transform. So just focus on building those core components before you add more complexity around those areas and how you can automate it and things like that. Just like what Ankush said."
Machine Learning Zoomcamp;2021;But how would we know that we have to open model2.bin when we don't know it exists?;I think I mentioned it in the homework. You saw how I created the Docker file. This is how you see that it exists.
Machine Learning Zoomcamp;2021;When deploying, is machine learning expected only to predict or also to automate decision-making?;Ideally, you just need to think about the users of your application. Do they want you to just predict or do they want you to make a decision? I think usually, it's the latter – the users of your application are not interested only in raw predictions. Let's take a look at this example. [image 1] Here, we explicitly say what the decision is. We say the user will churn through, “Okay, now send an email campaign to this user.” If we just output probability here, then maybe the users of the service would be like, “Okay, it's 72% probability of churning. What do I do with this?” That's why I think it's a good idea to actually automate some decision-making to basically make a decision for the user.
Machine Learning Zoomcamp;2021;Can we host ML models on GitHub Pages?;I don't think you can because you actually need a web server. GitHub Pages can only serve static pages like HTML. Unfortunately, you cannot host machine learning models on GitHub Pages.
Data Engineering Zoomcamp;2022;In your opinion, which topic requires the most effort to understand properly for someone brand new to data engineering?;"Ankush_x000D_
That really depends upon your interest and what kind of knowledge you already have. For example, if you have a lot of SQL knowledge, then understanding DBT would be super easy for you. Maybe Spark and Kafka streaming would be something hard. If you're coming more from a software background, if you've been doing software development, especially backend engineering for some time, and you have been working with distributed databases or with some sort of streaming, then switching over to Kafka and Spark would be relatively easier. So it really depends upon your background. But I would say, generally, all the topics are relatively intermediate. I'm pretty sure in the videos, we will be able to explain to you in a way that you understand all of them. _x000D_
Alexey_x000D_
Maybe I would add that the internals of databases is a pretty complex topic. We will not cover that in detail. But again, these books that we suggested – they are not easy, but they are interesting. For me, for example, Martin Kleppmann's book, I need to reread some of the chapters. It was worth it. It wasn't easy, but it was worth it._x000D_
Ankush_x000D_
Definitely. I think I finished the whole book in a couple of years. Basically, I picked up the topics that were interesting for me, and then went back and did the other parts later on."
Data Engineering Zoomcamp;2022;Could you do a high level overview of the week 2 homework and what main points we should be getting out of this?;"Alexey_x000D_
Yes. That's the homework and the first two questions are about yellow taxi data. You will need to write a DAG for putting this to Google Cloud Storage for years 2019 and 2020, for yellow taxi. For this week, in the video about Airflow, you get some code that you can use as a starter for putting files to Google Cloud Storage. You will need to change them a little bit, or maybe not a little bit, but you will need to change them. The instructions “Re-running the DAGs for past dates” tells you what exactly you will need to do in order to re-execute this. _x000D_
You will need to set up the catch up parameter to true. Then you should be careful with running many jobs in parallel. Some students reported that they had problems with this. Their computer froze when they tried to execute many jobs in parallel, even the virtual machine in the cloud also had problems with that. So don't do that. And then, this is an important thing, you need to rename your DAG. If you don't, Airflow will not see that it changed. _x000D_
Let's say, depending on your answer here – if the answer is different from what is currently there, then you will need to do a rename in order for Airflow to pick this up. Also because we changed the start date, Airflow will also need to rediscover that. That's why you want to change the name. There is one extra thing that you might need to do. Because these files are quite large, it will be helpful to manually remove them so they don't take a lot of space. _x000D_
Then question 3 is about for-hire vehicles. You will just need to do the same thing as you did for the yellow taxis, but for this dataset. Then question 4 is the same thing but for zones. Eventually there will be a form for submitting solutions and you can use that forum to submit. It will be the same thing as week 1. _x000D_
I think that's a good high-level overview. The main point here is that you will learn how to create a DAG, you will learn how to move data from this web to your Google Cloud Storage, and then the other thing is that you will have data for week 3."
Data Engineering Zoomcamp;2022;What do you all enjoy and do outside of work and data?;"Sejal_x000D_
I like enjoying the sun after work. It's been consistently sunny for the last couple of weeks, so that's really nice for Berlin. At least as per the Berlin weather._x000D_
Victoria_x000D_
Yeah, definitely enjoying the sun, going to park, being a tourist, also – even in Berlin, or in Argentina. Reading, being with friends. I think I'm pretty normal. [chuckles] A bit boring_x000D_
Alexey_x000D_
I like Lego. My son and I make these things from Lego. It's something I quite enjoy doing. I don't know why – it has this meditative effect when you have like 3000 pieces and you need to build something from this. This is pretty fun. [chuckles]_x000D_
Sejal_x000D_
I'm inclined to get a jigsaw puzzle for the same reasons. Because at times, I feel like I don't know what to do with myself. It would be nice to do something where I'm building – not code or technical projects, but just something fun for a change. [chuckles]_x000D_
Alexey_x000D_
What is a jigsaw puzzle?_x000D_
Victoria_x000D_
It's just a puzzle. I have several as well. I can lend you some. Because after you've done it once, then the second time is kind of boring._x000D_
Alexey_x000D_
Yeah, I saw this one. But the one I saw was just a white thing, and you need to figure out how to build it. [chuckles] But that's probably very cruel, and this is for people who are already good with solving the usual ones. Yes, I guess that's all we do. Well, I guess that's not the only thing I do, but..._x000D_
Sejal_x000D_
Yeah, we know you like building things – building communities, building projects._x000D_
Alexey_x000D_
But I don't know if it counts us “outside of data or work”. Does it? I mean, outside of work, yes. But outside of data? Not sure._x000D_
Victoria_x000D_
I think it counts. It counts as a hobby. _x000D_
Alexey_x000D_
Okay. [chuckles]"
Machine Learning Zoomcamp;2021;My Flask app is showing an error. How many marks will you deduct for this? All my things are correct but Flask app is showing an error in output.;"Does your flask app produce a result? Does it actually return a prediction? If it does, then you're good. If it doesn't, then you need to fix that. To answer your question regarding how many points will be deducted. I think the whole project kind of assumes that you managed to deploy it with Flask, because if you don't, then you will not be able to put it in Docker. You will not be able to deploy it in the cloud and then you will lose like five or six points or something along those lines. So I think it's a good idea to make sure that your Flask returns something. _x000D_
If you have problems with that, please share your code. I think I know who is asking that question, because we talked about that already in Slack. Maybe there is an error somewhere in your predict.py script. You just need to make sure that you fix this error and then it should work. Perhaps what you can do is just take the code from the lectures that we did – I think you have the XGBoost model there and not logistic regression. You will need to put your x variable into a t-matrix from XGBoost, and then instead of using predict_proba, you will need to use predict. And I think that's it. Do that – if it doesn't work, please write in Slack and let's try to figure out what's not working."
Machine Learning Zoomcamp;2021;I'm struggling to run python script with TensorFlow and TensorFlow Lite runtime on WSL with Ubuntu 18.04 with Python 3.8. Any leads?;What I would suggest you do is try Python 3.7 on this Ubuntu. Either upgrade Ubuntu to 20.04 or downgrade your Python to 3.7. On my laptop, I have the exact same setup and Python 3.8 doesn't work. Or run this in Docker. If you run this in Docker, you won't have any problems. But if you really want to run it on your host machine with this Ubuntu and this Python, you have to either upgrade Ubuntu or downgrade Python.
Data Engineering Zoomcamp;2022;How do you think the skills that analytics engineers need are useful for data engineers? For example, knowing DBT? Is it quite useful for data engineers or maybe it's not their main focus?;"Victoria_x000D_
I would say it's not the main focus for us. For example, at our company the data engineers don't really use DBT. Everyone in the office has access, but it's something that the analytics engineer will do. Then the data engineer mainly focuses on what happens before the warehouse. _x000D_
Right now, with Kafka, S3 buckets to pipelines and all of this. But I think it's still good to know what's going on because where is your data going as a data engineer, let's say. Then your stakeholder happens to be the analytics engineer so I think it's good knowledge to know what is actually happening there._x000D_
Alexey_x000D_
The reason I mentioned Ankush is because one year ago or maybe more, when we first met, he was still working at a different company as a data engineer. He mentioned this tool, DBT and I had no idea what this tool was, so I looked it up. He told me that this is something data engineers use in that company. This was the first time I heard about this. Apparently, some data engineers also need to use this tool sometimes._x000D_
Victoria_x000D_
If you think about it, many companies don't have analytics engineers. It's quite a new role. Either the data engineer has to pick it up or the data analyst and I'd say that it makes more sense that the data engineer picks it up. But I think companies that use DBT are already quite familiar with the role of the analytics engineer so it makes more sense that that would be the person that would take over DBT._x000D_
Alexey_x000D_
And I think I’ve asked you that in our podcast, about analytics engineering, but some data engineers transition to analytics engineering sometimes, right?_x000D_
Victoria_x000D_
Yeah. The analytics engineer is a person that up to a year ago was doing something else – either data engineering or data analysis. Then you just either go get a little bit more technical or go get a little bit more business focus. So, yes."
Machine Learning Zoomcamp;2021;Do you have separate materials or videos on setting up the environment in WSL and VSC?;I do not have anything like that, but feel free to ask anything in Slack. I don't think I did anything special there. A lot of links were already shared in Slack, like how to set up the environment in VCL. So maybe you can also do a search – just go to Slack and look up WSL and you'll see some links there.
Data Engineering Zoomcamp;2022;If you've completed the entire course and did all the homework, is there any way you can get the certificate if you're not interested in the project?;"Alexey_x000D_
The way we thought about this is – just watching the course is not enough. Maybe you watch a course and you feel like you know everything, but when you try to do something, you realize that it's actually not as easy as you thought. We want you to have this experience. We want you to sit down and try to apply all this knowledge that you picked up. That's why we decided that the only way you can get a certificate at the end is if you did the project. That's the purpose. It's not enough just to watch it. You need to do these things yourself. So the answer to this is no, there is no way. You need to do a project if you're interested in a certificate."
Data Engineering Zoomcamp;2022;In both your and Victoriaâ€™s experience, how much has the environment of data engineering changed? It feels like there have been many changes/tools in a short amount of time.;"Victoria_x000D_
I can actually tell a funny anecdote. I built a program for my parents with Java and it had a database. It was a MySQL database. Yesterday, I was trying to fix something and I checked everything. I tried to use the workbench [inaudible] old version in 2014. It was very hard for me to use it. I was like, “Oh my God! How well things have improved over all these years that now I find it difficult to do this compared to when I was still studying.” I didn't have much experience or anything. I would say definitely tools have made things very, very easy. The fact that you can do a pipeline in five minutes with a tool, like with Leap or with Airflow, instead of having to do [inaudible]._x000D_
Alexey_x000D_
But on the other hand, MySQL is still MySQL, right? It was 10 years ago, so some things changed but some things still stay the same. There are things that have changed, indeed, but there are some fundamental things like Postgres, which I don't think changed significantly. When I started with Postgres – when I first used it – it was a usual desktop app, a pgAdmin. Now it’s a web app, so now it's different. But it kind of still has the same look and feel. Previously, you needed to install Postgres. Now we can just run it from Docker, which is helpful, at least for me. I know many of you had problems with running it from Docker. But I remember how difficult it is first to install Postgres and then figure out how to actually delete it when you no longer need it. Because it runs as a service and then it's hard to kill. I remember that it gave me some problems. Now with Docker, you just start it (provided that it works) and when you no longer need it, you just forget about this. That’s pretty cool._x000D_
Victoria_x000D_
Before (I'm not sure if you used the same thing) I used to use VirtualBox a lot as well. So Docker has definitely been a game changer. I remember for my thesis, I had to do a distributed system and I had like five year VirtualBoxes running in my machine to fake those five servers. Now you can do it all on the web with cloud warehouses. Because there was no such thing as free trials and things like that._x000D_
Alexey_x000D_
With Docker Compose, you can just start five Docker containers. I think now, as you mentioned, the Etleap tools – they're like no-code tools, almost. You can connect things with arrows. You just say “Okay, there is your data source, there is the destination.” You connect them and it works. This is also cool. _x000D_
Victoria_x000D_
Now you have way more options, which can be very overwhelming because a few years ago your data warehouse was probably one of the big ones, if you were in a company, like Oracle or SQL Server. But right now, there are a lot of options. There's a lot of competition, so the technology improves very fast. But there are also easier things to do. You can still code, but you can also do no-code and still get to have the parts you need._x000D_
Alexey_x000D_
If you're worried about learning something but a new tool appears – fundamentally, things haven't changed that much. We still have ETL processes, but instead of using tools like Informatica, we now use something else for that."
Machine Learning Zoomcamp;2021;For rating data (number of stars) can we use a linear regression model?;Yes, you can.
Data Engineering Zoomcamp;2022;Can Jenkins or GitLab be used instead of Airflow?;"Victoria_x000D_
No. _x000D_
Alexey_x000D_
To some extent, you can. I think they are more like general purpose CI/CD tools. You don't use them for data workflows. You can, in principle. But things like Airflow and other workflow engines are tailored to a specific use case. You have all these nice things there._x000D_
Victoria_x000D_
Definitely for a different use case. For me, they overlap, but they fit different use cases._x000D_
Alexey_x000D_
I must admit that we do use Jenkins for that at my work. But don't do that. [chuckles] Please don't do that. You will regret it. [chuckles] _x000D_
Victoria_x000D_
You said before that I was legacy, so…_x000D_
Alexey_x000D_
Yeah, it is still legacy. We just need to maintain it. Nobody wants to touch it, because you don't know what happens and nobody wants to spend time moving it elsewhere. So it just kind of runs there. I personally wouldn't come anywhere near Jenkins. But if you like Jenkins, why not?"
Machine Learning Zoomcamp;2021;I may not be able to submit this time. Can I see some other people's work to learn what they did in their projects?;I think many people actually shared their midterm projects on LinkedIn and Twitter. Perhaps you can follow the #mlzoomcamp hashtag to see what kind of projects there are. I think it's also a good idea. After this midterm project is over and we do peer reviewing, we will just ask everyone who wants to share their work – they will just share and you will be able to look at it. I don't really want to share the form because maybe some people don't want to share their work with everyone. But if somebody wants to share, you can just post the links to Slack.
Machine Learning Zoomcamp;2021;How many points do we get for the article about the project?;I need to check what exactly I promised in the first lesson. 5-10 maybe – something like this. I will write about this. We don't need to write an article yet.
Machine Learning Zoomcamp;2021;Why didn't the mean change after filling nans?;"That's an interesting one. Let's say we have a NumPy array. [image for reference] We have some numbers here –  from 1 to 6 – and we compute the mean here. The mean is 3.5. What we can do here is we add this (3.5) multiple times, the mean doesn't change. This is what happens when we have nan here. Pandas simply ignores it – it doesn't look at this. It pretends it's not there. _x000D_
Basically, what Pandas does is it ignores these nans. It looks only at these numbers. When we replace this nans with mean (in this case is 3.5) we get more data, effectively and the mean doesn't change. I can try to show it with a formula. Let's say we have. There will be a bit of math here now. Let's say we have four cities, and then we have a couple of N/As. Basically, this part is the usual values and this part is N/As. This is the missing part. Let's say we want to compute the mean of this, when we do .mean() in Pandas. What happens is, let's say we have n numbers like that, and k is missing values. In total, our series is k+n – the total length. When we compute the mean on just this part, what we get is 1/nΣxi=x̄. This is our mean. In our case, this is 3.5. _x000D_
Now let's replace these N/As with this x̄. So what happens now is that instead of looking at 1/n, we look at 1/n+k, because this is the total number of elements here. This is when we already filled in. Then we have the sum that we had before and we have another sum of this x̄. So we have k of this, and we have n of this. Because we summed k times this x̄, it becomes kx̄. And if we have a formula like this, we can just multiply it by n/n. n/n is simply 1. So it's the same as just multiplying this thing by one. What we get is n*x̄ because this part here is our mean. [image for reference] For those who do not have a clue what I'm talking about, just bear with me. It's almost over. _x000D_
Then what we have as a result is (1/(n+k))(n*x̄+kx̄). Now we can leave x out of this equation (1/(n+k))(n+k)*x̄=x̄. Basically, this shows that you can add as many means as you like here, and the result will always be x̄. I'm not sure if everyone enjoyed this little proof. [image for reference] But this is how you can prove it. I saw this question before the meeting and I got curious, like “How can I show this mathematically?” And I spent like 10 minutes trying to figure out how. It was a nice exercise for my brain. But I'm not sure I would be able to show this proof on the spot, to be honest."
Data Engineering Zoomcamp;2022;How would you productionize DBT with Airflow?;"Victoria_x000D_
There's a DBT package. There are two ways. If you use DBT cloud, you could actually put an Airflow trigger shell into DBT Cloud that you already have in there. Something to think about, for example, we created some shells in there to run everything from Airflow. That job will have an ID, and I could trigger that you send my credentials from DBT Cloud. That would be one way. But I'm guessing this is targeted to a shell using Airflow as a scheduler and not DBT cloud, which is completely valid. For that, there's a package that you could use. There's another thing that says “Invoking DBT through the bash operator.” On this page is the official documentation on how to run the DBT in production in general. One of the options is to do it via Airflow. You can check it out. There's the link of this package, and also the link to an explanation of how to use it via bash operator. You can also check out the other ways, I guess. For example, I suggested using Chrome, if you're working locally. This is also something that's there. It’s not as fancy as having an Airflow setup that Chrome never fails._x000D_
Alexey_x000D_
Well, Airflow is nothing but a fancy Chrome, right? _x000D_
Victoria_x000D_
Yeah, exactly. [chuckles]"
Data Engineering Zoomcamp;2022;I've heard of Airbyte. Where does it fit in a pipeline?;"Alexey_x000D_
Have you used Airbyte, Ankush? _x000D_
Ankush_x000D_
No. What is it? _x000D_
Alexey_x000D_
Airbyte is something similar to Fivetran, but open source. This is also like a pipeline tool _x000D_
Yeah, open source data integration for modern data teams. It's very similar to Fivetran, but it's open source and they have open source connectors. I haven't used it. I want to learn it – I want to try it. It's unlikely that we will use it at work because we already have an existing data infrastructure. But it looks pretty interesting._x000D_
Ankush_x000D_
I think it's like a data engineering template, right? Just choose the technologies and fit them in._x000D_
Alexey_x000D_
I want to try it and see how it works._x000D_
Ankush_x000D_
It should be powerful. _x000D_
Alexey_x000D_
Maybe if you want to explore this for a project, it could be a good idea. But on the other hand, don't spend too much time on that. If you want to give it a try, remember that you're time-bounded, and if you see that you cannot figure out how to make it work, then move on and use a tool from the course so that you can make sure that you can finish the project."
Machine Learning Zoomcamp;2021;Software engineering utilizes test driven development. Since machine learning isn't rule-based, what kind of tests are written for ML projects?;"or that you can do this. For example, we have this model. [image for reference] First of all, you can write integration tests. You put this in Docker. You set it up, so it lives on your machine and you can start hitting it with different HTTPS requests and then you get some results. Then you want to first test that the model doesn't take too long to respond and then you want to look at the numbers and then maybe with these numbers, you look at the mean prediction, the maximum prediction, and things like this. Maybe for some cars, it doesn't have to be a specific number. But let's say you can expect for a car like this (worth $50K), maybe the price should be between $40K and $50K. If your model, all of a sudden, stops producing the price for this car within this age range, then you look at this and you try to figure out what happened. _x000D_
Basically, again, you have a bunch of test cases for which you want to test that the predictions of your model are the same. And if some of the tests break, you want to take a look at and understand why this is happening. So mainly these two. Then of course, there are some models – for example, we talked about bugs – so maybe this bug has nothing to do with a model and it's just a data pre-processing issue. For that, you can write usual unit tests._x000D_
Basically, for machine learning, you write the same tests as you write for usual software, except they're maybe a little bit different in nature, because you expect that the ranges are not always the same answer."
Machine Learning Zoomcamp;2021;Are there slides for the week 9 serverless videos?;I don't think there were slides. I don't remember. Maybe there were one or two drawings that I did. Maybe I just didn't see the point of uploading them. It was mostly hands-on, without slides.
Machine Learning Zoomcamp;2021;Do we need to deploy our model in week five? Our country doesn't support AWS payment.;No, you don't. You don't have to use AWS. You can use any other cloud provider. You can use Heroku. You can use PythonAnywhere – some of them actually give you free access, where you can deploy one thing for free. But this is just for your experiments. For homework, you do not need to deploy anything anywhere – only locally. The last question there is to deploy a model locally and score a customer. For homework, you do not need to do this. However, I highly encourage you to actually try deploying your model to the cloud. It doesn't have to be AWS. You can try to find something else. For example, for Google Cloud, they give a couple of hundred dollars for credit that you can spend on their platform. They give it for free and you can play with it and then deploy your models there. I highly encourage you to do that.
Machine Learning Zoomcamp;2021;How many homework submissions were there this week?;It's 404. Somebody managed to do it right before I stopped accepting responses. So somebody got lucky.
Data Engineering Zoomcamp;2022;I'm still confused about when to use the data frame or when to use the Spark SQL. Any advice or some simple case?;"Ankush_x000D_
They are pretty similar but Spark SQL does not offer everything. There are some limitations when it comes to Spark SQL. In those cases, you might want to go towards the more “raw” level of data, let's say – basically, data frame or RDD. But if you are more comfortable with SQL, then start with SQL and start using Spark SQL and most probably, quite a few of your cases would be covered there. If you're coming from a data science background, then data frame might fit more naturally to you. It can also be a choice that you can make, but for simple use cases Spark SQL works fine, but maybe it doesn't work for more complicated or more optimized jobs._x000D_
Alexey_x000D_
The way I often use it is try to have best of both worlds. For some things where writing a SQL query is faster, for example, if we need to do join, or group by – in simple cases, I would just go with SQL. But if I need to use a UDF or something like this, then I would do some initial preprocessing with SQL, then a SQL execution of SQL returns another data frame. Then you can execute some things on that data frame as well. So you can combine them."
Machine Learning Zoomcamp;2021;How did the plus come in the second last step?;I don't know if you're referring to this. [image for reference] Maybe just watch the video afterwards and let me know if you have questions because I'm not sure what exactly you're referring to.
Data Engineering Zoomcamp;2022;Can you suggest any interesting datasets to try our data pipelines?;"Alexey_x000D_
There was a discussion/thread in the announcement channel a week ago – a lot of datasets were shared there. I didn't know that there are so many cool websites with datasets. So check it out. _x000D_
Victoria_x000D_
I put it in the announcements once more._x000D_
Alexey_x000D_
I also want to do a Slack dump from DataTalks.Club that you can also use to play around with this dataset and see what it can do. It's a bunch of JSON files. You can build some data pipelines there as well."
Machine Learning Zoomcamp;2021;Why not use PCA to discriminate between important and less important features?;We didn't really cover PCA. I don't know how to explain PCA in a few words. Dmitry, maybe you know how to explain it in just a few words? I think we lost Dmitry. Okay. Then I'll try to do this. Basically, this is a feature – PCA finds a way to create new features from existing ones. Usually when you use PCA, the features that you get are less easy to interpret. Anyways, I don't want to go into much detail about PCA. You can just look it up using PCA for feature importance and then you can ask questions in Slack.
Data Engineering Zoomcamp;2022;Why are you doing this for free? What motivates you all to put in this amount of work and dedication?;"Ankush_x000D_
As I said, for me, it was Alexey's machine learning course. As we said, Learning in Public is really important. For me, personally, it was also important to teach in public and learn from this experience. What about you guys?_x000D_
Sejal_x000D_
I think for me, it's just, like it says in the next question like, “Is it altruism?” Yeah, kind of. It is that. It's just a form of giving back to the community, because I also learned from the community – from looking at open source projects and looking at a lot of publicly available material. This is how I kind of proved myself in a self-made way. I did not really receive any theoretical course. I have a background in computer science, and I never had any data-related education. I just learned it all from my experience, from my colleagues and from the public material outside. So yeah, this is just a form of giving back to the community._x000D_
Victoria_x000D_
For me it’s the same. I feel like if I have the knowledge and I can put in some time and effort, then why not do it? Especially if more people can get into data as a result. At least I get to see if that's something that they would be interested in, especially because the course covers several parts and topics. It's also a good opportunity for people not having to pay for a very expensive course or something like that, and still get a chance to learn if they put effort into it and actually shift their career to data engineering._x000D_
Alexey_x000D_
I have nothing to add. [chuckles]_x000D_
Victoria_x000D_
Although you're the biggest contributor, so. [chuckles]_x000D_
Alexey_x000D_
Not to this one. The feedback I got from the previous course, Machine Learning Zoomcamp, was really awesome. Seeing feedback like “Do not do Coursera. Go directly to this course,” is really motivating. Although I don't think you should… [chuckles] This is a strange feeling that people compare my course with the courses I was actually taking and then say that mine was better. That is very motivating. And that's why I really like doing that. So we will do more of that. I hope you liked it. Also, please give us feedback. If you think you didn't understand something, your feedback will also be helpful. Positive feedback as well – like how you landed an internship after taking this course or how you found your job and things like this. This will be very helpful for us and very motivating. One of the reasons I'm doing this is because I really like this feedback. So please reach out to us with your stories._x000D_
Ankush_x000D_
I think we should create a separate page for feedback, so we can put it there and so that we are more motivated to make more courses in the future."
Data Engineering Zoomcamp;2022;It would be really helpful if you can make one video per week (or as per your comfort) that summarizes that week's concepts for our better understanding.;"Alexey_x000D_
Maybe you can tell us what problems you are having? What are the concepts you have problems with? Maybe we can, again, ask you to help us with that. Perhaps it could be written and doesn't have to be a video. If there are some concepts that you have trouble understanding, you can just ask and then others will answer. We'll have that in some sort of document that you can use to make sure you really understand that. That could be one suggestion._x000D_
Sejal_x000D_
I think we've also updated summarized notes on the repository. For week 1 and week 2, it's already there. Possibly, based on the other instructors’ convenience, they will be releasing other weeks also. My understanding was that this would be a summarization, or a wrap-up, if you ever want to go back and reference or just recall whatever you learned. That would also be a good place to go._x000D_
Alexey_x000D_
Please let us know what concepts are hard to understand. It's also useful feedback for us. The next time we record something, maybe we'll try to explain it clearer. Or at least we will know that maybe this thing needs a better explanation."
Machine Learning Zoomcamp;2021;Is it possible to bypass pipenv/any other Python virtual environments and go straight to Docker for development?;Of course, it's possible. But let's say you want to work on multiple projects on your computer (on your host machine / on your laptop) you don't want to put everything in Docker every time because Docker adds some overhead. You need to build an image – it's just too much overhead. That's why having something lightweight to separate the different environments, different Python projects from each other, I think is a good idea. Of course, you can do this with Docker, but just for local development, for local testing, I think it's better to do it without Docker. But when you deploy it, I think you can do this, but I would suggest you use your environment manager.
Data Engineering Zoomcamp;2022;Do the homework assignments have deadlines? I found a lot of issues in week 1 in terms of dependencies and software versions working with each other.;"Alexey_x000D_
Yes, we have deadlines to keep you focused. It will help you to stay focused. However, as I said, if you feel overwhelmed and you cannot finish the homework in time, don't worry – you will still get the certificate if you complete the project (even though we just said that certificates don't matter). This one maybe also doesn't matter much. What matters is the knowledge you get at the end. Don't be hard on yourself. If you miss a deadline, it's fine. But in terms of scores, you will not get points for the homework that you don’t finish on time. But who cares? As long as you learn something new. That's the most important thing. Sorry to hear that you found a lot of issues. Hopefully you were able to resolve them."
Data Engineering Zoomcamp;2022;Will the YouTube Playlist still be available after the course?;"Alexey_x000D_
Yes, the YouTube playlist will still be available."
Data Engineering Zoomcamp;2022;When there is data shuffling, is it possible that the same key spills into another partition (because of partition size limit) so another shuffle is required?;"Alexey_x000D_
Ankush, you're shaking your head – that means it's not possible?_x000D_
Ankush_x000D_
No. It depends upon the operation you are doing, but if you do an operation in which all your key value pairs have to be at the same node then that will happen on the same node and you will run out of memory. Then either you will need to upgrade your machines to have high memory or you will need to find out your skewed keys and then basically unskew them. That would be the case. But let's assume you are doing something like account or something like an average. An average would also work. If you do an average, for example, then you can have intermediate nodes which will do some precalculation and give only the precalculation ahead to the other nodes. In those cases you can imagine that the shuffling is not happening only on one node. Basically, not all the data is getting there in one node and then something is happening. There are three steps that are happening. In those cases, you might still be able to run with limited memory. But if you do something like a join, then you will need to think about your data skew and maybe your memory._x000D_
Alexey_x000D_
For data skew, I think Ankush sent me a link about how to deal with this data skew. I think this article was about joins. [Ankush agees] Maybe you can share this link later as it is quite useful just to understand how joins work in Spark in addition to what I already explained, because I only scratched the surface. I'll try to find it, but maybe you can just post it._x000D_
Ankush_x000D_
I will also have to look for it, but I think it should be in the slides."
Data Engineering Zoomcamp;2022;With DBT local, can one visualize DAGs?;"Victoria_x000D_
Yes. You can actually visualize not just the DAGs, but the whole DBT doc service on the cloud. The way you do it is through the terminal. I thought I had shown it in the video, but what you basically do is run the DBT doc spinner that will generate a manifest with all of the relationships and everything. And then you do DBT docs serve and this will host the DBT docs in your local host. It's in the repo, I'm pretty sure. You can see how to use it or something like that – you'll find that in the comments there. If you go into the project readme, you can see how to generate the DBT docs. If not, go to DBT Docs Generate to make sure you always have the latest version. Everything, depending on how you run it, you could post it somewhere else if you want it. But this is how you will look at it locally._x000D_
Alexey_x000D_
And it will show a picture, right?_x000D_
Victoria_x000D_
Yeah, I will show the DBT Docs that I show in the videos. You will have kind of like a data catalog of everything. From every table, you can go and check the DAG as well."
Data Engineering Zoomcamp;2022;What was the final conversion rate for your previous course in terms of how many people registered and actually finished?;"Alexey_x000D_
[chuckles] Yeah, the conversion is terrible. There were more than 6000 people who registered, 400 attempted the first homework, and around 50 submitted the last homework. So it's like 1% – a little bit less. But I think what happened is – life happened to most people. Many just left an email. When it came time to actually do the course many realized that there is something else. But that's the beauty of having it openly. Maybe many people just watch and they do not submit homework. That's fine as well. Just take the best out of this course and tell us if you need help – if you get stuck. Don't just stop if you get stuck. Ask for help and we'll be happy to help you._x000D_
Ankush_x000D_
Definitely. I personally also registered for the ML course and watched a lot of videos, but had no time to do any of the homework assignments or finish the project. But I learned a lot from the videos. It was totally worth it."
Data Engineering Zoomcamp;2022;What are your suggestions for reinforcing what we have learned? I was able to get everything working, but I want things to stick.;"Sejal_x000D_
We will have a project in the end, over the last three weeks. We haven't exactly finalized our idea for the project but we will be covering all of the modules that were covered from the previous six weeks so that you can build a project around that. _x000D_
Alexey_x000D_
We will still need to think exactly how to organize the project as it's still not finished, but you can already try to start looking for a dataset that you want to process. Then think about what exactly you want to do with this dataset. You can already start exploring this dataset. Basically, try to do what we did here in this course, but with this dataset. Another thing that you can do to reinforce what we learned is take notes – don’t just take notes in your notebook, but actually share these notes – put them in GitHub, for example, or on Notion. It's very time consuming, I know, but for me personally, notes that I took like five years ago, I still remember things from there. In contrast, the things that I didn’t take notes for, I don't remember. To me, personally, taking notes and then processing them and regurgitating them and putting them out there, is very helpful for information to stay in my brain. That would be one recommendation, but it's very time consuming._x000D_
Ankush_x000D_
I also have another recommendation. If you are already in a data engineering or backend development job, and you see an opportunity to work on Docker or Terraform, take it. Because now you have learned it in the first week, so why not try it out in real life in production instances? If you are not working right now, maybe you’re a student, then just try to build a Docker image on your own. A simple Hello World will do. You do not have to overcomplicate it. And I think that will stick with you forever. If you build something like a Docker file from scratch and you Google around trying to figure out what the best approach to it is, how you will deploy it, and if you are successful in doing it, it will really stick with you more than anything else. The same goes for Terraform. You now have a Google Cloud service account, you have Terraform, which will work for it, and you know how to run it. Just create another storage bucket, just create another transfer service, or Google around what would be the easiest resource to create on the Google Cloud services and just do it. I think the things that you do hands-on on your own will really stick with you._x000D_
Alexey_x000D_
Yeah, that's probably the best advice. Just get your hands dirty."
Machine Learning Zoomcamp;2021;I'm aware of how logs simplify computations involving products, but is NP also prone to FLT PT errors?;"To be honest, I have no idea what this question means… Oh! FLT PT is floating pointers. Okay. So yes, in NumPy, you have these floating point errors. For those who don't know what this is – on computers, let's say, when we multiply two numbers or when we do some operations. When we take two numbers, you see that instead of 3, we have 3, followed by a lot of zeros and then a 4. [image for reference] Again, this is because floating point operations are not precise on computers. That's why when we do summing or multiplying, we have this little floating point arithmetic error. _x000D_
NumPy, because it's running on a computer, it's also prone to that. But I guess your question is “Why do I keep using logs after the data exploration?” You're probably referring to week 2, I think, when we use the logarithm of the price. The main reason for that is not numerical instability – it’s not because there are floating point errors – but (you'll probably learn about this in the video) simply because these distributions have long tails. Please refer to the EDA video (probably second or third) where I explain why we need to do this. And that's why we keep using logs for that. If we don't apply logarithm, then it's very difficult for machine learning models to actually learn from this data."
Machine Learning Zoomcamp;2021;How do you enable the package info for pop-ups on Jupyter Notebook?;I just use tab. I don't think I have Jupyter running here on my computer and I need to start it. But maybe let me just show it with IPython – “import numpy” and then I just do tab. I just press tab and it shows what is there and then when I press tab, it shows what kind of things I can put here. It’s the same in the Jupyter Notebook. You just go inside the parenthesis and then press tab. Then you will see different options shown.
Machine Learning Zoomcamp;2021;Docker in Windows is a nightmare. Is there any way to make it easier?;Yes, you can go to Ubuntu and work there. [chuckles] I don't know. I have a tablet with Docker and Docker doesn't work there because I have Arch 64 architecture on my Windows. People who maybe use the new chip from Mac know what I'm talking about. Basically, I don’t know how to install Docker on my Windows tablet. But I'm not sure I should. For regular laptops I think it's best to just install Linux and use it from there.
Data Engineering Zoomcamp;2022;Are we building a project through this course? If not, could you give us some advice regarding that?;"Victoria_x000D_
Yes. All of the weeks belong to one bigger project, which is about the taxi data for New York that was shown at the very beginning. We cover that in different stages and the idea is that you also use that afterwards to build your own project as the final homework."
Data Engineering Zoomcamp;2022;How many students stay active learners after week 1, 2, and 3?;"Alexey_x000D_
In week 1, we had 400 submissions. In week 2, let me check one more time – it was 200 submissions. But Week two is quite difficult as well. It's 227 responses for week 2. Some of them are duplicates. For week 3, it's too early to say, because it's not over yet. Week 2 is not over yet either, but I don't think this number will increase too much. From what I see – from the number of views of videos – there is also a decay. Some of the videos from week 1 were watched like 5000 times, but for week 2, it's 1000. _x000D_
And for week 3, I think it’s like 500. It's also going down. I hope it's not too difficult for you. Maybe all this Docker and Airflow stuff is just too much. But I must admit that in real life, unfortunately, this is what you also have to deal with – things like setting up an environment. It's very annoying. It feels like you're not learning anything because you need to focus on setting up this Airflow thing instead of working on creating Airflow DAGS. But this is a very useful skill._x000D_
I hope the difficulties you encountered are not too discouraging for you. What doesn't kill us makes us stronger, right? This is a very good skill, being able to figure out these things. You will surely need them at work – figuring out why this tool doesn't work the way you want. I have to deal with this kind of stuff pretty much every day – if not every day, the once per month for sure."
Data Engineering Zoomcamp;2022;The deadline for week 2 is the 7th of February, 2022?;"Alexey_x000D_
Let me see. It is the 31st of January right now. I think we released the homework on Friday, so the deadline is the 4th of February. I'm not sure where the 7th comes from. Is it because it's next Monday? I think at the beginning, we talked about making it on a Monday. Maybe this is something we'll talk about internally and we will see how you're doing. But right now the deadline is Friday. So stick to the deadline. We cannot promise that it will be on a Friday."
Data Engineering Zoomcamp;2022;If somebody does not have a GCP account and wants to run our project, how can it be created? And vice versa â€“ if we need to correct an AWS Azure-based project?;"Alexey_x000D_
Well, use your judgment. If you want to try it, we cannot force you to register an account in Azure and then run it there. If you want, you can do this. If you don't, then maybe just look at the code and see if there are any errors there. It's really up to you here._x000D_
Ankush_x000D_
Maybe when we are doing the matching, we can ask for criteria – like which of those you used – and maybe we can match corresponding people._x000D_
Alexey_x000D_
That's a nice idea. Let's see. I'm afraid it will complicate things a bit and I already have a script for matching and it doesn't use any criteria. It's just random. _x000D_
Ankush_x000D_
So now we need to modify that script with one column. _x000D_
Alexey_x000D_
Yeah, perhaps. Let's see. _x000D_
Ankush_x000D_
Should be easier enough. We can just group them into different things._x000D_
Alexey_x000D_
So when we create a form for submitting, we can ask what Cloud you used. Right? _x000D_
Ankush_x000D_
Exactly. Right._x000D_
Alexey_x000D_
That makes sense. But what we did in the Machine Learning Zoomcamp is – only those who wanted to re-execute the whole thing re-executed it. And then it's up to you. If you really want to learn from this project – because you execute this not just for grading, but also for learning – so if you want to learn more, and you really want to learn how these things work, you execute and then you learn by doing this. But if you don't have time, since executing on a different cloud might take a lot of time – if you don't have that much time, we cannot really ask you to invest something like five hours into figuring out how Azure works. So use your own judgment here. I guess that's the answer. But we will try to do matching to minimize this."
Machine Learning Zoomcamp;2021;Is TensorFlow always used with Keras? I heard Keras was absorbed by TensorFlow.;"Dmitry_x000D_
No? Before, it was kind of different things. I was more into Keras. Recently they kind of united. Right now it's one thing. But you, for sure, can still use Keras as a [cross-talk] _x000D_
Alexey_x000D_
It can work as a standalone thing, right?_x000D_
Dmitry_x000D_
Yeah. I don't know when they plan to finish supporting it. I don't know their plans, but right now you can still use it._x000D_
Alexey_x000D_
Initially Keras didn't work with TensorFlow. The first backend for Keras was… I don't remember what the name of that library was. Do you remember, Dmitry? TensorFlow appeared and everyone forgot about that library, I guess. But was it something that people used before TensorFlow? Theano was the name, exactly. That was the thing that was used as a backend for Keras. But then TensorFlow appeared, Keras added support for TensorFlow as backend, but then the main author of Keras joined Google. Maybe that's the main reason why Keras is now a part of TensorFlow. Just a move from Google to get more users for TensorFlow, I guess. [chuckles]_x000D_
Dmitry_x000D_
But we just don't know the answer. I think we’re just speculating."
Data Engineering Zoomcamp;2022;How did you all meet? Did you work at the same company?;"Ankush_x000D_
No, we do not work at the same company, as you have seen. We all work for different companies. I think Sejal is now moving to Berlin, whereas before she was living in Munich. So we were not even in the same city. We all met through DataTalks.Club. I think it was Alexey’s motivation and all the videos and the ML Zoomcamp he was putting out that gave us the encouragement to contribute and work with him towards creating a new course in data engineering._x000D_
Victoria_x000D_
Both Sejal and I also participated in the podcast. That's how we met"
Data Engineering Zoomcamp;2022;Is there a similar solution like Kubernetes for deploying stateful applications (for example, databases) which will restart a stateful application?;"Alexey_x000D_
There are solutions like Kubernetes for deploying stateful applications like ECS (Amazon elastic container service) for example, which is like Kubernetes, but it's not Kubernetes. With ECS, you don't need to worry about upgrading to the latest version of Kubernetes and so on. It's less popular, maybe also as well-documented compared to Kubernetes, but you can use it. In some aspects, it's easier to manage and in some aspects, it's more difficult because the documentation is maybe not as good. _x000D_
But then I'm not sure if you really want to deploy your database there. If there is an option to use a managed database, then maybe you should go with that. Or you can go with something simpler like DynamoDB, which you don't need to manage at all. I think at Google, there is also something similar. I'm not sure what this part means, “which will restart the stateful application”._x000D_
There is also a thing called HashiCorp Nomad, which is like Kubernetes but cooler (more hip?). You can try that. They're all quite similar."
Machine Learning Zoomcamp;2021;Can the model with the ROC AUC score of around 0.70 from validation be used as a final model?;Yes, it can. It's really dataset dependent. For some datasets, this is a good score. For some datasets 0.6 is a good score. For example, on Kaggle, there was a competition about click prediction. The best models had 0.65 AUC there. So it really depends on your dataset. Already, even with this core, some of the models can be used. There is no rule for every model. Yeah, 0.70 is a reasonably good AUC value to go ahead and use this as a final model.
Machine Learning Zoomcamp;2021;When I click tab in the parentheses of the iPython code, it doesn't show as much info as yours. Is it because of different versions?;Let's say I do “import numpy as np” and then, let's say “np.random.exponential()” I noticed that what I do is press Shift+Tab. If I press Shift+Tab, then it shows the docstring, and if I do this multiple times (press Shift+Tab) then it expands. Then they can see what. The funny thing is, for me, I don't even realize that I do this. I have muscle memory, that's why I wasn't saying this out loud. Now you know how to do this. You just go there and do Shift+Tab. If you do this only once, you will have a small thing open. If you do this twice, it will expand. Another thing you can do is just put a question mark instead of the parentheses here“np.random.exponential?”  and then it will show the docstring. Another thing, I think you can do is put “help(np.random.exponential)” and it will just print the docstring.
Data Engineering Zoomcamp;2022;Can you please explain the use cases of Splunk and Snowflake?;"Alexey_x000D_
Splunk – I don’t know. It's not a data warehouse, is it? Victoria, do you know? _x000D_
Victoria_x000D_
No. Snowflake has an integration with Splunk and Splunk is a data observability platform. I’ve never used it. I don't feel confident enough to explain any use case. I know Monte Carlo, for example. Do you know it? Have you used it? [Alexey confirms] I think it's kind of the same as Monte Carlo. It's just observability. It also provides some security features around your data in your data warehouse. Can we use these tools for the project? I mean, you can use it. It doesn't fully do anything. We taught to do it some other way around, as far as I know. It's kind of like a layer on top, I would say. It adds more information that you would probably end up using in a company but it's up to you. If you want to try it, I think it's a good opportunity since you already have a project. Other than that, it’s not necessary. Plus I guess you'll have to use it with Snowflake. I'm sure if it’s warehouse agnostic or it's just for Snowflake. That part I'm missing._x000D_
Alexey_x000D_
Is there a trial version for Snowflake? _x000D_
Victoria_x000D_
Yes, 300 credits as well. You could use them more because you can only use the warehouse. [cross-talk]_x000D_
Alexey_x000D_
You can have Snowflake in AWS, you can have it in Google Cloud Platform. You can have it in any cloud. Can you? _x000D_
Victoria_x000D_
Yeah. I'm not entirely sure, when you do the free trial, if you have some kind of limitation on where they host it. When you buy it, you choose definitely that – where to host it, location and everything. For the free trial, I can’t remember now. But I know that it's $300._x000D_
Alexey_x000D_
Basically, the use case for Snowflake is the same – an alternative for BigQuery. You would use it in the same way that you would use BigQuery. It's a data warehouse. Another alternative could be Redshift, for example, which is from AWS. Then there’s Firebolt, I think, or something like this. _x000D_
Victoria_x000D_
Yeah. But Firebolt it’s not entirely… because Firebolt is more oriented to performance. For example, I've heard some very interesting use cases where people use Snowflake for everything like transformation and all that. Then they connected the tool to Firebolt, because apparently their query is very fast. I don't think you could use Firebolt entirely as an alternative to Snowflake or BigQuery, for example. I guess it's like everything else. If you're doing a project and you have the time and you're confident enough with everything and you want to try it, then do it. If you're not sure about the time and everything, try not to use new tools. You can always do another perfect or even continue on the same one. Because otherwise, you’ll just overcomplicate yourself. Even though it's an alternative, even though it’s still a data warehouse, you still have to learn it._x000D_
Alexey_x000D_
If I correctly understand what Splunk is – I know what data observability is in general. Let's say you have a streaming pipeline, there is some data going through your pipeline, or you have a batch pipeline. You have some data there. What these tools give you is the possibility to monitor data quality. Every day, you can see how many records there are, how many records with no values in certain columns, and then they give you some nice dashboards and all that. We covered data quality a bit with DBT. We have that there. And Splunk is a more advanced data quality tool, I would say. There is, for example, a tool called Great Expectations, which is also a data quality tool. Another interesting one is Soda SQL._x000D_
Victoria_x000D_
Actually, in Great Expectations, there's a package for DBT, if you’re also interested._x000D_
Alexey_x000D_
You can check them out. They're open source. There is also a tool called Whylogs, which is also good. It's more focused on machine learning, because for machine learning, data pipelines are a big part of machine learning pipelines. So they also do data pipelines. They, for example, support Spark as well. So there are quite a bunch of tools, but we do not cover these tools. We only touch upon data quality a little bit in DBT. Maybe it's a bit advanced, but at work, when you will have a real data pipeline, having these quality checks is a good idea."
Machine Learning Zoomcamp;2021;Why did you use model2bin in the last question? It was not mentioned anywhere in homework.md.;Yes, it was not mentioned. But what was mentioned in the homework was that you have to use the Docker image that I prepared. This was actually done on purpose, because it's possible to just run model1.bin without Docker and get the question and I wanted you to execute it from Docker. This model2.bin was not available anywhere but in the Docker image and this is why the instruction was asking to base your solution on that image.
Machine Learning Zoomcamp;2021;Would you mind clarifying evaluation on test and validation sets or give an example? When I tried with different regularizers, the values were pretty close.;"Alexey_x000D_
I guess the question is why we need to do evaluation on test and validation sets?_x000D_
Dmitry_x000D_
I think it's not very obvious what the question is._x000D_
Yeah. But I'll try to answer. Maybe I'm answering the completely wrong question. Sorry, it's difficult to understand. For validation, we talked about, for example, when you don't know whether you need regularization for the r-value, what kind of regularization parameter you need, what kind of features are useful, what kind of features are not useful, whether you need to fill the missing values with zero, whether you need to fill it with mean, whether you need to fill it with something else, etc. So you have all these questions that you need to answer, and you answer them using the validation dataset? You do this three-way split: train, validation, test. You test the hypothesis on the validation dataset. But then you don't want to throw it away – you still want to use it for training your final model. That's why you combine your train set and your validation dataset into one (like we saw in the last question from today's homework) and you use all this data that is available to train the final model. Then you do the final evaluation on the test dataset. I don't know if I actually answered the question you asked or a question I invented."
Data Engineering Zoomcamp FAQ;2023;Airflow - I’ve got this error: google.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found.;"
Change the path of the google_credentials mounting in the docker-compose file to an absolute one. For example in Ubuntu,
Instead of this: /.google/credentials/:/.google/credentials:ro
Use this: /home/<username>/.google/credentials/:/.google/credentials
I got the error below when I was running download_dataset_task:

*** Log file does not exist: /opt/airflow/logs/taxi_zone_dag/download_dataset_task/2022-02-02T09:39:17.124318+00:00/6.log
*** Fetching from: http://:8793/log/taxi_zone_dag/download_dataset_task/2022-02-02T09:39:17.124318+00:00/6.log
*** Failed to fetch log file from worker. Request URL missing either an 'http://' or 'https://' protocol.

✅I resolved it by running:
docker-compose down -v --rmi all --remove-orphans

After that, remove the following line from my codes:
From datetime import time

And then, restart docker-compose again:
docker-compose up
"
Data Engineering Zoomcamp FAQ;2023;"Airflow DAG is failing with ""CsvOptions can only be specified if storage format is CSV."" error while trying to create external table in BigQuery";"
Case:
Airflow DAG tries to create external table in BigQuery, which refers to Google cloud storage parquet file, and fails with the error: ""CsvOptions can only be specified if storage format is CSV.""

Failing part of the code of data_ingestion_gcs_dag.py:

    bigquery_external_table_task = BigQueryCreateExternalTableOperator(
        task_id=""bigquery_external_table_task"",
        table_resource={
            ""tableReference"": {
                ""projectId"": PROJECT_ID,
                ""datasetId"": BIGQUERY_DATASET,
                ""tableId"": ""external_table"",
            },
            ""externalDataConfiguration"": {
                ""sourceFormat"": ""PARQUET"",
                ""sourceUris"": [f""gs://{BUCKET}/raw/{parquet_file}""],
            },
        },
    )

Solution:
Add an underscore sign in the names of the two parameters of BigQueryCreateExternalTableOperator:
""source_format""
""source_uris""


    bigquery_external_table_task = BigQueryCreateExternalTableOperator(
        task_id=""bigquery_external_table_task"",
        table_resource={
            ""tableReference"": {
                ""projectId"": PROJECT_ID,
                ""datasetId"": BIGQUERY_DATASET,
                ""tableId"": ""external_table"",
            },
            ""externalDataConfiguration"": {
                ""source_format"": ""PARQUET"",
                ""source_uris"": [f""gs://{BUCKET}/raw/{parquet_file}""],
                ""autodetect"": True,
            },
        },
    )


The full class description with the correct names can be found in the following source file:
https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/bigquery.py


"
Data Engineering Zoomcamp FAQ;2023;Airflow web login issue on docker:;"
I was unable to log onto my linux instance of airflow with the web password until I modified the config file in docker_compose.yaml from:
_AIRFLOW_WWW_USER_CREATE: 'true'
_AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
 _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}

 to :
_AIRFLOW_WWW_USER_CREATE=True
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow
"
Data Engineering Zoomcamp FAQ;2023;Airflow won’t update the DAG / It keeps returning errors even though I supposedly installed additional Python libraries;"
Make sure that you update your Airflow image to a more recent one. Inside your Dockerfile, modify the FROM apache/airflow:2.2.3 to any of the more recent images available in the official Airflow Docker repository, available at https://hub.docker.com/r/apache/airflow/tags
"
Data Engineering Zoomcamp FAQ;2023;Blocker: DE Zoomcamp 2.2.6 at around 11:11: denied: requested access to the resource is denied;"✅ Solution:
It is assumed that you have an active Docker account. If not sign-up here.  

Make sure you use below command, before pushing image to Docker hub
docker login

docker build -t docker image DOCKER_USERNAME/IMAGE_NAME:VERSION 
docker build -t docker image timapple/prefect_cloud:v000
docker push DOCKER_USERNAME/IMAGE_NAME:VERSION 
docker push timapple/prefect_cloud:v000

URL: In case the above does not work. Kindly read this thread. 
"
Data Engineering Zoomcamp FAQ;2023;Blocker: NotADirectoryError: [Errno 20] Not a directory: '/opt/prefect/flows';"Code block:

✅Solution: 
From Jeff Hale: Looks like a Docker copy error. Maybe you can't copy a file to a directory that is being created in the same statement. So change the copy command from the .py file to the name of the folder.  For example, copy the local flows folder into the /opt/prefect/flows folder.

Change code to:
Code
Then I revert to 

Build the image and push it again then deploy. And it works! The mystery of the universe

URL: https://datatalks-club.slack.com/archives/C01FABYF2RG/p1674964665817089?thread_ts=1674570724.323079&cid=C01FABYF2RG

Alternative solution:

Managed to make this work by adding a / to the copy lines in Dockerfile, like so:

COPY parameterized_flow.py /opt/prefect/flows/
COPY data /opt/prefect/data/

Running pip command in Dockerfile
THE ERROR:
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc88c50a940>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pip/

✅THE SOLUTION:
Add the nameserver line below into /etc/resolv.conf
sudo nano /etc/resolv.conf
```
nameserver 8.8.8.8
```
"
Data Engineering Zoomcamp FAQ;2023;Connecting to postgres in docker from local machine (windows) using pgcli via port 5432;"
Returns : Password Authentcation Failed for user <username>
Cause: I had pgAdmin installed in my windows PC and it was already using port 5432.
Fix: delete the container and rerun it. This time change the port binding from 5432:5432 to 5433:5432 . This will ask enable docker binds it’s port 5432 to your PC’s port 5433 since your PC’s port 5432 is already in use.
"
Data Engineering Zoomcamp FAQ;2023;Database Error in rpc request (from remote system.sql) Location europe-west3 does not support this operation. (source destination error);"
The only regions available in Europe for Data Transfer are europe-north1, europe-west2 and europe-west6  [Source]

Even if the source and destination locations are same, Big query can still cause errors, apparently its an internal bug
for americas, one of the “bug free” regions is us-east4 where the source destination error should not occur.
I had to re-create my bigQuery dataset, re-run the airflow DAGs only for creating external and partitioned table (didn’t have to change location of gcs) and then dbt run worked
"
Data Engineering Zoomcamp FAQ;2023;GCP credentials json file cannot be found when running a DAG;"I got this error when running a DAG which needs to authenticate connection to the GCP:

Make sure first that you put the credentials file into the directory. Then proceed.
How did I solve it? Those are the changes I made on the docker-compose.yaml:



In this stage, you might get another error, associated with directory permissions. Just grant a per
mission to the directory via this expression (run it on the terminal):
chmod 774 ~/.google/credentials/<filename.json>

"
Data Engineering Zoomcamp FAQ;2023;Homework Week 5: `**HVFHW June 2021**`;"HVFHW in the question refers to the FHVHV dataset linked here: https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhvhv

Literally, the best resource to study Kafka: https://www.gentlydownthe.stream/




"
Data Engineering Zoomcamp FAQ;2023;Installing pyamlython libraries in airflow;"Under this section of the docker-compose.yaml file, find the 
_PIP_ADDITIONAL_REQUIREMENTS: 
  build:
    context: .
    dockerfile: ./Dockerfile
  environment:

_PIP_ADDITIONAL_REQUIREMENTS:${_PIP_ADDITIONAL_REQUIREMENTS:-}

E.g
_PIP_ADDITIONAL_REQUIREMENTS:${_PIP_ADDITIONAL_REQUIREMENTS:- pyspark}

See documentation: https://airflow.apache.org/docs/docker-stack/entrypoint.html#installing-additional-requirements
"
Data Engineering Zoomcamp FAQ;2023;Memory is not enough to run airflow init;"✅ Solution:
 Make a .wslconfig file at the  HOME directory and add the following configurations 
        ```
        # Settings apply across all Linux distros running on WSL 2
        [wsl2]

        # Limits VM memory to use no more than 4 GB, this can be set as whole numbers using GB or MB
        memory=6GB

       ```

"
Data Engineering Zoomcamp FAQ;2023;"Postgres is failing with 'could not open relation mapping file ""global/pg_filenode.map"" '";"
See this link

Assigning the unprivileged Postgres user as the owner of the Postgres data directory

sudo chown -R postgres /usr/local/var/postgres
"
Data Engineering Zoomcamp FAQ;2023;Prefect ERROR: This profile is already authenticated with that key.;"When running prefect cli: `prefect cloud login -k $ENV_VAR` Having PREFECT_API_KEY as env var prevent from login to cloud. Open issue on Prefect front: https://github.com/PrefectHQ/prefect/issues/7797
Quick fix: just name your variable name something else than PREFECT_API_KEY
"
Data Engineering Zoomcamp FAQ;2023;Question 4. Github Storage Block;"pydantic.error_wrappers.ValidationError: 1 validation error for Deployment
infrastructure.
Infrastructure block must have 'run-infrastructure' capabilities. (type=value_error)




prefect deployment build path/to/etl_web_to_gcs.py:etl_web_to_gcs -n ""github-deployment"" -sb github/github-zoomcamp



Running out of space on GCP VM.
While deploying docker images I received a notification that my VM (GCP, Ubuntu) was out of space.
This can be confirmed in the terminal with df -h where 99% of /dev/root was used.
The easiest way to fix this is via the GCP GUI https://console.cloud.google.com/compute/disksDetail/
Here you can edit and increase the size without having to mount extra disks.
You will need to restart your VM but no other changes will need to be made.
"
Data Engineering Zoomcamp FAQ;2023;Running the ingestion file using python;"
File ""/usr/lib/python3/dist-packages/psycopg2/__init__.py"", line 122, in connect 
	conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name ""pg-database"" to address: Name or service not known

This is due to not being able to connect to the container port. In this case, we can point to the localhost port.
Command: 
 python3 ingest_data.py \
      --user=root \
      --password=root \
      --host=localhost \
      --port=5432 \
      --db=ny_taxi \
      --table_name=green_taxi_trips \
      --url=""https://github.com/DataTalksClub/nyc-tlc-data/releasexecute binaryes/download/green/green_tripdata_2019-01.csv.gz""

"
Data Engineering Zoomcamp FAQ;2023;Unable to find block document named zoom-gcs for block type gcs-bucket when running $ python3 etl_web_to_gcs.py;"You have not set up the gcs-block yet. Follow this excellent summary by Padhila, Section: DE Zoomcamp 2.2.3 - ETL with GCP & Prefect up until Step 8. Note: If you've replaced your GCP credentials you can generate new one via: GCP > MENU > IAM & Admin > Service Accounts > ""User"" > Keys > Add Key > Create New Key > JSON

UPDATE: I noticed that this can happen if you are logged into the prefect cloud even in another terminal. Log out of prefect cloud `prefect cloud logout`
"
Data Engineering Zoomcamp FAQ;2023;Vscode shortcut keys;"Cmd+j - Hide/Show terminal
"
Data Engineering Zoomcamp FAQ;2023;google.api_core.exceptions.NotFound:;"If you stuck with this problem - check this - https://github.com/mozilla/bigquery-etl/issues/1409

P.S. These entities must be created by your terraform main.tf file
"
Data Engineering Zoomcamp FAQ;2023;Alternative news source other than Slack;"https://t.me/dezoomcamp
Awesome Data Engineering"
Data Engineering Zoomcamp FAQ;2023;Any books or additional resources you recommend? ;"Yes to both! check out this document 
Awesome Data EngineeriAwesome Data Engineeringng"
Data Engineering Zoomcamp FAQ;2023;Are we still using the NYC Trip data for January 2021 or are we using the 2022 data? ;"We will use the same data, as the project will essentially remain the same as last year’s. The data is available here
"
Data Engineering Zoomcamp FAQ;2023;Besides the “Office Hour” which are the live zoom calls?;"We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.
"
Data Engineering Zoomcamp FAQ;2023;Can I follow the course after it finishes?;"Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.
"
Data Engineering Zoomcamp FAQ;2023;Can I follow the course in a self-paced mode and get a certificate?;"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.
"
Data Engineering Zoomcamp FAQ;2023;Can I get support if I take the course in the self-paced mode?;"Yes, the slack channel remains open and you can ask questions there. But always search the channel first and second, check the FAQ, most likely all your questions are already answered here.
You can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.
"
Data Engineering Zoomcamp FAQ;2023;Can I still join the course?;"Yes, even if you don't submit the homeworks, you're still eligible for a certificate as long as you successfully pass the project at the end.

Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.
"
Data Engineering Zoomcamp FAQ;2023;Can I use Airflow instead for my final project? ;"
Yes, you can use any tool you want for your project.  "
Data Engineering Zoomcamp FAQ;2023;Do I need to pay for the cloud services?;"No, if you use GCP and take advantage of their free trial.
"
Data Engineering Zoomcamp FAQ;2023;How can we contribute to the course?;"Star the repo! Share it with friends if you find it useful ❣️
"
Data Engineering Zoomcamp FAQ;2023;How many hours per week am I expected to spend on this course?;"It depends on your background, and previous experience of the modules. It is expected to require about 5 - 15 hours per week. [source1] [source2] 
"
Data Engineering Zoomcamp FAQ;2023;How to troubleshoot issues;"The first step is to try to solve the issue on your own; get used to solving problems. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.

What does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?
Restart the application or server/pc. 
Google it, use ChatGPT, Bing AI etc. 
It is going to be rare that you are the first to have the problem, someone out there has posted the issue and likely the solution. 
Search using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist. 
There are often different solutions for the same problem due to variation in environments. 
Check the tech’s documentation. Use its search if available or use the browsers search function. 
Try uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Don’t forget to restart the server/pc for reinstalls.
Sometimes reinstalling fails to resolve the issue but works if you uninstall first.
Post your question to Stackoverflow. Be sure to read the Stackoverflow guide on posting good questions.
https://stackoverflow.com/help/how-to-ask 
This will be your real life ask an expert in the future (in addition to coworkers). 
Ask in Slack
Before asking a question, 
Check Pins (where the shortcut to the repo and this FAQ is located)
Use the slack app’s search function
Use the bot @ZoomcampQABot to do the search for you
check the FAQ (this document), use search [ctrl+f]
DO NOT use screenshots, especially don’t take pictures from a phone.
DO NOT tag instructors, it may discourage others from helping you. Copy and past errors; if it’s long, just post it in a reply to your thread. 
Use ``` for formatting your code.
Use the same thread for the conversation (that means reply to your own thread). 
DO NOT create multiple posts to discuss the issue.
You may create a new post if the issue reemerges down the road. Be sure to describe what has changed in the environment.
Provide additional information in the same thread of the steps you have taken for resolution.
Take a break and come back to it later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day. 
Remember technology issues in real life sometimes take days or even weeks to resolve.
 If somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students."
Data Engineering Zoomcamp FAQ;2023;I can’t attend the “Office hours” will it be recorded?;"Yes! Every “Office Hours” will be recorded so you can attend whenever you want.
"
Data Engineering Zoomcamp FAQ;2023;I don’t want to watch the weekly youtube videos or do homework. Can I still do the final capstone?;"Yes :) You can do the final capstone and if you pass it, you will get a certificate.
"
Data Engineering Zoomcamp FAQ;2023;I have registered for the Data Engineering Zoomcamp. When can I expect to receive the confirmation email?;You don't need it. You're accepted.
Data Engineering Zoomcamp FAQ;2023;I want to use AWS. May I do that?;"Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!
The problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.
Also see Is it possible to use x tool instead of the one tool you use?
"
Data Engineering Zoomcamp FAQ;2023;Is it possible to use x tool instead of the one tool you use? ;"Yes, this applies if you want to use Airflow instead of Prefect, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio. 
The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use either one of those, or use your tool of choice.
You do need to take in consideration that we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.
"
Data Engineering Zoomcamp FAQ;2023;Is the 2022 repo deleted?;"No, but we moved the 2022 stuff here 
"
Data Engineering Zoomcamp FAQ;2023;Is the 20yy cohort going to be different from the 20xx cohort?;"Yes. (DE Zoomcamp was first launched in 2022.)
For 2023, the main difference is the orchestration tool — we will use Prefect and not Airflow. And new homeworks 🙂
For 2024 (source), 
we will again have a different orchestrator for the 2nd module - it'll be Mage instead of prefect
terraform videos are re-recorded
"
Data Engineering Zoomcamp FAQ;2023;Is the course [Windows/mac/Linux/...] friendly? ;"Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully 
"
Data Engineering Zoomcamp FAQ;2023;Should I use my local machine, GCP, or GitHub Codespaces for my environment?;"You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.
You might face some challenges, especially for Windows users.
If you face challenges and seek alternatives, you may use a virtual machine like GCP.
For those who have a weak machine, VM is your best option.
You may also use GitHub Codespaces.
You can access GCP & Codespaces virtually to work anywhere, even on different laptops/PCs.
If you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.
However, if you prefer to set up a virtual machine, you may start with these first:
Using GitHub Codespaces
Setting up the environment on a cloud VM
I decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.
"
Data Engineering Zoomcamp FAQ;2023;The GCP and other cloud providers are not available in some countries. Is it possible to provide a guide to installing a home lab?;"You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.
For everything in the course, there’s a local alternative. You could even do the whole course locally.
"
Data Engineering Zoomcamp FAQ;2023;To complete the course, you'll build your data engineering pipeline from scratch using the knowledge acquired during the course.;"・Workflow Orchestration
・Data Warehousing
・Analytical engineering
・Batch processing
・Stream processing
 "
Data Engineering Zoomcamp FAQ;2023;Using 2 email ids;"I'm using 2 email ids while working on the zoomcamp. While uploading the homework/projects etc are you mapping progress to the email id which is shared as an input in the form or the one used while being logged into Google(required to submit the form)?
You can use any email you want for the homeworks, it doesn't have to be the same as the one you used for signing up. Just make sure you use the same email for all the homeworks and project(s)
"
Data Engineering Zoomcamp FAQ;2023;What are the homework deadlines and project deadlines? ;"
You can find the latest and uptodate deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml
"
Data Engineering Zoomcamp FAQ;2023;What are the prerequisites for this course?;"GitHub - DataTalksClub/data-engineering-zoomcamp
"
Data Engineering Zoomcamp FAQ;2023;What can I do before the course starts? ;"You can start by installing and setup all the requirements:
Google cloud account  
Google Cloud SDK
Python 3 (installed with Anaconda)
Terraform
Look over the prerequisites and syllabus to see if you are comfortable with these subjects. "
Data Engineering Zoomcamp FAQ;2023;What is Project Cohort#1 and Project Cohort#2 exactly?;"You will have two attempts for a project. If the final project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.

"
Data Engineering Zoomcamp FAQ;2023;What is the video/zoom link to the stream for the first “Office Hour”? ;"It should be posted in the announcements channel on Telegram before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.
"
Data Engineering Zoomcamp FAQ;2023;When will the course start? ;"The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first “Office Hours” live.
Subscribe to course public Google Calendar (it works from Desktop only).
Register for the course before the course starts using this link.
Join the course Telegram channel with announcements.2
Don’t forget to register in DataTalks.Club's Slack and join the #course-data-engineering channel."
Data Engineering Zoomcamp FAQ;2023;Why are we using GCP and not other cloud providers?;"Because everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.
Note that to sign up for a free GCP account, you need to have a valid credit card.
"
Data Engineering Zoomcamp FAQ;2023;Error: Unable to initialize main class org.example.JsonProducer - Caused by: java.lang.NoClassDefFoundError: CsvException;"
Solution: Just open a “java project” in your visual studio code, and the dependencies etc. seem to be loaded correctly."
Data Engineering Zoomcamp FAQ;2023;I already have the .parquet files from Week 4 (Question 8) on my Google Cloud Storage, how can I use that in conjunction with my local dbt + DuckDB + PipeRider setup ?;"
Step 1.:
Start off by copying the local times from your GCS to a local directory in your computer with gsutil:

gsutil cp -r ""gs://BUCKET_NAME/PATH/TO/FOLDER/WITH/DATASETS ."" 

For instance:
gsutil cp -r ""gs://iobruno_datalake_raw/dtc_ny_taxi_tripdata .""

datasets/
├── fhv
│   ├── fhv_tripdata_2019-01.parquet.snappy
│   ├── fhv_tripdata_2019-02.parquet.snappy
│   ├── fhv_tripdata_2019-03.parquet.snappy
│   ├── fhv_tripdata_2019-04.parquet.snappy
│   ├── fhv_tripdata_2019-05.parquet.snappy
│   ├── fhv_tripdata_2019-06.parquet.snappy
│   ├── fhv_tripdata_2019-07.parquet.snappy
│   ├── fhv_tripdata_2019-08.parquet.snappy
│   ├── fhv_tripdata_2019-09.parquet.snappy
│   ├── fhv_tripdata_2019-10.parquet.snappy
│   ├── fhv_tripdata_2019-11.parquet.snappy
│   └── fhv_tripdata_2019-12.parquet.snappy
├── green
│   ├── green_tripdata_2019-01.parquet.snappy
│   ├── green_tripdata_2019-02.parquet.snappy
│   ├── green_tripdata_2019-03.parquet.snappy
│   ├── green_tripdata_2019-04.parquet.snappy
│   ├── green_tripdata_2019-05.parquet.snappy
│   ├── green_tripdata_2019-06.parquet.snappy
│   ├── green_tripdata_2019-07.parquet.snappy
│   ├── green_tripdata_2019-08.parquet.snappy
│   ├── green_tripdata_2019-09.parquet.snappy
│   ├── green_tripdata_2019-10.parquet.snappy
│   ├── green_tripdata_2019-11.parquet.snappy
│   ├── green_tripdata_2019-12.parquet.snappy
│   ├── green_tripdata_2020-01.parquet.snappy
│   ├── green_tripdata_2020-02.parquet.snappy
│   ├── green_tripdata_2020-03.parquet.snappy
│   ├── green_tripdata_2020-04.parquet.snappy
│   ├── green_tripdata_2020-05.parquet.snappy
│   ├── green_tripdata_2020-06.parquet.snappy
│   ├── green_tripdata_2020-07.parquet.snappy
│   ├── green_tripdata_2020-08.parquet.snappy
│   ├── green_tripdata_2020-09.parquet.snappy
│   ├── green_tripdata_2020-10.parquet.snappy
│   ├── green_tripdata_2020-11.parquet.snappy
│   └── green_tripdata_2020-12.parquet.snappy
├── yellow
│   ├── yellow_tripdata_2019-01.parquet.snappy
│   ├── yellow_tripdata_2019-02.parquet.snappy
│   ├── yellow_tripdata_2019-03.parquet.snappy
│   ├── yellow_tripdata_2019-04.parquet.snappy
│   ├── yellow_tripdata_2019-05.parquet.snappy
│   ├── yellow_tripdata_2019-06.parquet.snappy
│   ├── yellow_tripdata_2019-07.parquet.snappy
│   ├── yellow_tripdata_2019-08.parquet.snappy
│   ├── yellow_tripdata_2019-09.parquet.snappy
│   ├── yellow_tripdata_2019-10.parquet.snappy
│   ├── yellow_tripdata_2019-11.parquet.snappy
│   ├── yellow_tripdata_2019-12.parquet.snappy
│   ├── yellow_tripdata_2020-01.parquet.snappy
│   ├── yellow_tripdata_2020-02.parquet.snappy
│   ├── yellow_tripdata_2020-03.parquet.snappy
│   ├── yellow_tripdata_2020-04.parquet.snappy
│   ├── yellow_tripdata_2020-05.parquet.snappy
│   ├── yellow_tripdata_2020-06.parquet.snappy
│   ├── yellow_tripdata_2020-07.parquet.snappy
│   ├── yellow_tripdata_2020-08.parquet.snappy
│   ├── yellow_tripdata_2020-09.parquet.snappy
│   ├── yellow_tripdata_2020-10.parquet.snappy
│   ├── yellow_tripdata_2020-11.parquet.snappy
│   └── yellow_tripdata_2020-12.parquet.snappy

Step 2.: 

Next, on your schema.yml, where you define your data sources:
Set up a name/alias for the sources, and add the meta block as shown down below:

meta:
    External_location: ""read_parquet('/path/to/datasets/where/you/downloadeded/from/gcs/{name}/*')"": 

Note the {name} expression above. That will be replaced by the name of tables that define in the block below. That means:

When you refer to {{ source('parquet', 'yellow') }} , for example:

 It will attempt to fetch the parquets for the yellow_tripdata dataset in: /path/to/datasets/where/you/downloadeded/from/gcs/yellow/

version: 2

sources:

 - name: parquet
   meta:
     external_location: ""read_parquet('/Users/iobruno/Vault/datasets/{name}/*.parquet.snappy')""
   tables:
     - name: fhv
     - name: green
     - name: yellow
     - name: zone_lookup


With that, you can start fresh with DuckDB, with no tables loaded on it whatsoever. 
Pretty neat, huh!?

"
Data Engineering Zoomcamp FAQ;2023;Since most of us ran week 4 dbt in dbt cloud, how is is possible to run piperider with dbt locally?;"The workshop tutorial can be used as a starting point, but instead of duckdb as data source the dbt-bigquery package must be installed like this here and running everything on dbt-core: https://docs.getdbt.com/reference/warehouse-setups/bigquery-setup
https://docs.getdbt.com/docs/get-started/getting-started-dbt-core
Afterwards the project repository of week 4 can be used and piperider can be added after installing it for bigquery.

"
Data Engineering Zoomcamp FAQ;2023;The .html report is not loading correctly on my VM. ;"Check out this extension for VS code. It should correct the issues.
https://marketplace.visualstudio.com/items?itemName=ritwickdey.LiveServer
"
Data Engineering Zoomcamp FAQ;2023;Does anyone know nice and relatively large datasets?;See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md 
Data Engineering Zoomcamp FAQ;2023;How is my capstone project going to be evaluated?;"
Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project. 

 You will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.

The final grade you get will be the median score of the grades you get from the peer reviewers. 

And of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.

"
Data Engineering Zoomcamp FAQ;2023;How to run python is start up script?;"You need to redefine the python environment varible to that of your user account
"
Data Engineering Zoomcamp FAQ;2023;Orchestrating DataProc with Airflow;"https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html

https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html

Give the following roles to you service account:
DataProc Administrator
Service Account User (explanation here)

Use DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.

When using  DataprocSubmitPySparkJobOperator, do not forget to add:
dataproc_jars = [""gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar""]

Because DataProc does not already have the BigQuery Connector.
"
Data Engineering Zoomcamp FAQ;2023;Orchestrating dbt - cloud with Prefect;"DBT cloud in not compatible with Prefect in the free version. The other option is to use dbt-core instead as this can work with Prefect.
"
Data Engineering Zoomcamp FAQ;2023;Orchestrating dbt with Airflow;"The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).
Detailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment
Source code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py
"
Data Engineering Zoomcamp FAQ;2023;Project evaluation - Reproducibility;"The slack thread : https://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999

The question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?

Alex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”
"
Data Engineering Zoomcamp FAQ;2023;Spark Streaming - How do I read from multiple topics in the same Spark Session;"
Initiate a Spark Session
spark = (SparkSession
         .builder
         .appName(app_name)
         .master(master=master)
         .getOrCreate())

spark.streams.resetTerminated()


query1 = spark
		.readStream
		…
		…
		.load()

query2 = spark
		.readStream
		…
		…
		.load()

query3 = spark
		.readStream
		…
		…
		.load()

query1.start()
query2.start()
query3.start()

spark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous

# On the contrary query3.start().awaitTermination() is a blocking call. Works well when we are reading only from one topic.
"
Data Engineering Zoomcamp FAQ;2023;Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`;"You can get the version of py4j from inside docker using this command
docker exec -it --user airflow airflow-airflow-scheduler-1 bash -c ""ls /opt/spark/python/lib""
"
Data Engineering Zoomcamp FAQ;2023;Why there’s ‘Learning in public links’ block in the Projects evaluation form?;"
Add here any post links where you share some thoughts and insights you’ve got during an assessment of other participants’ projects.
"
Data Engineering Zoomcamp FAQ;2023; Docker-Compose - Which docker-compose binary to use for WSL?;"To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:
uname -s  -> return Linux most likely
uname -m -> return ""flavor""
Or try this command - 
sudo curl -L ""https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)"" -o /usr/local/bin/docker-compose
"
Data Engineering Zoomcamp FAQ;2023; GCP VM - VM connection request timeout;"Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.

✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.
cd ~/.ssh
code config ← this opens the config file in VSCode
"
Data Engineering Zoomcamp FAQ;2023;CURL - curl: (6) Could not resolve host: output.csv;"Solution (for mac users): os.system(f""curl {url} --output {csv_name}"")
"
Data Engineering Zoomcamp FAQ;2023;Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?;"Make sure you're able to start the Docker daemon, and check the issue immediately down below:
And don’t forget to update the wsl in powershell the  command is wsl –update"
Data Engineering Zoomcamp FAQ;2023;Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).;"
Run this command before starting your VM:
On Intel CPU:
modprobe -r kvm_intel
modprobe kvm_intel nested=1
On AMD CPU:
modprobe -r kvm_amd
modprobe kvm_amd nested=1
"
Data Engineering Zoomcamp FAQ;2023;Docker - Cannot pip install on Docker container (Windows);"You may have this error:
Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u
rllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':
/simple/pandas/

Possible solution might be:
$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9
"
Data Engineering Zoomcamp FAQ;2023;Docker - Connecting from VS Code;"It’s very easy to manage your docker container, images, network and compose projects from VS Code.
Just install the official extension and launch it from the left side icon.

It will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.

Docker - How to stop a container?

Use the following command:

$ docker stop <container_id>
"
Data Engineering Zoomcamp FAQ;2023;"Docker - Could not change permissions of directory ""/var/lib/postgresql/data"": Operation not permitted ";"$ docker run -it\
  -e POSTGRES_USER=""root"" \
  -e POSTGRES_PASSWORD=""admin"" \
  -e POSTGRES_DB=""ny_taxi"" \
  -v ""/mnt/path/to/ny_taxi_postgres_data"":""/var/lib/postgresql/data"" \
  -p 5432:5432 \
  postgres:13

CCW
The files belonging to this database system will be owned by user ""postgres"".
This use The database cluster will be initialized with locale ""en_US.utf8"".
The default databerrorase encoding has accordingly been set to ""UTF8"".
xt search configuration will be set to ""english"".

Data page checksums are disabled.
fixing permissions on existing directory /var/lib/postgresql/data ... initdb: 
error: could not change permissions of directory ""/var/lib/postgresql/data"": Operation not permitted  volume
One way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data
The input dtc_postgres_volume_local must match in both commands below

$ docker volume create --name dtc_postgres_volume_local -d local
$ docker run -it\
  -e POSTGRES_USER=""root"" \
  -e POSTGRES_PASSWORD=""root"" \
  -e POSTGRES_DB=""ny_taxi"" \
  -v dtc_postgres_volume_local:/var/lib/postgresql/data \
  -p 5432:5432 \
  postgres:13

To verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.

An alternate error could be:

initdb: error: directory ""/var/lib/postgresql/data"" exists but is not empty
If you want to create a new database system, either remove or empty
the directory ""/var/lib/postgresql/data"" or run initdb
witls
"
Data Engineering Zoomcamp FAQ;2023;Docker - Docker network name (solution for mac) ? ;"
Get the network name via: $ docker network ls.



"
Data Engineering Zoomcamp FAQ;2023;Docker - Docker won't start or is stuck in settings (Windows 10 / 11);"
First off, make sure you're running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to ""Upgrade"" doesn't work (which is another clear indicator for you to uninstall, and reinstall with the latest version)
 
If docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa

[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)

In order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11

If you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here

[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here

If even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.
"
Data Engineering Zoomcamp FAQ;2023;Docker - ERRO[0000] error waiting for container: context canceled ;"You might have installed docker via snap. Run “sudo snap status docker” to verify. 
If you have “error: unknown command ""status"", see 'snap help'.” as a response than deinstall docker and install via the official website 
"
Data Engineering Zoomcamp FAQ;2023;"Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: ""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create"" : open //./pipe/docker_engine: The system cannot find the file specified";"
As the official Docker for Windows documentation says, the Docker engine can either use the Hyper-V or WSL2 as its backend. However, a few constraints might apply

Windows 10 Pro / 11 Pro Users: 

In order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11


Windows 10 Home / 11 Home Users: 

On the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url

You can find the detailed instructions to do so here: https://pureinfotech.com/install-wsl-windows-11/

In case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: 

https://github.com/microsoft/WSL/issues/5393
"
Data Engineering Zoomcamp FAQ;2023;"Docker - Error response from daemon: Conflict. The container name ""pg-database"" is already in use by container “xxx”.  You have to remove (or rename) that container to be able to reuse that name.";"Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:
>>> If the container is running state, use docker stop <container_name>
>>> then, docker rm pg-database
Or use docker start instead of docker run in order to restart the docker image without removing it."
Data Engineering Zoomcamp FAQ;2023;Docker - Error response from daemon: invalid mode: \Program Files\Git\var\lib\postgresql\data.;"
Change the mounting path. Replace it with one of following: 
-v /e/zoomcamp/...:/var/lib/postgresql/data
-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\ (leading slash in front of c:)

"
Data Engineering Zoomcamp FAQ;2023;Docker - I am trying to run postgres with volume mounting it ran success and the database server started but i could not see the files on my machine for persistency. I am using Ubuntu WSL this is the code I used to run docker:;"docker run -it \
    -e POSTGRES_USER=""root"" \
    -e POSTGRES_PASSWORD=""root"" \
    -e POSTGRES_DB=""ny_taxi"" \
    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgres/data \
    -p 5432:5432 \
    postgres:13

I had to do this to make it work. got it from the readme files on the repo
Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage). 

IF the repository is public, the fetch and download happens without any issue whatsoever.

For instance: 
docker pull postgres:13
docker pull dpage/pgadmin4	

BE ADVISED:

The Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).

Meaning: you are NOT required to perform a docker login to fetch them. 

So if you get the message above saying ""docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:

For instance:

$ docker pull dbpage/pgadmin4

Will throw that exception telling you ""repository does not exist or may require 'docker login'

$ docker pull dbpage/pgadmin4                              ✔  base   07:45:46

		Using default tag: latest

		Error response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or 
		may require 'docker login': denied: requested access to the resource is denied

But that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4

How to fix it:

$ docker pull dpage/pgadmin4 

	$ docker pull dpage/pgadmin4

	Using default tag: latest

latest: Pulling from dpage/pgadmin4
a9eaa45ef418: Already exists
942bbf3d7389: Pull complete
fbe23c71dc3b: Pull complete
7c1be9e99602: Pull complete
ccc31a15f27f: Pull complete
617b6e01309f: Pull complete
e6cfa0ba7132: Pull complete
9dd539b143fa: Pull complete
6f3ff58d53db: Pull complete
a79e40a556fb: Pull complete
b05884a10df3: Pull complete
3a39531f7518: Pull complete
0337d3baf297: Pull complete
c7a9de9c5d61: Pull complete

	Digest: sha256:79b2d8da14e537129c28469035524a9be7cfe9107764cc96781a166c8374da1f
Status: Downloaded newer image for dpage/pgadmin4:latest
docker.io/dpage/pgadmin4:latest


EXTRA NOTES:

In the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to. 

For which cases, you must first execute:
$ docker login

Fill in the details of your username and password. 
And only then perform the `docker pull` against that private repository
"
Data Engineering Zoomcamp FAQ;2023;Docker - Setting up Docker on Mac ;"Check this article for details - Setting up docker in macOS
From researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.



"
Data Engineering Zoomcamp FAQ;2023;Docker - Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?;"It is recommended by the Docker docs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).
More info in the Docker Docs on Best Practises
"
Data Engineering Zoomcamp FAQ;2023;Docker - The input device is not a TTY (Docker run for Windows);"You may have this error:
$ docker run -it ubuntu bash
the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty''

Solution:
Use winpty before docker command (source)
$ winpty docker run -it ubuntu bash

You also can make an alias:
echo ""alias docker='winpty docker'"" >> ~/.bashrc

OR
echo ""alias docker='winpty docker'"" >> ~/.bash_profile
"
Data Engineering Zoomcamp FAQ;2023;Docker - build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’;"Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).
✅Solution:
Just add permission for everyone to the corresponding folder
sudo chmod -R 777 <path_to_folder>
Example:
sudo chmod -R 777 ny_taxi_postgres_data/
"
Data Engineering Zoomcamp FAQ;2023;Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.;"This error appeared when running the command: docker build -t taxi_ingest:v001 .
When feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.
Since at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).
A more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata
"
Data Engineering Zoomcamp FAQ;2023;Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.;"This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.

$ docker build -t taxi_ingest:v001 .

A folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this command;

$ sudo chmod -R 755 ny_taxi_postgres_data

Or use 777 if you still see problems. 755 grants write access to only the owner.
"
Data Engineering Zoomcamp FAQ;2023;Docker - ingestion when using docker-compose could not translate host name;"Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name ""pgdatabase"" to address: Name or service not known

When running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase

E.g.: 
pg-network becomes 2docker_default
Pgdatabase becomes 2docker-pgdatabase-1"
Data Engineering Zoomcamp FAQ;2023;Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows);"Mapping volumes on Windows could be tricky. The way it was done in the course video doesn’t work for everyone.

First, if you have spaces in the path, move your data to some folder without spaces. E.g. if your code is in “C:/Users/Alexey Grigorev/git/…”, move it to “C:/git/…”
Try replacing the “-v” part with one of the following options:
-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data
-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data
-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data
-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data
--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data

Try adding winpty before the whole command
 winpty docker run -it 
   -e POSTGRES_USER=""root"" 
   -e POSTGRES_PASSWORD=""root"" 
   -e POSTGRES_DB=""ny_taxi"" 
   -v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data 
   -p 5432:5432 
    postgres:13 

Try adding quotes:
-v ""/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data""
-v ""//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data""
-v “/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data""
-v ""//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data""
-v ""c:\some\path\ny_taxi_postgres_data"":/var/lib/postgresql/data
Note:  (Window) if it automatically creates a folder called “ny_taxi_postgres_data;C” suggests you have problems with volume mapping, try deleting both folders and replacing “-v” part with other options. For me “//c/” works instead of “/c/”. And it will work by automatically creating a correct folder called “ny_taxi_postgres_data”.
A possible solution to this error would be to use /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes’ position varying as in the above list).

Yes for windows use the command it works perfectly fine
-v /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data

Important: note how the quotes are placed. 
If none of these options work, you can use a volume name instead of the path:
-v ny_taxi_postgres_data:/var/lib/postgresql/data
For Mac: You can wrap $(pwd) with quotes like the highlighted.
docker run -it \
  -e POSTGRES_USER=""root"" \
  -e POSTGRES_PASSWORD=""root"" \
  -e POSTGRES_DB=""ny_taxi"" \
  -v ""$(pwd)""/ny_taxi_postgres_data:/var/lib/postgresql/data \
  -p 5432:5432 \
  Postgres:13

docker run -it \
     -e POSTGRES_USER=""root"" \
      -e POSTGRES_PASSWORD=""root"" \
      -e POSTGRES_DB=""ny_taxi"" \
      -v ""$(pwd)""/ny_taxi_postgres_data:/var/lib/postgresql/data \
      -p 5432:5432 \
      postgres:13

Source:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase
"
Data Engineering Zoomcamp FAQ;2023;"Docker-Compose -  Data retention (could not translate host name ""pg-database"" to address: Name or service not known)";"After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name ""pg-database"" to address: Name or service not known

Docker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.
 "
Data Engineering Zoomcamp FAQ;2023;Docker-Compose - Error getting credentials after running docker-compose up -d;"Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078
"
Data Engineering Zoomcamp FAQ;2023;"Docker-Compose - Error translating host name to address
";"Couldn’t translate host name to address

Make sure postgres database is running.

​​Use the command to start containers in detached mode: docker-compose up -d

(data-engineering-zoomcamp) hw % docker compose up -d
[+] Running 2/2
 ⠿ Container pg-admin     Started                                                                                                                                                                      0.6s
 ⠿ Container pg-database  Started

To view the containers use: docker ps.

(data-engineering-zoomcamp) hw % docker ps
CONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES
faf05090972e   postgres:13      ""docker-entrypoint.s…""   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database
6344dcecd58f   dpage/pgadmin4   ""/entrypoint.sh""         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin
hw

To view logs for a container: docker logs <containerid>

(data-engineering-zoomcamp) hw % docker logs faf05090972e

PostgreSQL Database directory appears to contain a database; Skipping initialization

2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC
2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in 
progress
2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910
2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0
2022-01-25 05:59:33.726 UTC [28

] LOG:  redo done at 0/98A3C128
2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections

If docker ps doesn’t show pgdatabase running, run: docker ps -a
This should show all containers, either running or stopped.
Get the container id for pgdatabase-1, and run 
"
Data Engineering Zoomcamp FAQ;2023;Docker-Compose - Error undefined volume in Windows/WSL;"If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:

service ""pgdatabase"" refers to undefined volume dtc_postgres_volume_local: invalid compose project

In order to make it work, you need to include the volume in your docker-compose file. Just add the following:
volumes:
  dtc_postgres_volume_local:

(Make sure volumes are at the same level as services.)"
Data Engineering Zoomcamp FAQ;2023;Docker-Compose - Hostname does not resolve;"It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found
Try:
docker ps -a to see all the stopped&running containers
d to nuke all the containers
Try: docker-compose up -d again ports

On localhost:8080 server → Unable to connect to server: could not translate host name 'pg-database' to address: Name does not resolve
Try: new host name, best without “ - ” e.g. pgdatabase
And on docker-compose.yml, should specify docker network & specify the same network in both  containers 
services:
  pgdatabase:
    image: postgres:13
    environment:
      - POSTGRES_USER=root
      - POSTGRES_PASSWORD=root
      - POSTGRES_DB=ny_taxi
    volumes:
      - ""./ny_taxi_postgres_data:/var/lib/postgresql/data:rw""
    ports:
      - ""5431:5432""
    networks:
      - pg-network       

  pgadmin:
    image: dpage/pgadmin4
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@admin.com
      - PGADMIN_DEFAULT_PASSWORD=root
    ports:
      - ""8080:80""
    networks:
      - pg-network
networks:
  pg-network:
    name: pg-network"
Data Engineering Zoomcamp FAQ;2023;Docker-Compose - Persist PGAdmin docker contents on GCP ;"So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example: 

services: 
	…
	…
pgadmin:
	…
	…
Volumes: 
“./pgadmin”:/var/lib/pgadmin:wr”

Might not work so in this use you can use Docker Volume to make it persist, by simply changing

services:
	…
	….
pgadmin: 
	…
	…
Volumes: 
pgadmin:/var/lib/pgadmin
volumes:
   pgadmin:

"
Data Engineering Zoomcamp FAQ;2023;Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied;"This happens if you did not create the docker group and added your user. Follow these steps from the link:
guides/docker-without-sudo.md at main · sindresorhus/guides · GitHub
And then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection
If you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:
In your docker-compose.yaml file, enter the following into your pgAdmin declaration:
    volumes:
      - type: volume
        source: pgadmin_data
        target: /var/lib/pgadmin

Also add the following to the end of the file:ls
volumes:
  Pgadmin_data:


"
Data Engineering Zoomcamp FAQ;2023;Docker-Compose - docker-compose still not available after changing .bashrc;"This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.
"
Data Engineering Zoomcamp FAQ;2023;Docker-Compose - mounting error;"error: could not change permissions of directory ""/var/lib/postgresql/data"": Operation not permitted  volume 
if you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:
	
volumes:  
dtc_postgres_volume_local:  # Define the named volume here

# services mentioned in the compose file auto become part of the same network!
services:
your remaining code here . . . 

now use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint
In my case, after i ran docker compose up the mouting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’
All i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)
run docker compose up again and check if the table is there or not!"
Data Engineering Zoomcamp FAQ;2023;Empty Folder - Ny_taxi_postgres_data;"
Even after properly running the docker script the folder is empty in the vs code  then try this 

 winpty docker run -it \
  -e POSTGRES_USER=""root"" \
  -e POSTGRES_PASSWORD=""root"" \
  -e POSTGRES_DB=""ny_taxi"" \
  -v  ""C:\Users\sai\Downloads\DE_Project_git_connected\DE_OLD\week1_set_up\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data"" \
  -p 5432:5432 \
  Postgres:13

Here quoting the absolute path in the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video
"
Data Engineering Zoomcamp FAQ;2023;GCP - Do I need to delete my instance in Google Cloud? ;"In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?

Nope. Do not delete your instance in Google Clo ud platform. Otherwise, you have to do this twice for the week 1 readings.

GCP - Windows Google Cloud SDK install issue:gcp
for windows if you having trouble install SDK try follow these steps on the link, if you getting this error: 
These credentials will be used by any library that requests Application Default Credentials (ADC).
WARNING:
Cannot find a quota project to add to ADC. You might receive a ""quota exceeded"" or ""API not enabled"" error. Run $ gcloud auth application-default set-quota-project to add a quota project.
For me:
I reinstalled the sdk using unzip file “install.bat”, 
after successfully checking gcloud version, 
run gcloud init to set up project before
you run gcloud auth application-default login 
https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md

GCP VM - I cannot get my Virtual Machine to start because GCP has no resources.
Click on your VM
Create an image of your VM
On the page of the image, tell GCP to create a new VM instance via the image
On the settings page, change the location
"
Data Engineering Zoomcamp FAQ;2023;GCP - OR-CBAT-15 ERROR Google cloud free trial account 	 ;"GCP Account Suspension Inquiry
If Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).
Unfortunately, there’s small hope that support will help.
It seems that Pyypl web-card should work too.


"
Data Engineering Zoomcamp FAQ;2023;GCP - Project creation failed: HttpError accessing … Requested entity alreadytpep_pickup_datetime exists;"It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.
WARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{'vtpep_pickup_datetimeary': 'Origin, X-Origin, Referer', 'content-type': 'application/json; charset=UTF-8', 'content-encoding': 'gzip', 'date': 'Mon, 24 Jan 2022 19:29:12 GMT', 'server': 'ESF', 'cache-control': 'private', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=189', 'alt-svc': 'h3="":443""; ma=2592000,h3-29="":443""; ma=2592000,h3-Q050="":443""; ma=2592000,h3-Q046="":443""; ma=2592000,h3-Q043="":443""; ma=2592000,quic="":443""; ma=2592000; v=""46,43""', 'transfer-encoding': 'chunked', 'status': 409}>, content <{
  ""error"": {
	""code"": 409,
	""message"": ""Requested entity alreadytpep_pickup_datetime exists"",
	""status"": ""ALREADY_EXISTS""
  }
}

From Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1
Project IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it's not surprising it's already taken.
"
Data Engineering Zoomcamp FAQ;2023;GCP - The project to be billed is associated with an absent billing account;"If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2.

The value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login. 

Ashish Agrawal 
Another possibility is that you have not linked your billing account to your current project
"
Data Engineering Zoomcamp FAQ;2023;GCP - Where can I find the “ny-ride.json” file?;"The ny-rides.json is your private file in Google Cloud Platform (GCP). 

And here’s the way to find it: 
GCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create
Note: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type"
Data Engineering Zoomcamp FAQ;2023;GCP VM -  connect to host port 22 no route to host;"
(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.
Go to section Automation
Add Startup script
```
#!/bin/bash
sudo ufw allow ssh
```
Stop and Start VM.
"
Data Engineering Zoomcamp FAQ;2023;GCP VM - Error while saving the file in VM via VS Code;"Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')
You need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.
ssh
sudo chown -R <user> <path to your directory>
"
Data Engineering Zoomcamp FAQ;2023;GCP VM - Is it necessary to use a GCP VM? When is it useful?;"The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.
And the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo."
Data Engineering Zoomcamp FAQ;2023;GCP VM - Port forwarding from GCP without using VS Code;"
You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:
First, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.
From the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm
Execute the same command but with ports 8080 and 8888.
Now you can access pgAdmin on local machine in browser typing localhost:8080
For Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.
To forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128
"
Data Engineering Zoomcamp FAQ;2023;GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied;"I am trying to create a directory but it won't let me do it
User1@DESKTOP-PD6UM8A MINGW64 /
$ mkdir .ssh
mkdir: cannot create directory ‘.ssh’: Permission denied

You should do it in your home directory. Should be your home (~)
Local. But it seems you're trying to do it in the root folder (/). Should be your home (~)
Link to Video 1.4.1
"
Data Engineering Zoomcamp FAQ;2023;Git Bash - Backslash as an escape character in Git Bash for Windows;For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\ (no need to include in .bashrc)
Data Engineering Zoomcamp FAQ;2023;How do I use Git / GitHub for this course?;"After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub

Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).
You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository

Remember to ignoreHow to Create a Git Repository | Atlassian Git Tutorial large database, .csv, and .gz files, as well as other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private). 

This is also a great resource: https://dangitgit.com/

"
Data Engineering Zoomcamp FAQ;2023;PGCLI -  stuck on password prompt;"If your Bash prompt is stuck on the password command for postgres


Use winpty: 7
Alternatively, try using Windows terminal or terminal in VS code.

"
Data Engineering Zoomcamp FAQ;2023;"PGCLI - FATAL: password authentication failed for user ""root"" (You already have Postgres)";"FATAL:  password authentication failed for user ""root""
observations: Below in bold do not forget the folder that was created ny_taxi_postgres_data
This can happen if you already have Postgres installed on your computer. If it’s the case, use a :different port, e.g. 5431:
  -p 5431:5432

And use it when connecting with 
pgcli:
pgcli -h localhost -p 5431 -U root -d ny_taxi

This will connect you to postgres.
If you want to debug: the following can help (on a MacOS)

To find out if something is blocking your port (on a MacOS): 
You can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi
Or list the running postgres services on your local machine with launchctl
`
`
 b
To unload the running service on your local machine (on a MacOS):
unload the launch agent for the PostgreSQL service, which will stop the service and free up the port  
`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`
this one to start it again
`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`
Changing port from 5432:5432 to 5431:5432 helped me to avoid this error.


"
Data Engineering Zoomcamp FAQ;2023;PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli';"I get this error
pgcli -h localhost -p 5432 -U root -d ny_taxi

Traceback (most recent call last):
  File ""/opt/anaconda3/bin/pgcli"", line 8, in <module>
    sys.exit(cli())
  File ""/opt/anaconda3/lib/python3.9/site-packages/click/core.py"", line 1128, in __call__
    return self.main(*args, **kwargs)
  File ""/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py"", line 
1053, in main
    rv = self.invoke(ctx)
  File ""/opt/anaconda3/lib/python3.9/site-packages/click/core.py"", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/opt/anaconda3/lib/python3.9/site-packages/click/core.py"", line 754, in invoke
    return __callback(*args, **kwargs)
  File ""/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py"", line 880, in cli

    os.makedirs(config_dir)
  File ""/opt/anaconda3/lib/python3.9/os.py"", line 225, in makedirspython
    mkdir(name, mode)PermissionError: [Errno 13] Permission denied: '/Users/vray/.config/pgcli'

Make sure you install pgcli without sudo. 
The recommended approach is to use conda/anaconda to make sure your system python is not affected. 
If conda install gets stuck at ""Solving environment"" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda"
Data Engineering Zoomcamp FAQ;2023;PGCLI - Should we run pgcli inside another docker container? ;"In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.
So No, you don’t need to run it inside another container. Your local system will do."
Data Engineering Zoomcamp FAQ;2023;PGCLI - case sensitive use “Quotations” around columns with capital letters;"PULocationID will not be recognized but “PULocationID” will be. This is because unquoted ""Localidentifiers are case insensitive. See docs."
Data Engineering Zoomcamp FAQ;2023;PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused;"Change 
pgcli -h localhost -p 5432 -u root -d ny_taxi TO Socket
pgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi
pgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi
"
Data Engineering Zoomcamp FAQ;2023;PGCLI - error column c.relhasoids does not exist;"When using the command `\d <database name>` you get the error column `c.relhasoids does not exist`. 
Resolution:
Uninstall pgcli
Reinstall pgclidatabase ""ny_taxi"" does not exist
Restart pc
"
Data Engineering Zoomcamp FAQ;2023;PGCLI - no pq wrapper available. ;"ImportError: no pq wrapper available. 
Attempts made:
- couldn't import \dt
opg 'c' implementation: No module named 'psycopg_c' 
- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'
- couldn't import psycopg 'python' implementation: libpq library not found

Solution: 
First, make sure your Python is set to 3.9, at least.
And the reason for that is we have had cases of 'psycopg2-binary' failing to install because of an old version of Python (3.7.3). 

0. You can check your current python version with: 
$ python -V(the V must be capital)

1. Based on the previous output, if you've got a 3.9, skip to Step #2
   Otherwise, you're better off with a new environment with 3.9pyth
$ conda create -n de-zoomcamp python=3.9
$ conda activate de-zoomcamp

2. Next, you should be able to install the lib for postgres like this:
```
$ e
$ pip install psycopg_binary
```
3. Finally, make sure you're also installing pgcli, but use conda for that:
```
$ pgcli -h localhost -U root -d ny_taxi
```
There, you should be good to go now!
"
Data Engineering Zoomcamp FAQ;2023;PGCLI - pgcli: command not found;"Problem: If you have already installed pgcli but bash doesn't recognize pgcli
On Git bash: bash: pgcli: command not found
On Windows Terminal: pgcli: The term 'pgcli' is not recognized…
Solution: Try adding a Python path C:\Users\...\AppData\Roaming\Python\Python39\Scripts to Windows PATH
For details:
Get the location: pip list -v
Copy C:\Users\...\AppData\Roaming\Python\Python39\site-packages
3. Replace site-packages with Scripts: C:\Users\...\AppData\Roaming\Python\Python39\Scripts
It can also be that you have Python installed elsewhere. 
For me it was under c:\python310\lib\site-packages 
So I had to add c:\python310\lib\Scripts to PATH, as shown below.
Put the above path in ""Path"" (or ""PATH"") in System Variables 

Reference: https://stackoverflow.com/a/68233660 
"
Data Engineering Zoomcamp FAQ;2023;PGCLI --help error;probably some installation error, check out Install (pgcli.com)
Data Engineering Zoomcamp FAQ;2023;"Postgres - ""Column does not exist"" but it actually does (Pyscopg2 error in MacBook Pro M2)";"In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.
✅Solution: But if we enclose the column names in double quotes then it will work

"
Data Engineering Zoomcamp FAQ;2023;Postgres - ModuleNotFoundError: No module named 'psycopg2';"Issue:

e…


Solution: 
pip install psycopg2-binary

If you already have it, you might need to update it:
pip install psycopg2-binary --upgrade

Other methods, if the above fails:
if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e
First uninstall the psycopg package
Then update conda or pip 
Then install psycopg again using pip.
if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql
"
Data Engineering Zoomcamp FAQ;2023;"Postgres - OperationalError: (psycopg2.OperationalError) connection to server at ""localhost"" (::1), port 5432 failed: FATAL:  database ""ny_taxi"" does not exist";"
~\anaconda3\lib\site-packages\psycopg2\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)
    120 
    121     dsn = _ext.make_dsn(dsn, **kwargs)
--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
    123     if cursor_factory is not None:
    124         conn.cursor_factory = cursor_factory

OperationalError: (psycopg2.OperationalError) connection to server at ""localhost"" (::1), port 5432 failed: FATAL:  database ""ny_taxi"" does not exist

Make sure postgres is running. You can check that by running `docker ps`
✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432
"
Data Engineering Zoomcamp FAQ;2023;"Postgres - OperationalError: (psycopg2.OperationalError) connection to server at ""localhost"" (::1), port 5432 failed: FATAL:  role ""root"" does not exist";"Can happen when connecting via pgcli 
pgcli -h localhost -p 5432 -U root -d ny_taxi
Or while uploading data via the connection in jupyter notebook
engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')
This can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).

To check whether there even is a root user with the ability to login:
Try: docker exec -it <your_container_name> /bin/bash











And then run: psql -h localhost -d ny_taxi -U root
Also, you could change port from 5432:5432 to 5431:5432

Other solution that worked: 
Changing `POSTGRES_USER=juroot` to `PGUSER=postgres`
Based on this: https://stackoverflow.com/questions/60193781/postgres-with-docker-compose-gives-fatal-role-root-does-not-exist-error
Also `docker compose down`, removing folder that had postgres volume, running `docker compose up` again."
Data Engineering Zoomcamp FAQ;2023;Python - Ingestion with Jupyter notebook - missing 100000 records;"If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same steps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.
However, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.S

✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time next(df_iter) is called should be within the while loop.
📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline

"
Data Engineering Zoomcamp FAQ;2023;Python - Iteration csv without error;"  {t_end - t_start} seconds"")

"
Data Engineering Zoomcamp FAQ;2023;Python - Pandas can read *.csv.gzip;"When a CSV file is compressed using Gzip, it is saved with a "".csv.gz"" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the "".csv.gz"" file as an argument to the read_csv() function. 
Here is an example of how to read a Gzip compressed CSV file using Pandas:

import pandas as pd
df = pd.read_csv('path/to/file.csv.gz', compression='gzip')

If you prefer to keep the uncompressed csv (easier preview n vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.

Python - How to iterate through and ingest parquet file

Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for 
 files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.parqu

import pyarrow.parquet as pq

output_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”

parquet_file = pq.ParquetFile(output_name)
parquet_size = parquet_file.metadata.num_rows

engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')

table_name=”yellow_taxi_schema”

# Clear table if exists
pq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')

# default (and max) batch size
index = 65536

for i in parquet_file.iter_batches(use_threads=True):
	t_start = time()
	print(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')
	i.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')
	index += 65536
	t_end = time()
	print(f'\t- it took %.1f seconds' % (t_end - t_start))"
Data Engineering Zoomcamp FAQ;2023;"SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist ";"
For the HW1 I encountered this issue. The solution is
	SELECT * FROM zones AS z WHERE z.""Zone"" = 'Astoria Zone';
I think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.

Addition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.
	SELECT * FROM zones AS z WHERE z.""Zone"" = 'Astoria’;
"
Data Engineering Zoomcamp FAQ;2023;SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist ;"It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after 
df = pd.read_csv(‘taxi+_zone_lookup.csv’)
Add the row:
df.columns = df.columns.str.lower()

"
Data Engineering Zoomcamp FAQ;2023;SSH Error: ssh: Could not resolve hostname linux: Name or service not known;"To resolve this, ensure that your config file is in C/User/Username/.ssh/config
"
Data Engineering Zoomcamp FAQ;2023;Taxi Data - Data Dictionary for NY Taxi data?;"Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf
Green Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf
"
Data Engineering Zoomcamp FAQ;2023;Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?;"
The pandas read_csv function can read csv.gz files directly. So no need to change anything in the script.
"
Data Engineering Zoomcamp FAQ;2023;Taxi Data - Yellow Taxi Trip Records downloading error, Error 403 or XML error webpage;"When you try to download the 2021 data from TLC website, you get this error:

If you click on the link, and ERROR 403: Forbidden on the terminal. 
We have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data
So the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz
  Note: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)
“gzip -d file.gz”
"
Data Engineering Zoomcamp FAQ;2023;Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?;"
One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.
	
"
Data Engineering Zoomcamp FAQ;2023;Terraform - Error 403 : Access denied;"│ Error: googleapi: Error 403: Access denied., forbidden

Your $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file 
run = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json
And then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS
"
Data Engineering Zoomcamp FAQ;2023;Terraform - Error acquiring the state lock;"https://github.com/hashicorp/terraform/issues/14513
"
Data Engineering Zoomcamp FAQ;2023;Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes;"
The error:
Error: googleapi: Error 403: Access denied., forbidden
│
and
│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.

For this solution make sure to run:
echo $GOOGLE_APPLICATION_CREDENTIALS
echo $?

Solution:
You have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:
export GOOGLE_APPLICATION_CREDENTIALS=""<path/to/your/service-account-authkeys>.json


"
Data Engineering Zoomcamp FAQ;2023;Terraform - Install for WSL;"https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845

 "
Data Engineering Zoomcamp FAQ;2023;Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g;"
You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command. "
Data Engineering Zoomcamp FAQ;2023;Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?;"Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip

"
Data Engineering Zoomcamp FAQ;2023;WSL - Insufficient system resources exist to complete the requested service. ;"Cause:
It happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.

Solution 
for updating Windows terminal which worked for me:
Go to Microsoft Store.
Go to the library of apps installed in your system. 
Search for Windows terminal.
Update the app and restart your system to  see the changes.

For updating the Windows security updates:
Go to Windows updates and check if there are any pending updates from Windows, especially security updates.
Do restart your system once the updates are downloaded and installed successfully.

"
Data Engineering Zoomcamp FAQ;2023;WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.;"Up restardoting the same issue appears. Happens out of the blue on windows.
Solution 1: Fixing DNS Issue (credit: reddit) this worked for me personally
reg add ""HKLM\System\CurrentControlSet\Services\Dnscache"" /v ""Start"" /t REG_DWORD /d ""4"" /f
Restart your computer and then enable it with the following
reg add ""HKLM\System\CurrentControlSet\Services\Dnscache"" /v ""Start"" /t REG_DWORD /d ""2"" /f
Restart your OS again. It should work.
Solution 2: right click on running Docker icon (next to clock) and chose ""Switch to Linux containers""

bash: conda: command not found
Database is uninitialized and superuser password is not specified.
Database is uninitialized and superuser password is not specified.
"
Data Engineering Zoomcamp FAQ;2023;WSL Docker  initdb: error: could not change permissions of directory;"Solution: Use Docker volumes.

</>  docker-compose.yaml
services:
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    user: ""0:0""
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=ny_taxi
    volumes:
      - ""pg-data:/var/lib/postgresql/data""
    ports:
      - ""5432:5432""
    networks:
      - pg-network

  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    user: ""${UID}:${GID}""
    environment:
      - PGADMIN_DEFAULT_EMAIL=email@some-site.com
      - PGADMIN_DEFAULT_PASSWORD=pgadmin
    volumes:
      - ""pg-admin:/var/lib/pgadmin""
    ports:
      - ""8080:80""
    networks:
      - pg-network

networks:
  pg-network:
    name: pg-network

volumes:
  pg-data:
    name: ingest_pgdata
  pg-admin:
    name: ingest_pgadmin
"
Data Engineering Zoomcamp FAQ;2023;pgAdmin - Create server dialog does not appear;"pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.

Python - ModuleNotFoundError: No module named 'pysqlite2' 
ImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named 'pysqlite2'

The issue seems to arise from the missing of sqlite3.dll in path "".\Anaconda\Dlls\"". 
✅I solved it by simply copying that .dll file from \Anaconda3\Library\bin and put it under the path mentioned above. (if you are using anaconda) "
Data Engineering Zoomcamp FAQ;2023;wget - ERROR: cannot verify <website> certificate  (MacOS);"Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):
Using the Python library wget you installed with pip, try python -m wget <url>

Write the usual command and add --no-check-certificate at the end. So it should be:
!wget <website_url> --no-check-certificate
"
Data Engineering Zoomcamp FAQ;2023;wget is not recognized as an internal or external command;"“wget is not recognized as an internal or external command”, you need to install it.

On Ubuntu, run:
$ sudo apt-get install wget

On MacOS, the easiest way to install wget is to use Brew:
$ brew install wget

On Windows, the easiest way to install wget is to use Chocolatey:
$ choco install wget

Or you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)

Also, you can following this step to install Wget on MS Windows
* Download the lastest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)
* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).
* Rename the file `wget64.exe` to `wget.exe` if necessary.
* Move wget.exe to your `Git\mingw64\bin\`.
Alternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use 	

python -m wget

You need to install it with pip first:
pip install wget

Alternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.

Also recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests
"
Data Engineering Zoomcamp FAQ;2023;"
Prefect blocks: raise RuntimeError(RuntimeError: Unable to load 'de-zoomcamp-gcs' of block type None due to failed validation. To load without validation, try loading again with `validate=False`.";"Probable cause of error:
Copied json key for credentials incorrectly or the service account doesn't have the necessary permissions
How to fix it:
Create a new Service Account with the permissions necessary, copy the json key, paste it into the credentials block and finally execute the pipeline again. 
This should be fixed!

"
Data Engineering Zoomcamp FAQ;2023;"2.2.5 OSError: Cannot save file into a non-existent directory: '..\\..\\data\\yellow'\n"")";"Add 
    if not path.parent.is_dir():
        path.parent.mkdir(parents=True)
    path = Path(path).as_posix()

see:
https://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG

"
Data Engineering Zoomcamp FAQ;2023;Cannot find ‘secret’ block in prefect local UI;"
To fix this error I had to run prefect orion database restart. After the restart, the blocks that were not appearing in the UI appeared (no extra pip installs or block registering should be required). Please note that this will delete all current blocks you have saved, such as your GCP credentials so note down how to create them before doing this step.
"
Data Engineering Zoomcamp FAQ;2023;Conda Env: Why and for what do we need to work with Conda environments exactly? What does this environment give us in terms of ability?;"Python and many other languages that advise on a virtual env suffer from a major flaw called dependency hell. In a nutshell, that means:
It's common sense that different projects have different requirements, right? That could mean different python versions, different libraries, frameworks, etc
So imagine you have a single Python installation (the one that comes with the operating system, for example), and you're invited to work in a Project A and in a Project B

Project A Dependencies:
A1
A2
A3

Project B Dependencies:
B1
B2
B3
Now, a dependency, at the end of the day, also makes use of other dependencies. (These ""implicit"" dependencies of the libraries/frameworks you are using in your project are called: transient dependencies)
Now, with that in mind:
A1 depends on X version 1.0
Meaning ""X"", v1.0, is a transient dependency of A1
B1 depends on X version 2.0
Meaning ""X"", v2.0, is a transient dependency of B1
When you are working on the same ""python environment"", the rule that applies is: Always keep the most updated version of a library. So when you ask your system to install the dependency ""A1"", it will also bring ""X"" at version 1.0
But then you switch to another project, called B, which also depends on X - but at version 2.0
That means that this Python environment will upgrade ""X"" from v1 to v2 to make it work for B1. But the thing is: when you hop between major versions of a library, things often break.
Meaning: B1 will work just fine, because its transient dependency (X v2.0) is satisfied. But A1 may stop working if v2.0 is incompatible with its previous version (v1.0).

The scenario I just said is quite simple, but in the real world, you're going to be working with different versions of Python and other sets of constraints. But instead we had many ppl in here with Python at 3.7 unable to install the project dependencies for psycopg2, and once they updated to Python 3.9, everything started to work fine.

But what if you DO have another project that is running in production on Python 3.7 and starts bugging out ? The best you can do to reproduce/replicate said environment is to make yourself an equivalent environment (w/ 3.7 instead of 3.9)... and the list goes on and on and on

So long story short,
It is considered a best practice to prevent catastrophic project issues like the ones I listed above, not because ""it's a best practice because it's a best practice"".

Besides, you NEVER EVER want to mess up with Python environment that comes with your Operating System (macOS / Linux) -> many many system tools today use Python, and if you break it the one that comes bundled with the OS, you're sure gonna have a lot of headache (as in: unexpected behaviors) to put the pieces back together

Hence why, once again, you use virtualenvs to provide you with isolation. Not only between your projects, but also, between the projects and the underlying infrastructure from the OS

WHICH is why you don't use virtualenvs for containers.
Did you notice that the Dockerfile you're using already comes with Python and we didn't actually have to install conda in there?

That's because the containers are not only meant to be disposable if/when they break, but they also run in an isolated workspace of their own (but this is new and entire different discussion) 

(Comment/Question: Your answer applies as well to Conda environments as also to virtualenv, right? I write this because I wonder why people install Conda at all as you can just use virtualenv.) 
"
Data Engineering Zoomcamp FAQ;2023;Docker: Error on docker image build -t ;"failed to solve with frontend dockerfile.v0: failed to create LLB definition: dockerfile parse error line 1: FROM requires either one or three arguments
Solved: check Dockerfile for some typo + space
 FROM prefecthq/prefect:2.7.7-python3.9 (should not have space)

Run out of space in your machine while running prefect
Check:  
ls ~./prefect/storage
There might be some data stored in there, try to remove them to get more space back"
Data Engineering Zoomcamp FAQ;2023;Docker: container crashed with status code 137.;"It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.

I would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.
"
Data Engineering Zoomcamp FAQ;2023;Docker: “Localhost (127.0.0.1), port 5433 failed: Connection refused.”;"
Instructor using port 5433 for week2. If you’re using previous containers you should set port 5432 and user/pass root/root

"
Data Engineering Zoomcamp FAQ;2023;Email Server Credentials block not found on Prefect Cloud;"Solved:
It seems that some of blocks that exist in Prefect local UI is not exist by default in Prefect Cloud. For this kind of blocks, we need to register it manually by execute command as follows: 
prefect block register -m <block-name>
For example, in my case:
Prefect block register -m prefect_email
Don’t forget to install the block module with pip install.

"
Data Engineering Zoomcamp FAQ;2023;Error Found in Python Script Already Deployed as a Flow;"Initial, I would have to run the entire process from edit the script to build and apply the news deployment. However, I found this to be best.

Solution:
Docker: Edit the python script, rebuild your docker image, push it to dockerhub using thesame name and tag, and run your deployment.
Github: Edit the python script, commit your changes, push it to github and run  your deployment.
"
Data Engineering Zoomcamp FAQ;2023;Error: Bucket name must begin and end with alphanumeric character.;"
Check your GCS Bucket block in Prefect UI @ http://127.0.0.1:4200 - you may have added blank spaces at the beginning/ending of the bucket name.
"
Data Engineering Zoomcamp FAQ;2023;File Path: Cannot save file into a non-existent directory: 'data/green';"
Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go. 

Or - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.

For some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.
"
Data Engineering Zoomcamp FAQ;2023;File path does not exist after building Prefect deployment using GitHub Bucket;"Error: OSError: Cannot save file into a non-existent directory: 'data\green'

Solution: Create a directory data/green at the home directory of your repo.
Or "
Data Engineering Zoomcamp FAQ;2023;File path does not exist after building Prefect to Docker;"
Tags:  prefect, docker, copy
ValueError: Path /opt/prefect/C:\Users\(your username)\.prefect\storage/5eeca69056a042a284e87ea46f757188 does not exist.

Solution:  Remove the `task input hash` at the decorator function of your task.  This is due to Prefect “remembering” the absolute directory instead of relative.
"
Data Engineering Zoomcamp FAQ;2023;FileNotFoundError: [Errno 2] No such file or directory:;"At “etl_web_to_gcs.py” when handling csv/parquet files, this error appear to me, as I didn’t have the directory where the path variable was pointing.
Solution: Create directories automatically, by splitting path variable in two variables, one for path_file and other for path_dir. On path_dir use the method “mkdir(parents=True, exist_ok=True)” to create all needed directories.
Also, we can join two path objects with one slash /


Access to a directory <folder_name> denied when trying to deploy and run workflow through agent
The problem is that prefect deployment check recursively every directory and all subdirectories and files from the directory in which the .yaml file is. In my case it was a volume directory for postgres from previous week and i accidentally create a docker container using sudo and that was the reason why prefect didn’t have acces to it
"
Data Engineering Zoomcamp FAQ;2023;Flow script fails with “killed” message:;"
16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...
Killed
Solution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.
"
Data Engineering Zoomcamp FAQ;2023;GCP VM: Disk Space is full;"
After playing around with prefect for a while this can happen. 
Ssh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.
Most likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.
"
Data Engineering Zoomcamp FAQ;2023;GCS Bucket Block not found on Prefect Cloud UI;"Step 1: From the command line type the following to register and configure the GCP blocks as follows-
prefect block register -m prefect_gcp

Step 2: Go back to the UI and refresh and look at the blocks again, the GCP Bucket Block will now be available for use.
"
Data Engineering Zoomcamp FAQ;2023;GITHUB PUSH ERROR PRE-HOOK DECLINED ;"You get this error if you accidentally send large files (over 120 MB) to github. Use the following commands to correct this:
Git reset –soft HEAD~1 , this would reset the commit back by 1 step, you can use HEAD~N to reset the commits back N steps. - - soft is used to retain changes made on the local directory.
Use git  restore - -staged <file name> to remove the large files from the staging area
Now you can push your commits and add the large file name to your .gitignore file.

"
Data Engineering Zoomcamp FAQ;2023;Github Block- OSError: Failed to pull from remote:  fatal: Too many arguments. (when running deployment);"
Make sure you don’t use the “Reference” field as description of the block cause otherwise it will look endlessly for a branch or name tag in your Github repository that  clearly doesn’t exist.

"
Data Engineering Zoomcamp FAQ;2023;ImportError: cannot import name 'SecretField' from 'pydantic';"
pip install pydantic==1.10.0

Pydantic changed to v2 and older Prefect versions are not compatible. 

Try: pip install --force-reinstall -v ""pydantic==1.10.0"""
Data Engineering Zoomcamp FAQ;2023;No column name lpep_pickup_datetime / tpep_pickup_datetime;"
Do not forget that the green dataset contains lpep_pickup_datetime while the yellow contains  tpep_pickup_datetime. Modify the script(s) accordingly based on the color of dataset when needed. 
"
Data Engineering Zoomcamp FAQ;2023;OSError: Cannot save file into a non-existent directory: ‘/data/yellow’;"
Change the path to go back 2 directories or however many you need so it goes to the proper path
path = Path(f""../../data/{color}/{dataset_file}.parquet"")

OR
I added 2 lines code to write_local Function: 
if not path.parent.is_dir():
	path.parent.mkdir(parents=True)

I suggest using the 2nd fix, the first can cause issues when running through prefect doing steps 2.2.5.

Previous If statement didn’t work for me. I added following line to the write_local function, which did the trick:
path.parent.mkdir(parents=True, exist_ok=True)

ALTERNATIVE:

Import os library, and do:
@task()
def write_local(df: pd.DataFrame, color: str, dataset_file: str) -> Path:
    """"""Write df out locally as a parquet file""""""

    outdir = f""./data/{color}""
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    path = Path(f""{outdir}/{dataset_file}.parquet"")
    df.to_parquet(path, compression='gzip')
    return path

The first block checks if the path already exists, and if it doesn’t, creates it first.
"
Data Engineering Zoomcamp FAQ;2023;OSError: Failed to pull from remote: 'git' is not recognized as an internal or external command.;"Faced this issue when trying github flow code deployment.

Solution:
Take clone of your repo to local directory.
- change directory to cloned repo ($ cd de-zoom-camp)
- Initialize git with command, git init
- open your terminal in VS code, and activate virtual environment(eg; conda)
  conda activate zoomcamp
-Install prefect-github with pip: 
 pip install prefect-github
- register the block,
prefect block register -m prefect_github

It worked successfully.

SSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]

pip install certifi
/Applications/Python\ {ver}/Install\ Certificates.command 
or
running the “Install Certificate.command” inside of the python{ver} folder
"
Data Engineering Zoomcamp FAQ;2023;Prefect API: Client error '404 Not Found';"
Error Message: 
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://ephemeral-orion/api/flow_runs/6aed1011-1599-4fba-afad-b8d999cf9073'
For more information check: https://httpstatuses.com/404
Solution: ✅
reference
[method-1]
Input the below command on local PC.
prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api
[method-2]
Add the below lines in the Dockerfile
ENV PREFECT_API_URL=http://127.0.0.1:4200/api
RUN prefect config set PREFECT_API_URL=""${PREFECT_API_URL}""
(if you need, you can set the Env in Prefect Orion, like below. And you can change the value depending on your demand, such as connection to Prefect Cloud)

"
Data Engineering Zoomcamp FAQ;2023;Prefect Block regenerate on Prefect Cloud or elsewhere:;"So, you have created a lot of blocks in your local and now you are deploying your flow on prefect cloud or gcp or somewhere. The issue which you face is you would have to copy all the prefect blocks required on that machine to run your flow.
Solution: Doing this yourself by typing could be really boring. So you could use Python API to create prefect-block and then you could just re run those scripts anywhere to get those blocks created easily.

"
Data Engineering Zoomcamp FAQ;2023;Prefect Blocks are not showing even after installing the requirements.txt as shown in the video. How to fix:;"
With the Prefect Orion up & running, execute:
prefect orion database reset 
prefect block register -m prefect_gcp
prefect block register -m prefect_sqlalchemy
"
Data Engineering Zoomcamp FAQ;2023;Prefect Deployment - How to pass multiple parameters to prefect deployment run?;"
During class 2.2.6, when he runs something like
prefect deployment run etl-parameters/docker-flow -p ""month= [1,2]""

How can I add more arguments? This didn’t work:
prefect deployment run etl-parameters/docker-flow -p ""color=yellow"" ""month= [1,2]"" ""year= 2021""
I'm getting some errors and I've found out that my deployment doesn't have defined parameters, that's why I need to know how to configure all parameters (we were using by etl_parameters-deployment.yaml file) 

✅Solution: 
With this syntax you can pass multiple parameters: 
prefect deployment run etl-parameters/docker-flow --params= '{""color"":""yellow"", ""month"":[1,2], ""year"":2021}'

More info in the prefect documentation on: Build the deployment
"
Data Engineering Zoomcamp FAQ;2023;Prefect blocks: (TimeOutError) requests.exceptions.ConnectionError: ('Connection aborted.', timeout('The write operation timed out'));"I was hitting the following error in the gcs_block.upload_from_path function
The solution for me was to set the timeout  parameter of the function to 120 (seconds).  gcs_block.upload_from_path(from_path=path, to_path=path, 	)
The default timeout is 60 seconds. Timeout may vary depending on your internet speed.
You could also opt to work from a VM.
Also you can try this workaround https://github.com/googleapis/python-storage/issues/74 (try smaller value of parameters _DEFAULT_CHUNKSIZE and _MAX_MULTIPART_SIZE for gcp blob lib which is used  under the hood of prefect gcp blocks) "
Data Engineering Zoomcamp FAQ;2023;Prefect deployment: Attempt to run deployment etl-parent-flow/docker-flow prefect.exceptions.ScriptError: Script at 'parameterized_flow.py' encountered an exception: FileNotFoundError(2, 'No such file or directory') ;"Please help with answer here  🙏 -> place parmeterized_flow.py in flows folder
Solution:
Ensure you copy the parameterized_flow.py file to the /opt/prefect/flows/ directory giving it the same name as the file parameterized_flow.py
0rameteri1_start/parameterized_flow.py /opt/prefect/flows/pazed_flow.py
build
Prefect Deployment Encountered Exception–prefect deployment build ./parameterized_flow.py:etl_parent_flow -n ""Parametereized ETL ""Script at './parameterized_flow.py' encountered an exception: TypeError(""'type' object is not subscriptable"")
"
Data Engineering Zoomcamp FAQ;2023;Prefect deployment: ERROR | Flow could not be retrieved from deployment ;"
When using the GitHub block, you have to run the prefect deployment build command in the same local folder as the root folder in the GitHub repository. 

In the build command, you have to provide the path to the python file:
path/to/my_file.py:my_flow

--path is used as the upload path, which doesn’t apply for GitHub repository-based storage.

Docker: Trying to build the docker image throws this error:
=> ERROR [5/5] RUN mkdir /opt/prefect/data/yellow 0.3s
——
> [5/5] RUN mkdir /opt/prefect/data/yellow: #9 0.299 mkdir: cannot create directory ‘/opt/prefect/data/yellow’: No such file or directory
—— executor failed running [/bin/sh -c mkdir /opt/prefect/data/yellow]: exit code: 1

Check dockerfile and add -p to mkdir command:
RUN mkdir -p /opt/prefect/data/yellow
Thank you! Fixed in repo now.
"
Data Engineering Zoomcamp FAQ;2023;Prefect deployment: ValueError: Path opt/prefect/C:\Users\user\.prefect\storage/5eeca69056a042a284e87ea46f757188 does not exist.;"Error when you run the command prefect deployment run etl-parent-flow/docker-flow -p ""months=[1,2]""
It looks like you ran the flow before with caching on. Now when it tries to find the cached location, it’s outside Docker and can’t be accessed. If using Prefect 2.7.8 you can refresh the cache and it should work.

Alternatively, you can set the cache_key_expiration to a short period of time - say a minute - and rerun outside Docker and then in Docker, it won’t try to find the cached result.

See more instructions in the docs.

✅Deleted the part that use the cache from the fetch task and the flow runs, i’ll provide further details if i find why it was trying to pull from local cache
# @task(retries=3,cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))
@task(retries=3)
def fetch(dataset_url: str) -> pd.DataFrame:

If using Docker with a function that says to look for a cached run, but you have cache stored locally outside Docker from a previous run, Docker can’t access the cached file, so it throws an error.
Also, make sure the Prefect Block you have for the GCS creds is having the service account JSON data directly instead of the path to the service account JSON file
made sure I removed the CACHE settings on the @task and  I re-ran every command (building docker image, pushing, updating deployment) works!!!
	
For more info, you can also refer this slack thread for the same error - https://datatalks-club.slack.com/archives/C01FABYF2RG/p1674928505612379


ISSUE with activating the credentials in the “Service Account Info” field
When inserting your JSON credentials-dict into the “Service Account Info” text field, it seems that prefect is not checking whether the json-input is correct or not. It happened to me that when copying the content with nano, something in the formatting or else must have been wrong. When just extracting the content with another text editor it worked.
You can have yourself a check by considering the following: If it fails, then it just prints eight stars: 
""********""

If it inserted correctly, then you should see the following masked by the stars (with a formatting of key/value per line):
{  ""type"": ""********"",  ""auth_uri"": ""********"",  ""client_id"": ""********"",  ""token_uri"": ""********"",  ""project_id"": ""********"",  ""private_key"": ""********"",  ""client_email"": ""********"",  ""private_key_id"": ""********"",  ""client_x509_cert_url"": ""********"",  ""auth_provider_x509_cert_url"": ""********""}


Prefect Flow: ERROR   | Task run 'write_local' - Encountered exception during execution
Remember, you have to create the folders where you keep the files from the repository.

Prefect Flow: ERROR   | Flow run 'xxxxxx' - Finished in state Failed('Flow run encountered an exception. google.api_core.exceptions.Forbidden: 403 GET: Access Denied: Table xxxxx: Permission bigquery.tables.get denied on table xxxxxx (or it may not exist).\n')
If you reuse the block with the service account which you created before to connect to Cloud Storage Bucket you have to add permissions of BigQuery Administrator.
"
Data Engineering Zoomcamp FAQ;2023;Prefect flow: Why Prefect DockerContainer deployments can’t connect to other dockers on the host;"Setup:
The project is running with docker compose. They include Prefect Orion, Prefect Agent, Postgres DB etc.

Problem: When trigger the flows of Prefect DockerContainer deployment it was unable to connect to other dockers.

Cause:
The DockerContainer deployment is not running on the same network as rest of the dockers

Fix:
Create a docker network
```
if [ ! ""$(docker network ls | grep dtc-network)"" ]; then
    echo ""\n**Create docker network""
    docker network create -d bridge dtc-network
else
    echo ""dtc-network already exists.""
fi
```

Add the networks to each container within docker-compose.yml
```
Container:
networks:
      		- dtc-network
 networks:
  dtc-network:
    driver: bridge
    name: dtc-network
```
"
Data Engineering Zoomcamp FAQ;2023;Prefect flow: Why are we writing the .csv locally in etl_web_to_gcs.py?;"
Just to so some transformation. It is not necessary to keep it.


Prefect Flow: could not be retrieved from deployment: github storage
Solved: ✅
It is due to prefect trying to read the file in origin or root level. Either try to push the file in root directory in github or mention full path during the deployment command.
Example: 
prefect deployment build -n “name” -sb github/username/directory/filename.py: flow_name –apply"
Data Engineering Zoomcamp FAQ;2023;Prefect flow: etl-parent-flow/docker-flow Crashed with ConnectionRefusedError: [Errno 111] Connect call failed ('127.0.0.1', 4200);"This error occurs when you run your prefect on WSL 2 on Windows 10. The error looks like this:
UI

Prefect Agent Log
httpx.ConnectError: All connection attempts failed
10:10:16.423 | INFO    | prefect.infrastructure.docker-container - Docker container 'placid-corgi' has status 'removing'
10:10:16.437 | INFO    | prefect.infrastructure.docker-container - Docker container 'placid-corgi' has status 'removing'
10:10:16.502 | INFO    | prefect.agent - Reported flow run '24feeb23-6eb9-4523-b330-19c365bd68fc' as crashed: Flow run infrastructure exited with non-zero status code 1.

The error above is because we run Prefect locally on our machine at localhost:4200, when we run docker without specifying their network, docker call the localhost:4200 inside the container but not the localhost:4200 on our machine.

To solve this, you only need to specify Network Mode to bridge.
Config from UI
 
Config from Code (make_docker_block.py)
from prefect.infrastructure.docker import DockerContainer

# alternative to creating DockerContainer block in the UI
docker_block = DockerContainer(
    image=""discdiver/prefect:zoom"", # insert your image here
    image_pull_policy=""ALWAYS"",
    auto_remove=True,
    network_mode=""bridge""
)

docker_block.save(""zoom"", overwrite=True)

"
Data Engineering Zoomcamp FAQ;2023;Prefect: Docker Connection Error on Windows 11/WSL 2;"Error Message: 
ERROR   | prefect.engine - Engine execution of flow run '<hash>' exited with unexpected exception
Traceback (most recent call last):
...
ConnectionRefusedError: [Errno 111] Connect call failed
...
httpcore.ConnectError: All connection attempts failed
...
httpx.ConnectError: All connection attempts failed
WARNING | prefect.infrastructure.docker-container - Docker container <container-name> was removed before we could wait for its completion.

Likely associated with how WSL handles IPv4 addresses, as in this issue: WSL by default will bind to an IPv6 address, which Prefect does not appear to handle.

A temporary fix can be had by using the subsystem IP assigned to the WSL instance. This should work for Debian-based distros on WSL 2, including Ubuntu. Steps as follows:
In a WSL terminal, run the command ip addr to find the subsystem IP. You’re looking for a connection with a valid inet address: in my example below, this is 172.22.53.106
eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:15:5d:f5:0c:25 brd ff:ff:ff:ff:ff:ff
    inet 172.22.53.106/20 brd 172.22.63.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::215:5dff:fef5:c25/64 scope link 
       valid_lft forever preferred_lft forever
While in WSL, start Prefect Orion with that IP address as host:
prefect orion start --host 172.22.53.106
While in WSL, set the Prefect API to that IP address. The exact code for this should be displayed when you start Prefect Orion in the step above.
prefect config set PREFECT_API_URL=http://172.22.53.106:4200/api
You should be able to run Prefect as set in the course videos from this point.

A couple of caveats:
This is only a temporary fix. You'll likely need to copy the steps above for every new instance of WSL- i.e., if you restart, look for the subsystem IP again and plug it into the right places.
I haven't completely ironed out the issues on my end, but I am getting new error messages that suggest the docker deployment is running and connected (i.e., prefect.exceptions.ScriptError: Script at 'parameterized_flow.py' encountered an exception: FileNotFoundError(2, 'No such file or directory') ) Your mileage may vary.

Although I haven’t tested this, you may also be able to resolve the issue using docker-ce in place of Docker Desktop for Windows.

"
Data Engineering Zoomcamp FAQ;2023;Prefect: With Windows, Prefect-gcp 0.2.3 converted / slashes in a path to \ in the to_path statement in the upload_from_path function;"
Bug with prefect-gcp 0.2.3 on Windows only. Couldn’t upload the file into a folder as in 
the video. 
✅SOLUTION: Use prefect-gcp 0.2.4 You can specify the new version in requirements.txt before installing or pip install -U prefect-gcp to upgrade in an existing environment.
Then use before the upload command."
Data Engineering Zoomcamp FAQ;2023;Process to download the VSC using Pandas is killed right away;" 

 pd.read_csv

 df_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)

The data needs to be appended to the parquet file using the fastparquet engine

df.to_parquet(path, compression=""gzip"", engine='fastparquet', append=True)
"
Data Engineering Zoomcamp FAQ;2023;Python lib: Problem with prefect with macOS M1 (arm) with poetry the package manager, I got the below message:;"
I got this problem while tried to run `prefect orion start` to start the prefect ui on my laptop

“ ValueError: the greenlet library is required to use this function. No module named 'greenlet' “

For the people who got the same problem with poetry I solve this issue by add `greenlet` to `pyproject.tom`

So run this command:
poetry add greenlet
"
Data Engineering Zoomcamp FAQ;2023;Question 4 Hint;"Read the docs here first you execute code using python to clone the code to your locals, then build deployment. Note: You can also build the github bucket from the UI, therefore, you can skip the first part and ensure to put token if your repo is private.


Docker - Finished in state Failed('Flow run encountered an exception. ValueError: Path /home/clamytoe/.prefect/storage/342da9557ddb4eecab92e8897192f906 does not exist.\n'):

While trying to run the code from a docker container, it kept failing with this error. It was trying to access the cache directory on my host system.

Solution: The issue turned out to be that the code had caching enabled. Once all of the caching code was removed from the script, it worked flawlessly.




Docker - Rancher Desktop on Mac M1 Ventura: ValueError: 'host.docker.internal' does not appear to be an IPv4 or IPv6 address 

When trying to run the code from a docker container declared as a block in Prefect, the DNS 'host.docker.internal' was not recognized.

Solution: 
prefect orion start --host 192.168.178.90
Where the IP is the internal IP of my MAC
prefect config set PREFECT_API_URL=http://192.168.178.90:4200/api
Declaring the docker container block in Prefect with Network Mode = host 

install -r requerements.txt on Mac
fatal error: Python.h: No such file or directory compilation terminated.


Add env variable:
export C_INCLUDE_PATH=/Library/Devpreeloper/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/Headers

Add new dependency in requerement.txt
backports.zoneinfo==0.2.1"
Data Engineering Zoomcamp FAQ;2023;RuntimeError: Tasks cannot be run from within tasks. Did you mean to call this task in a flow?;"For example: you run the `bigquery_load_cloud_storage` function inside the task. You will get this error.

Solution: That function creates a task. You can’t run a task inside a task in Prefect. The solution is to call that function from the flow itself.

"
Data Engineering Zoomcamp FAQ;2023;Slack Webhook Not Working/‘404’;"Error Message: 
None
(The functionality of the webhook can be tested with the below code after setting up a slack block, which if not functioning properly, will throw an error)
from prefect.blocks.notifications import SlackWebhook

slack_webhook_block = SlackWebhook.load(""slack-block-name"")
slack_webhook_block.notify(""Hello!"")
WARNING | apprise - Failed to send to Slack: Page not found., error=404

This can be fixed by adding your own webhook to the channel.

While logged in to the Temporary Data Engineering Zoomcamp slack (check the week 2 homework for an updated link), visit the Slack API Apps page to create a new app.
Using the splash or via the green “Create a new app” button, create an app “From an app manifest”
Select the workspace “Temporary DE Zoomcamp Prefect Notifications”

Click “next” at the app manifest page (step 2 of 3)
Click “create”
At the next page, select “Incoming Webhooks” under “Add features and functionality”

Set the toggle to the right of “Activate incoming webhooks” to “On” 
Click on the “Add new webhook to workspace” button now at the bottom of the screen

Select the #testing-notifications channel

The Webhook URL provided below is your new webhook- use it in place of the webhook supplied by the homework.

Or you can use an existing one configuration:
Go to https://temp-notify.slack.com/apps/A0F7XDUAZ-incoming-webhooks?tab=settings&next_id=0 
Click “edit” on any existing configuration 
Scroll down to Webhook URL and copy URL 
Paste it in your prefect notifications 

"
Data Engineering Zoomcamp FAQ;2023;Timeout due to slow upload internet;"
In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory
This leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.

Solution:
if you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)

the yellow taxi data for feb 2019 is about 100MB as parquet file

gcp_cloud_storage_bucket_block.upload_from_path(
        from_path=f""{path}"",
        to_path=path,
        timeout=600
    )
"
Data Engineering Zoomcamp FAQ;2023;Uploading to GCS Does not put the file inside a folder but instead the file is named ‘data\green\...’ (Affects Windows);"
Add .as_posix() to the path variable. 
 path = Path(f""data/{color}/{dataset_file}.parquet"").as_posix()

Additionally, in my case I needed to update the prefect_gcp package to the version 0.2.6.
"
Data Engineering Zoomcamp FAQ;2023;While performing SQL query in python using pandas, I am facing the error : TypeError: __init__() got multiple values for argument 'schema';"Install an earlier version of sqlalchemy (1.4.46 release). Sqlalchemy version 2.0.0 was released recently and isn't compatible with pandasql. If you have the latest version of sqlalchemy (v2.0.0), just do:
pip uninstall sqlalchemy
pip install sqlalchemy==1.4.46

$ python parameterized_flow.py 
Traceback (most recent call last):
  File ""parameterized_flow.py"", line 60, in <module>
    def etl_parent_flow(months: list[int] = [1,2], year: int = 2021, color: str = ""yellow""):
TypeError: 'type' object is not subscriptable
"
Data Engineering Zoomcamp FAQ;2023;Why does Jeff have the same line twice in video 2.2.3 and 2.2.4, etl_web_to_gcs.py and parameterized_flow.py?;"df[“tpep_pickup_datetime”] = pd.to_datetime(df[“tpep_pickup_datetime”])
Second one should be:
df[“tpep_dropoff_datetime”] = pd.to_datetime(df[“tpep_dropoff_datetime”])
Repo is updated. Thank you jralduaveuthey and Valentine Zaretsky for catching!"
Data Engineering Zoomcamp FAQ;2023;Why does Jeff use a default parameter assignment in etl_web_to_gcs.py instead of a type hint around time 12:40 in video 2.2.3 and in 2.2.4? ;"That’s a typo. Should be:
@task(log_prints=True)
def clean(df: pd.DataFrame) -> pd.DataFrame:
not
@task(log_prints=True)
def clean(df=pd.DataFrame) -> pd.DataFrame:
Repo is updated. Thank you Valentine Zaretsky for catching it!"
Data Engineering Zoomcamp FAQ;2023;alembic.script.revision.ResolutionError: No such revision or branch;"
https://discourse.prefect.io/t/installation-error-prefect-version-cmd-errors/1784/5

Run the following commands to restart your prefect

rm ~/.prefect/orion.db
prefect orion start
"
Data Engineering Zoomcamp FAQ;2023;denied: requested access to the resource is denied;"This can happen when you 
Haven't logged in properly to Docker Desktop (use docker login -u ""myusername"")
Have used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on
	docker image build -t <myusername>/<imagename>:<tag> 
docker image push <myusername>/<imagename>:<tag>"
Data Engineering Zoomcamp FAQ;2023;Cannot read and write in different locations when creating external table ;"
Error Message:
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721: invalid start byte

Solution:
Step 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:
pd.read_csv(dataset_url, low_memory=False, encoding='latin1') 
Step 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:
df.to_csv(path_on_gsc, compression=""gzip"", encoding='utf-8')

"
Data Engineering Zoomcamp FAQ;2023;Cannot read and write in different locations: source: asia-south2, destination: US;"Solution: This problem arises if your gcs and bigquery storage is in different region. 
One potential way to solve it: 
Go to your google cloud bucket and check the region in field named “Location”

Now in bigquery, click on three dot icon near your project name and select create dataset.

In region filed choose the same regions as you saw in your google cloud bucket


"
Data Engineering Zoomcamp FAQ;2023;DATE() Error in BigQuery;"
Error Message: 
PARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))
Solution: 
Convert the column to datetime first.
df[""pickup_datetime""] = pd.to_datetime(df[""pickup_datetime""])
df[""dropOff_datetime""] = pd.to_datetime(df[""dropOff_datetime""])
"
Data Engineering Zoomcamp FAQ;2023;Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets;"A:
1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages
2) Use python ZipFile package, which is included in all modern python distributions
"
Data Engineering Zoomcamp FAQ;2023;Does Q2 of the homework refer to Q1?;"This question could be old. I am confused: Could it be that the wording of the question changed during the week? I think it was not as clear as now, at the beginning of the week. See the question also in: https://app.slack.com/client/T01ATQK62F8/threads/thread/C01FABYF2RG-1675969917.402839

Suggestion:
For question 2, I feel the question is not correct its either the count of the entire dataset for both tables, which the answer is there or the answer is not there.
"
Data Engineering Zoomcamp FAQ;2023;ERROR Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):;"Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.





This means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset."
Data Engineering Zoomcamp FAQ;2023;Encoding error when writing data from web to GCS:;"Make sure to use Nullable dataTypes, such as Int64 when appliable.


"
Data Engineering Zoomcamp FAQ;2023;"Error: Missing close double quote ("") character";"To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:
$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name ""gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz""


Error on Running Prefect Flow to Load data to GCS
ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.

Remove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.
Note: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That mean, if you have this ```cache_key``` in your initial run, this might cause the error.
"
Data Engineering Zoomcamp FAQ;2023;Failed to create table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet;"
Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema. 

When dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for '2019-05', and 2019-06, have the columns ""PUlocationID"" and ""DOlocationID"" as Integers, while for the period of '2019-01' through '2019-04', the same column is defined as FLOAT.

So while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table. 



So, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:

pd.read_csv(""path_or_url"").astype({
	""col1_name"": ""datatype"",	
	""col2_name"": ""datatype"",	
	...					
	""colN_name"": ""datatype"" 	
})



"
Data Engineering Zoomcamp FAQ;2023;Fix Error when importing FHV data to GCS;"If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\n\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz
Emphasising the ‘/releases/download’ part of the URL.
"
Data Engineering Zoomcamp FAQ;2023;How to handle type error from big query and parquet data?;"
Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this: 
error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE. 
Solution: 
Fix the data type issue in data pipeline 
Before injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.

Something like:
    df[""PUlocationID""] = df.PUlocationID.astype(""Int64"")
    df[""DOlocationID""] = df.DOlocationID.astype(""Int64"")
NOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery
"
Data Engineering Zoomcamp FAQ;2023;I am having problems with columns datatype while running DBT/BigQuery;"R: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.
✅Solution: Defined the schema while running web_to_gcp.py pipeline.
Sebastian adapted the script:
https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py 

Same ERROR - When running dbt run for fact_trips.sql, the task failed with error:
“Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”
Reason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.

There are some possible fixes:
Drop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery 
SELECT * EXCEPT (ehail_fee) FROM… 
Modify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.
Modify Airflow dag to make the conversion and avoid the error. 
pv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))
Same type of ERROR - parquet files with different data types - Fix it with pandas
Here is another possibility that could be interesting:
You can specify the dtypes when importing the file from csv to a dataframe with pandas 
pd.from_csv(..., dtype=type_dict)
One obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.
Sources:
https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html
Nullable integer data type — pandas 1.5.3 documentation
"
Data Engineering Zoomcamp FAQ;2023;I query my dataset and get a Bad character (ASCII 0) error?;"Check the Schema
You might have a wrong formatting
Try to upload the CSV.GZ files without formatting or going through pandas via wget
See this Slack conversation for helpful tips

Uploading files to GCS via GUI
This can help avoid schema issues in the homework. 
Download files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.
"
Data Engineering Zoomcamp FAQ;2023;"If you’re having problems loading the FHV_2021 data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:
";"URL_TEMPLATE = URL_PREFIX + ""/fhv_tripdata_{{ execution_date.strftime(\'%Y-%m\') }}.parquet?raw=true""

Second, update make sure the URL_PREFIX is set to the following value:

URL_PREFIX = ""https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv""

It is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. 
"
Data Engineering Zoomcamp FAQ;2023;Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project;"
Problem occurs when misplacing content after from clause in BigQuery SQLs.
Check to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only

"
Data Engineering Zoomcamp FAQ;2023;Native tables vs External tables in BigQuery?;"
Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.

Resources:
https://cloud.google.com/bigquery/docs/external-tables
https://cloud.google.com/bigquery/docs/tables-intro"
Data Engineering Zoomcamp FAQ;2023;Problem with prefect with macOS M1 (arm);"In video 2.2.2 at 15:16 when running the command `python ingest_data_flow.py`, I got a big error message who said this:

ValueError: the greenlet library is required to use this function....
... is an incompatible architecture (have 'x86_64', need 'arm64'))

My computer is a MacBook Pro M1 and miniconda arm64 installed.

I searched for the arm version of greenlet but couldn't find it, so I found the following solution instead.

I used the instructions from this site How to Manage Conda Environments on an Apple Silicon M1 Mac to create a conda x86 environment.

So instead of running the command:
conda create -n zoom python=3.9
Instead, I ran the command:
create_x86_conda_environment myenv_x86 python=3.9

This solution works! And I can now continue to prefect on my arm computer!
"
Data Engineering Zoomcamp FAQ;2023;Question 5: The partitioned/clustered table isn’t giving me the prediction I expected;"Take a careful look at the format of the dates in the question.

"
Data Engineering Zoomcamp FAQ;2023;Reading parquets from nyc.gov directly into pandas returns Out of bounds error;"If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:
pyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds
Cause:
there is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019. 
pandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`
This becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max
Fix:
Use pyarrow to read it:
import pyarrow.parquet as pq df = pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)
However this results in weird timestamps for the offending record
Read the datetime columns separately using pq.read_table

table = pq.read_table(‘taxi.parquet’)
datetimes = [‘list of datetime column names’]
df_dts = pd.DataFrame()
    for col in datetimes:
        df_dts[col] = pd.to_datetime(table .column(col), errors='coerce')

The `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min
Use parquet.compute.filter to remove the offending rows

import pyarrow.compute as pc

table = pq.read_table(""‘taxi.parquet"")
df = table.filter(
    pc.less_equal(table[""dropOff_datetime""], pa.scalar(pd.Timestamp.max))
).to_pandas()
"
Data Engineering Zoomcamp FAQ;2023;Remember to save your queries;"By the way, this isn’t a problem/solution, but a useful hint:
Please, remember to save your progress in BigQuery SQL Editor.
I was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.
Save your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.

Alternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.

"
Data Engineering Zoomcamp FAQ;2023;These won't work. You need to make sure you use Int64: ;"Incorrect:
df['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or
df['DOlocationID'] = df['DOlocationID'].astype(int)
Correct:
df['DOlocationID'] = df['DOlocationID'].astype('Int64')


"
Data Engineering Zoomcamp FAQ;2023;Tip: Downloading csv.gz from a url in a prefect environment (sample snippet).;"
@task
def download_file(url: str, file_path: str):
    response = requests.get(url)
    open(file_path, ""wb"").write(response.content)
    return file_path

@flow
def extract_from_web() -> None:
file_path = download_file(url=f'{url-filename}.csv.gz',file_path=f'{filename}.csv.gz')


"
Data Engineering Zoomcamp FAQ;2023;Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:;"
There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud. 

Use below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.

import tempfile
import requests
import logging
from google.cloud import bigquery

def hello_world(request):

    # table_id = <project_id.dataset_id.table_id>
    table_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'

    # Create a new BigQuery client
    client = bigquery.Client()


    for month in range(4, 13):
        # Define the schema for the data in the CSV.gz files
        url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)

        # Download the CSV.gz file from Github
        response = requests.get(url)

        # Create new table if loading first month data else append
        write_disposition_string = ""WRITE_APPEND"" if month > 1 else ""WRITE_TRUNCATE""

        # Defining LoadJobConfig with schema of table to prevent it from changing with every table
        job_config = bigquery.LoadJobConfig(
                schema=[
                    bigquery.SchemaField(""dispatching_base_num"", ""STRING""),
                    bigquery.SchemaField(""pickup_datetime"", ""TIMESTAMP""),
                    bigquery.SchemaField(""dropOff_datetime"", ""TIMESTAMP""),
                    bigquery.SchemaField(""PUlocationID"", ""STRING""),
                    bigquery.SchemaField(""DOlocationID"", ""STRING""),
                    bigquery.SchemaField(""SR_Flag"", ""STRING""),
                    bigquery.SchemaField(""Affiliated_base_number"", ""STRING""),
                ],
                    skip_leading_rows=1,
                    write_disposition=write_disposition_string,
                    autodetect=True,
                    source_format=""CSV"",
                )

        # Load the data into BigQuery 
        # Create a temporary file to prevent the exception- AttributeError: 'bytes' object has no attribute 'tell'""
        with tempfile.NamedTemporaryFile() as f:
            f.write(response.content)
            f.seek(0)

            job = client.load_table_from_file(
                f,
                table_id,
                location=""US"",
                job_config=job_config,
            )
            job.result()
            logging.info(""Data for month %d successfully loaded into table %s."", month, table_id)


    return 'Data loaded into table {}.'.format(table_id)
"
Data Engineering Zoomcamp FAQ;2023;Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage;"
could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)

This error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materizlied table and insert the fields with the timestamp data type.

Unable to run command (shown in video) to export ML model from BQ to GCS
Issue: Tried running command to export ML model from BQ to GCS from Week 3
bq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model
It is failing on following error:
BigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US
I verified the BQ data set and gcs bucket are in the same region- us-west1. Not sure how it gets location US. I couldn’t find the solution yet.
Solution:  Please enter correct project_id and gcs_bucket folder address. Mine gcs_bucket folder address is gs://dtc_data_lake_optimum-airfoil-376815/tip_model 
"
Data Engineering Zoomcamp FAQ;2023;What do I do if my VM runs out of space?;"Try deleting data you’ve saved to your VM locally during ETLs
Kill processes related to deleted files
Download ncdu and look for large files (pay particular attention to files related to Prefect)
If you delete any files related to Prefect, eliminate caching from your flow code
"
Data Engineering Zoomcamp FAQ;2023;What does it mean Stop with loading the files into a bucket.' Stop with loading the files into a bucket? In the homework;"
What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)
"
Data Engineering Zoomcamp FAQ;2023;When querying two different tables external and materialized you get the same result when count(distinct(*));"You need to uncheck cache preferences in query settings

"
Data Engineering Zoomcamp FAQ;2023;“bq: command not found” ;"Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list
You can also use bq.cmd instead of bq to make it work.
"
Data Engineering Zoomcamp FAQ;2023;  'NoneType' object is not iterable;"  > in macro test_accepted_values (tests/generic/builtin.sql)
 > called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)

Remember that you have to add to dbt_project.yml the vars:

vars:
  payment_type_values: [1, 2, 3, 4, 5, 6]

"
Data Engineering Zoomcamp FAQ;2023;All of sudden ssh stopped working for my VM after my last restart;"One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem. 
"
Data Engineering Zoomcamp FAQ;2023;Bad int64 value: 0.0 error;"
Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error, 
Solution:
Using safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:
 {{ dbt_utils.safe_cast('ehail_fee',  api.Column.translate_type(""integer""))}} as ehail_fee,"
Data Engineering Zoomcamp FAQ;2023;BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6;"Got to Account settings >> Project Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.
"
Data Engineering Zoomcamp FAQ;2023;BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.;"
R: After the second SELECT, change this line:
date_trunc('month', pickup_datetime) as revenue_month,
To this line:
date_trunc(pickup_datetime, month) as revenue_month,
Make sure that “month” isn’t surrounded by quotes!"
Data Engineering Zoomcamp FAQ;2023;BigQuery returns an error when i try to run ‘dbt run’: ;"My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data. 
When I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.

"
Data Engineering Zoomcamp FAQ;2023;CREATE TABLE has columns with duplicate name locationid.;"This error could result if you are using some select * query without mentioning the name of table for ex: 

with dim_zones as (
    select * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`
    where borough != 'Unknown'
),

fhv as (
    select * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`
)
select * from fhv
inner join dim_zones as pickup_zone
on fhv.PUlocationID = pickup_zone.locationid
inner join dim_zones as dropoff_zone
on fhv.DOlocationID = dropoff_zone.locationid
    );

To resolve just replace use : select fhv.* from fhv
  
"
Data Engineering Zoomcamp FAQ;2023;Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found);"
If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.


"
Data Engineering Zoomcamp FAQ;2023;Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment);"
Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.
Another thing to check is your .gitignore file. Make sure that the .csv extension is not included.

"
Data Engineering Zoomcamp FAQ;2023;Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found;"
Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db
Example: select * from {{ source('staging',<your table name in the database>') }}

"
Data Engineering Zoomcamp FAQ;2023;Could not parse the dbt project. please check that the repository contains a valid dbt project;"
Running the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.




"
Data Engineering Zoomcamp FAQ;2023;DBT Docs Served but Not Accessible via Browser;"Try removing the “network: host” line in docker-compose. 
"
Data Engineering Zoomcamp FAQ;2023;Data Type Error when running fact table;"If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.

Solution: try casting it to FLOAT datatype instead of NUMERIC
"
Data Engineering Zoomcamp FAQ;2023;"Data type errors when ingesting with parquet files
";"The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:
CREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`
OPTIONS (
  format = 'CSV',
  uris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']
);
As an example. You should no longer have any data type issues for week 4.
"
Data Engineering Zoomcamp FAQ;2023;Error thrown by format_to_parquet_task when converting fhv_tripdata_2020-01.csv using Airflow;"
R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:
pyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765
Cause: Some random line breaks in this particular file.
Fixed by opening a bash in the container executing the dag and manually running the following command that deletes all \n not preceded by \r.
perl -i -pe 's/(?<!\r)\n/\1/g' fhv_tripdata_2020-01.csv
After that, clear the failed task in Airflow to force re-execution.
"
Data Engineering Zoomcamp FAQ;2023;How to automatically infer the column data type (pandas missing value issues)?;"Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type. 
Solution: 
One way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.
However, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type
    # Make pandas to infer correct data type (as pandas parse int with missing as float)
    df.fillna(-999999, inplace=True)
    df = df.convert_dtypes()
    df = df.replace(-999999, None)
"
Data Engineering Zoomcamp FAQ;2023;How to set subdirectory of the github repository as the dbt project root;"There is a project setting which allows you to set `Project subdirectory` in dbt cloud:

"
Data Engineering Zoomcamp FAQ;2023;I changed location in dbt, but dbt run still gives me an error;"Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location
"
Data Engineering Zoomcamp FAQ;2023;I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.;"Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.
"
Data Engineering Zoomcamp FAQ;2023;If you have lost SSH access to your machine due to lack of space. Permission denied (publickey);"You can try to do this steps:

"
Data Engineering Zoomcamp FAQ;2023;Inconsistent number of rows when re-running fact_trips model;"This is due to the way the deduplication is done in the two staging files.

Solution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.

Explanation (a bit convoluted, feel free to clarify, correct etc.)

We partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.
"
Data Engineering Zoomcamp FAQ;2023;It appears that I can't edit the files because I'm in read-only. Does anyone know how I can change that?;"Create a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.

"
Data Engineering Zoomcamp FAQ;2023;I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?;"Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.

"
Data Engineering Zoomcamp FAQ;2023;Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.;"
Ensure you properly format your yml file."
Data Engineering Zoomcamp FAQ;2023;Made change to your modeling files and commit the your development branch, but Job still runs on on old file?;"Change to main branch, make a pull request from the development branch.
Note: this will take you to github.
Approve the merging and rerun you job, it would work as planned now


"
Data Engineering Zoomcamp FAQ;2023;Main branch is “read-only”;"Create a new branch to edit. More on this can be found here in the dbt docs.
"
Data Engineering Zoomcamp FAQ;2023;Not able to change Environment Type as it is greyed out and inaccessible;"
You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'

"
Data Engineering Zoomcamp FAQ;2023;Prefect Agent retrieving runs from queue sometimes fails with httpx.LocalProtocolError;"Error Message: 
Investigate Sentry error: ProtocolError ""Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED""
Solution: 
reference
Run it again because it happens sometimes. Or wait a few minutes, it will continue.

"
Data Engineering Zoomcamp FAQ;2023;Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:;"
Use the syntax below instead if the code in the tutorial is not working. 
dbt run --select stg_green_tripdata --vars '{""is_test_run"": false}'
"
Data Engineering Zoomcamp FAQ;2023;The - vars argument must be a YAML dictionary, but was of type str;"Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.
It should be:
dbt run --var 'is_test_run: false'
"
Data Engineering Zoomcamp FAQ;2023;Troubleshooting in dbt:;"The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.

"
Data Engineering Zoomcamp FAQ;2023;Unable to configure Continuous Integration (CI) with Github;"If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:
On Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith




Disconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.

Once we have confirmed the change, we can configure it again. This time, choose Github and it will appears all repositories which you have allowed to work with dbt. Select your repository and it’s ready.


Go to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:
"
Data Engineering Zoomcamp FAQ;2023;When You are getting error dbt_utils not found;"You need to create packages.yml file in main project directory and add packages’ meta data: 
ckage: dbt-labs/dbt_utils
   version: 0.8.0packages
:
 - pa

After creating file run: 
And hit enter.

"
Data Engineering Zoomcamp FAQ;2023;When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket;"If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):
We can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.
Instructions on how to download the CLI here: https://github.com/cli/cli
Commands to use:
gh auth login
gh release list -R DataTalksClub/nyc-tlc-data
gh release download yellow -R DataTalksClub/nyc-tlc-data
gh release download green -R DataTalksClub/nyc-tlc-data
etc.
Now you can upload the files to a GCS bucket using the GUI.
"
Data Engineering Zoomcamp FAQ;2023;When executing dbt run after fact_trips.sql has been created, the task failed with error: ;"R: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”
1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.
2. Add the related roles to the service account in use in GCS.

"
Data Engineering Zoomcamp FAQ;2023;When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated;"Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`

Fix:
Replace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata
"
Data Engineering Zoomcamp FAQ;2023;When executing dbt run after using fhv_tripdata as an external table: you get “Access Denied: BigQuery BigQuery: Permission denied”;"1. Go to your dbt cloud service account
1. Adding the  [Storage Object Admin,Storage Admin] role in addition to BigQuery Admin.
"
Data Engineering Zoomcamp FAQ;2023;When loading github repo raise exception that ‘taxi_zone_lookup’ not found;Seed files loaded from directory with name ‘seed’, thats why you should rename dir with name ‘data’ to ‘seed’
Data Engineering Zoomcamp FAQ;2023;When running your first dbt model, if it fails with an error: 404 Not found: Dataset was not found in location US404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1;"
R: Go to BigQuery, and check the location of BOTH
The source dataset (trips_data_all), and
The schema you’re trying to write to (name should be 	dbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))
Likely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.
Alternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy

 In DBT cloud you can actually specify the location using the following steps:
Go to your profile page (top right drop-down --> profile)
Then go to under Credentials --> Analytics (you may have customised this name)
Click on Bigquery >
Hit Edit
Update your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)"
Data Engineering Zoomcamp FAQ;2023;Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?;"It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:

{% macro generate_schema_name(custom_schema_name, node) -%}
    {%- set default_schema = target.schema -%}
    {%- if custom_schema_name is none -%}
        {{ default_schema }}
    {%- else -%}
        {{ custom_schema_name | trim }}
    {%- endif -%}
{%- endmacro %}

Now you can override default custom schema in “dbt_project.yml”:



"
Data Engineering Zoomcamp FAQ;2023;Why do my Fact_trips only contain a few days of data?;"Make sure you use:
 dbt run --var ‘is_test_run: false’ or dbt build --var ‘is_test_run: false’  (watch out for formatted text from this document: re-type the single quotes)
"
Data Engineering Zoomcamp FAQ;2023;Why do my fact_trips only contain one month of data?;"Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=""replace"" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=""append"" 
if_exists=""replace"" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)
if_exists=""append"" will append the new monthly data -> you end up with data from all months
"
Data Engineering Zoomcamp FAQ;2023;Why do we need the Staging dataset?;"Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?

R: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.
Vic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.
"
Data Engineering Zoomcamp FAQ;2023;‘taxi_zone_lookup’ not found;"Check the .gitignore file and make sure you don’t have *.csv in it

Dbt error 404 was not found in location
My specific error:
Runtime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c
Make sure all of your datasets have the correct region and not a generalised region:
Europe-west6 as opposed to EU

Match this in dbt settings:
dbt -> projects -> optional settings -> manually set location to match
"
Data Engineering Zoomcamp FAQ;2023;AttributeError: 'DataFrame' object has no attribute 'iteritems';"
This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,
because as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.
So when you run the command spark.createDataFrame(df1_pandas).show(),
You get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3

Another option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) : 
pd.DataFrame.iteritems = pd.DataFrame.items
"
Data Engineering Zoomcamp FAQ;2023;Compressed file ended before the end-of-stream marker was reached;"I solved this issue: unzip the file with:

!gzip -d fhvhv_tripdata_2021-01.csv.gz

 before creating head.csv
"
Data Engineering Zoomcamp FAQ;2023;Compression Error: zcat output is gibberish, seems like still compressed;"
In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.

✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work 

URL=""${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz""
   LOCAL_PREFIX=""data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}""
   LOCAL_FILE=""${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz""
   LOCAL_PATH=""${LOCAL_PREFIX}/${LOCAL_FILE}""

   echo ""downloading ${URL} to ${LOCAL_PATH}""
   mkdir -p ${LOCAL_PREFIX}
   wget ${URL} -O ${LOCAL_PATH}
  
   echo ""compressing ${LOCAL_PATH}""
   # gzip ${LOCAL_PATH} <- uncomment this line

"
Data Engineering Zoomcamp FAQ;2023;Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?;"Make sure you have your credentials of your GCP in your VM under the location defined in the script.
"
Data Engineering Zoomcamp FAQ;2023;DataType error when creating Spark DataFrame with a specified schema?;"Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video). 

When defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:
Parquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64
Change the schema definition from IntegerType to LongType and it should work"
Data Engineering Zoomcamp FAQ;2023;ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.;"Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:

gcloud dataproc jobs submit pyspark \
--cluster=my_cluster \ 
--region=us-central1 \
 --project=my-dtc-project-1010101 \
gs://my-dtc-bucket-id/code/06_spark_sql.py 
 -- \ 
	…
"
Data Engineering Zoomcamp FAQ;2023;Easy setup with miniconda env (worked on MacOS);"If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps

Install OpenJDK 11, 
on MacOS: $ brew install java11
Add export PATH=""/opt/homebrew/opt/openjdk@11/bin:$PATH""
to ~/.bashrc or ~/zshrc
Activate working environment (by pipenv / poetry / conda)
Run $ pip install pyspark
Work with exercises as normal

All default commands of spark will be also available at shell session under activated enviroment.

Hope this can help!

P.s. you won’t need findspark to firstly initialize.


Py4JJavaError: An error occurred while calling o35.csv. : java.net.ConnectException: Call From USERNAME/192.168.29.38 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; 
If you're getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You're most likely using compatible versions of the JDK or Python with Spark.

As of the current latest Spark version (3.3.2), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan!:

$ sdk list java
$ sdk install java 17.0.6-librca

More importantly, Python 3.11 is not yet stable for PySpark. So, make sure you're setting up your virtualenv with either Python 3.9 or Python 3.10

$ conda create -n ENV_NAME python=3.10  or
$ conda create -n ENV_NAME python=3.9

Followed by install pySpark properly witihin the virtualenv
$ conda activate ENV_NAME
$ pip install pyspark

"
Data Engineering Zoomcamp FAQ;2023;Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code;"
I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code

Instead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:


"
Data Engineering Zoomcamp FAQ;2023;Error java.io.FileNotFoundException;"Code executed:
df = spark.read.parquet(pq_path)
… some operations on df …
df.write.parquet(pq_path, mode=""overwrite"")

java.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist

The problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)

✅Solution: Write to a different directorydf
df.write.parquet(pq_path_temp, mode=""overwrite"")
"
Data Engineering Zoomcamp FAQ;2023;"Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z";"If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables. 
For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\bin” to the PATH variable.


Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io
"
Data Engineering Zoomcamp FAQ;2023;Exception: Jupyter command `jupyter-notebook` not found. ;"Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Notebook for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):

Full steps:
Update and upgrade packages:
sudo apt update && sudo apt -y upgrade
Install Python:
sudo apt install python3-pip python3-dev
Install Python virtualenv:
sudo -H pip3 install --upgrade pip
sudo -H pip3 install virtualenv
Create a Python Virtual Environment:
mkdir notebook
cd notebook
virtualenv jupyterenv
source jupyterenv/bin/activate
Install Jupyter Notebook:
pip install jupyter
Run Jupyter Notebook:
jupyter notebook
"
Data Engineering Zoomcamp FAQ;2023;Export PYTHONPATH command in linux is temporary;"
You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your jupyter notebook:

import findspark 
findspark.init()
"
Data Engineering Zoomcamp FAQ;2023;FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows);"You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .
"
Data Engineering Zoomcamp FAQ;2023;How can I read a small number of rows from the parquet file directly?;"from pyarrow.parquet import ParquetFile
pf = ParquetFile('fhvhv_tripdata_2021-01.parquet')
#pyarrow builds tables, not dataframes
tbl_small = next(pf.iter_batches(batch_size = 1000))
#this function converts the table to a dataframe of manageable size
df = tbl_small.to_pandas()

Alternatively without PyArrow:
df = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')
df1 = df.sort('DOLocationID').limit(1000)
pdf = df1.select(""*"").toPandas()
gcsu"
Data Engineering Zoomcamp FAQ;2023;How do you read data stored in gcs on pandas with your local computer?;"
To do this
pip install gcsfs,
Thereafter copy the uri path to the file and use 
df = pandas.read_csc(gs://path)
"
Data Engineering Zoomcamp FAQ;2023;How to port forward outside VS Code;"
I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1
"
Data Engineering Zoomcamp FAQ;2023;How to spark standalone cluster is run on windows OS;"
Change the working directory to the spark directory:
if you have setup up your SPARK_HOME variable, use the following;
cd %SPARK_HOME%    
if not, use the following;
cd <path to spark installation>

Creating a Local Spark Cluster

To start Spark Master:

bin\spark-class org.apache.spark.deploy.master.Master --host localhost

Starting up a cluster:

bin\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost
"
Data Engineering Zoomcamp FAQ;2023;Java.io.IOException. Cannot run program “C:\hadoop\bin\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.;"Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com) 
If this does not work try to change other versions found in this repository. 
For more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)

"
Data Engineering Zoomcamp FAQ;2023;MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory;"Default executor memory is 1gb. This error appeared when working with the homework dataset.


Solution:
Increase the memory of the executor when creating the Spark session like this:

Remember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.
"
Data Engineering Zoomcamp FAQ;2023;ModuleNotFoundError: No module named 'py4j';"This error was resolved by adding:
export PYTHONPATH=${SPARK_HOME}/python/:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):${PYTHONPATH}

To bashrc.
"
Data Engineering Zoomcamp FAQ;2023;ModuleNotFoundError: No module named 'py4j' (Solve with latest version);"If below does not work, then download the latest available py4j version with
conda install -c conda-forge py4j
Take care of the latest version number in the website to replace appropriately. 
Now add
export PYTHONPATH=""${SPARK_HOME}/python/:$PYTHONPATH""
export PYTHONPATH=""${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH""
in your  .bashrc file.

"
Data Engineering Zoomcamp FAQ;2023;ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`;"Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`. 
For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`. 
Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=""${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH""` appropriately.
Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.
"
Data Engineering Zoomcamp FAQ;2023;Nano b nanoModule Not Found Error in Jupyter Notebook .;"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook . 

The solution which worked for me(use following in jupyter notebook) :
!pip install findspark
import findspark
findspark.init()

Thereafter , import pyspark and create spark contex<<t as usual
None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.
"
Data Engineering Zoomcamp FAQ;2023;PicklingError: Could not serialise object: IndexError: tuple index out of range.;"Occurred while running : spark.createDataFrame(df_pandas).show() 
This error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.
"
Data Engineering Zoomcamp FAQ;2023;"Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.";"I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.
Solution:
 pip install findspark on the command line inside proper environment
Add to the top of the script
import findspark
findspark.init()"
Data Engineering Zoomcamp FAQ;2023;Run Local Cluster Spark in Windows 10 with CMD;"
Go to %SPARK_HOME%\bin 
Run spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port
Run spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.
Create a new Jupyter notebook:
spark = SparkSession.builder \
    .master(""spark://192.168.0.38:7077"") \
    .appName('test') \
    .getOrCreate()
 Check on Spark UI the master, worker and app.
"
Data Engineering Zoomcamp FAQ;2023;ServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).;"
This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:

gcloud auth login

This will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:

gcloud config set project <YOUR PROJECT_ID>

Then you can run the command to upload the pq dir to a GCS Bucket:

gsutil -m cp -r pq/ <YOUR URI from gsutil>/pq

"
Data Engineering Zoomcamp FAQ;2023;Spark BigQuery connector Automatic configuration;"While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2
spark = SparkSession.builder.master('local').appName('bq').config(""spark.jars.packages"", ""com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2"").getOrCreate() 

automatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here
"
Data Engineering Zoomcamp FAQ;2023;Spark Cloud Storage connector;"Link to Slack Thread 
"
Data Engineering Zoomcamp FAQ;2023;Spark Standalone Mode on Windows;"Open a CMD terminal in administrator mode
cd %SPARK_HOME%
Start a master node: bin\spark-class org.apache.spark.deploy.master.Master
Start a worker node: bin\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>
 spark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077
Use --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.
Now you can access Spark UI through localhost:8080

Homework for Week 5:
Do not refer to the homework file located under week_5_batch_processing. The correct file is located under cohorts/2023/week_5_batch_processing/.
https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2023/week_5_batch_processing/homework.md

The homework says we will be loading data from June 2021 HVFHV Data. 
This is very basic but it will save some time. 
"
Data Engineering Zoomcamp FAQ;2023;Spark docker-compose setup;"To run spark in docker setup
1. Build bitnamy spark docker
	a. clone bitnami repo using command
    	git clone https://github.com/bitnami/containers.git
    	(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)   	 
	b. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following
        	""python-3.10.10-2-linux-${OS_ARCH}-debian-11"" \
        	""java-17.0.5-8-3-linux-${OS_ARCH}-debian-11"" \

    	reference: https://github.com/bitnami/containers/issues/13409
	c. build docker image by navigating to above directory and running docker build command
    	navigate cd bitnami/spark/3.3/debian-11/
    	build command docker build -t spark:3.3-java-17 . 
2. run docker compose
	using following file
```yaml docker-compose.yml
version: '2'

services:
  spark:
	image: spark:3.3-java-17
	environment:
  	- SPARK_MODE=master
  	- SPARK_RPC_AUTHENTICATION_ENABLED=no
  	- SPARK_RPC_ENCRYPTION_ENABLED=no
  	- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  	- SPARK_SSL_ENABLED=no
	volumes:
  	- ""./:/home/jovyan/work:rw""
	ports:
  	- '8080:8080'
  	- '7077:7077'
  spark-worker:
	image: spark:3.3-java-17
	environment:
  	- SPARK_MODE=worker
  	- SPARK_MASTER_URL=spark://spark:7077
  	- SPARK_WORKER_MEMORY=1G
  	- SPARK_WORKER_CORES=1
  	- SPARK_RPC_AUTHENTICATION_ENABLED=no
  	- SPARK_RPC_ENCRYPTION_ENABLED=no
  	- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  	- SPARK_SSL_ENABLED=no
	volumes:
  	- ""./:/home/jovyan/work:rw""
	ports:
  	- '8081:8081'
  spark-nb:   
	image: jupyter/pyspark-notebook:java-17.0.5
	environment:
  	- SPARK_MASTER_URL=spark://spark:7077
	volumes:
  	- ""./:/home/jovyan/work:rw""
    
	ports:
  	- '8888:8888'
  	- '4040:4040'
```
run command to deploy docker compose
docker-compose up
Access jupyter notebook using link logged in docker compose logs
Spark master url is spark://spark:7077
"
Data Engineering Zoomcamp FAQ;2023;Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries;"
✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1
I also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.

To create the SparkSession:
spark = SparkSession.builder.master('local[*]') \
    .appName('spark-read-from-bigquery') \
    .config('BigQueryProjectId','razor-project-xxxxxxx) \
    .config('BigQueryDatasetLocation','de_final_data') \
    .config('parentProject','razor-project-xxxxxxx) \
    .config(""google.cloud.auth.service.account.enable"", ""true"") \
    .config(""credentialsFile"", ""google_credentials.json"") \
    .config(""GcpJsonKeyFile"", ""google_credentials.json"") \
    .config(""spark.driver.memory"", ""4g"") \
    .config(""spark.executor.memory"", ""2g"") \
    .config(""spark.memory.offHeap.enabled"",True) \
    .config(""spark.memory.offHeap.size"",""5g"") \
    .config('google.cloud.auth.service.account.json.keyfile', ""google_credentials.json"") \
    .config(""fs.gs.project.id"", ""razor-project-xxxxxxx"") \
    .config(""fs.gs.impl"", ""com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"") \
    .config(""fs.AbstractFileSystem.gs.impl"", ""com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"") \
    .getOrCreate()
"
Data Engineering Zoomcamp FAQ;2023;Spark-shell: unable to load native-hadoop library for platform - Windows;"If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:
java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed 
module @0x3c947bc5

Solution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file."
Data Engineering Zoomcamp FAQ;2023;The spark viewer on localhost:4040 was not showing the current run;"✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.
If port is in use, then Spark uses next. It can be even 4044. You can run 
spark.sparkContext.uiWebUrl
and result will be some like
'http://172.19.10.61:4041' 
"
Data Engineering Zoomcamp FAQ;2023;TypeError when using spark.createDataFrame function on a pandas df;"
Error:
spark.createDataFrame(df_pandas).schema
TypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>

Solution:

Affiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help: 

df = spark.read \
    .options( 
    header = ""true"", \
    inferSchema = ""true"", \
            ) \
    .csv('path/to/your/csv/file/')


Solution B:
It's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.

# Only take rows that have no null values
pandas_df= pandas_df[pandas_df.notnull().all(1)]


"
Data Engineering Zoomcamp FAQ;2023;TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11);"
This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it.
"
Data Engineering Zoomcamp FAQ;2023;Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?;"
Actually Spark SQL is one independent “type” of SQL - Spark SQL.
The several SQL providers are very similar:
SELECT [attributes]
FROM [table]
WHERE [filter]
GROUP BY [grouping attributes]
HAVING [filtering the groups]
ORDER BY [attribute to order]
(INNER/FULL/LEFT/RIGHT) JOIN [table2]
ON [attributes table joining table2] (...)

What differs the most between several SQL providers are built-in functions.
For Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html
Extra information on SPARK SQL :
https://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data. 
"
Data Engineering Zoomcamp FAQ;2023;`spark-submit` errors;"when trying to:
URL=""spark://$HOSTNAME:7077""
spark-submit \
    --master=""{$URL}"" \
    06_spark_sql.py \
         --input_green=data/pq/green/2021/*/ \
         --input_yellow=data/pq/yellow/2021/*/ \
         --output=data/report-2021
and you get errors like the following (SUMMARIZED):
WARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..
WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Setting default log level to ""WARN"".
Exception in thread ""main"" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at …

Try replacing --master=""{$URL}""
with --master=$URL (edited)

"
Data Engineering Zoomcamp FAQ;2023;has anyone figured out how to read from GCP data lake instead of downloading all the taxi lodata again?;"There’s a few extra steps to go into reading from GCS with PySpark

1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters
As the name implies, this .jar file is what essentially connects PySpark with your GCS

2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under ""/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)

3.) In your Python script, there are a few extra classes you’ll have to import:
import pyspark
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from pyspark.context import SparkContext

4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:
conf = SparkConf() \
    .setMaster('local[*]') \
    .setAppName('test') \
    .set(""spark.jars"", ""/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar"") \
    .set(""spark.hadoop.google.cloud.auth.service.account.enable"", ""true"") \
    .set(""spark.hadoop.google.cloud.auth.service.account.json.keyfile"", ""path/to/google_credentials.json"")

sc = SparkContext(conf=conf)

sc._jsc.hadoopConfiguration().set(""fs.AbstractFileSystem.gs.impl"",  ""com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"")
sc._jsc.hadoopConfiguration().set(""fs.gs.impl"", ""com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"")
sc._jsc.hadoopConfiguration().set(""fs.gs.auth.service.account.json.keyfile"", ""path/to/google_credentials.json"")
sc._jsc.hadoopConfiguration().set(""fs.gs.auth.service.account.enable"", ""true"")

5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:
spark = SparkSession.builder \
    .config(conf=sc.getConf()) \
    .getOrCreate()

6.) Finally, you’re able to read your files straight from GCS!
df_green = spark.read.parquet(""gs://{BUCKET}/green/202*/"")
"
Data Engineering Zoomcamp FAQ;2023;java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation);"✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.

RuntimeError: Java gateway process exited before sending its port number
Shows java_home is not set on the notebook log
https://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/
"
Data Engineering Zoomcamp FAQ;2023;lsRuntimeError: Java gateway process exited before sending its port number;"
After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook
import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master(""local[*]"") \
    .appName('test') \
    .getOrCreate()

df = spark.read \
    .option(""header"", ""true"") \
    .csv('taxi+_zone_lookup.csv')

df.show()

it gives the error: 
RuntimeError: Java gateway process exited before sending its port number

✅The solution (for me) was:
 pip install findspark on the command line and then
Add
import findspark
findspark.init()
to the top of the script. 
Another possible solution is:
Check that pyspark is pointing to the correct location. 
Run pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos. 
If it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content. 

To add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide
Once everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.
"
Data Engineering Zoomcamp FAQ;2023;py4j.protocol.Py4JJavaError  GCP;"
When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.
"
Data Engineering Zoomcamp FAQ;2023;“wc -l” is giving a different result then shown in the video;"
If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result. Unzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.

"
Data Engineering Zoomcamp FAQ;2023;Could not start docker image “control-center” from the docker-compose.yaml file.;On Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.
Data Engineering Zoomcamp FAQ;2023;Error importing cimpl dll when running avro examples;"ImportError: DLL load failed while importing cimpl: The specified module could not be found
... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:
from ctypes import CDLL
CDLL(""C:\\Users\\YOUR_USER_NAME\\anaconda3\\envs\\dtcde\\Lib\\site-packages\\confluent_kafka.libs\librdkafka-5d2e2910.dll"")
It seems that the error may occur depending on the OS and python version installed.

ALTERNATIVE:
ImportError: DLL load failed while importing cimpl

✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell. 
You need to set this DLL manually in Conda Env.
Source: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2
"
Data Engineering Zoomcamp FAQ;2023;Error while running python3 stream.py worker;"If you get an error while running the command python3 stream.py worker
Run pip uninstall kafka-python
Then run pip install kafka-python==1.4.6
"
Data Engineering Zoomcamp FAQ;2023;How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied;"If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub. 
In the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose"
Data Engineering Zoomcamp FAQ;2023;Kafka homwork Q3, there are options that support scaling concept more than the others:;"Ankush said we can focus on horizontal scaling option.
“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”
"
Data Engineering Zoomcamp FAQ;2023;Kafka- python videos have low audio and hard to follow up;"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.

Kafka Python Videos - Rides.csv
There is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv. 

"
Data Engineering Zoomcamp FAQ;2023;Module “kafka” not found when trying to run producer.py;"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.
To create a virtual env and install packages (run only once)

python -m venv env
source env/bin/activate
pip install -r ../requirements.txt
To activate it (you'll need to run it every time you need the virtual env):

source env/bin/activate

To deactivate it:

deactivate
This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)

Also the virtual environment should be created only to run the python file. Docker images should first all be up and running."
Data Engineering Zoomcamp FAQ;2023;ModuleNotFoundError: No module named 'avro';"✅SOLUTION: pip install confluent-kafka[avro]. 
For some reason, Conda also doesn't include this when installing confluent-kafka via pip.

More sources on Anaconda and confluent-kafka issues:
https://github.com/confluentinc/confluent-kafka-python/issues/590
https://github.com/confluentinc/confluent-kafka-python/issues/1221
https://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka
"
Data Engineering Zoomcamp FAQ;2023;Negsignal:SIGKILL while converting dta files to parquet format;"
Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that. 

Solution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.  
"
Data Engineering Zoomcamp FAQ;2023;data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing;"Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv  
"
Data Engineering Zoomcamp FAQ;2023;kafka.errors.NoBrokersAvailable: NoBrokersAvailable;"
If you have this error, it most likely that your kafka broker docker container is not working.
Use docker ps to confirm
Then in the docker compose yaml file folder, run docker compose up -d to start all the instances.
"
Machine Learning Zoomcamp FAQ;2023;Adding community notes;"You can create your own github repository for the course with your notes, homework, projects, etc.
Then fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.
After that's done, create a pull request to sync your fork with the original course repo. 

(By Wesley Barreto)
"
Machine Learning Zoomcamp FAQ;2023;Any particular hardware requirements for the course or everything is BentoMLmostly cloud? TIA! Couldn't really find this in the FAQ.;"For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).
(Rileen Sinha; based on response by Alexey on Slack) 
"
Machine Learning Zoomcamp FAQ;2023;Can I submit the homework after the due date?;"No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course. 
"
Machine Learning Zoomcamp FAQ;2023;Computing the hash for the leaderboard and project review;"
Leaderboard Links: 
2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml 
2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml 

Python Code:

from hashlib import sha1

def compute_hash(email):
    return sha1(email.lower().encode('utf-8')).hexdigest()

You need to call the function as follows:

print(compute_hash('YOUR_EMAIL_HERE'))

The quotes are required to denote that your email is a string.
(By Wesley Barreto)

You can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.

(Mélanie Fouesnard)

"
Machine Learning Zoomcamp FAQ;2023;How do I sign up?;"In the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo 
work"
Machine Learning Zoomcamp FAQ;2023;How long is the course?;"Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)
"
Machine Learning Zoomcamp FAQ;2023;How much Python should I know?;"Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the course too :)  

You can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis. 

(Mélanie Fouesnard)
"
Machine Learning Zoomcamp FAQ;2023;How much theory will you cover?;"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python

For example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.

"
Machine Learning Zoomcamp FAQ;2023;How much time do I need for this course?;"Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article
"
Machine Learning Zoomcamp FAQ;2023;How to setup TensorFlow with GPU support on Ubuntu?;Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/ 
Machine Learning Zoomcamp FAQ;2023;I don't know math. Can I take the course?;"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code. 
Here are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.

(Mélanie Fouesnard)
"
Machine Learning Zoomcamp FAQ;2023;I filled the form, but haven't received a confirmation email. Is it normal?;"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.

If you unsubscribed from our newsletter, you won't get course related updates too.

But don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.
"
Machine Learning Zoomcamp FAQ;2023;I just joined. What should I do next? How can I access course materials?;"Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.

Click on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.

Or you can just use this link: http://mlzoomcamp.com/#syllabus 
"
Machine Learning Zoomcamp FAQ;2023;Is it going to be live? When? ;"The course videos are pre-recorded, you can start watching the course right now.

We will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too. 
You can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.
"
Machine Learning Zoomcamp FAQ;2023;I’m new to Slack and can’t find the course channel. Where is it?;"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel 

Click “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.
Browse the list of public channels in your workspace, or use the search bar to search by channel name or description.
Select a channel from the list to view it.
Click Join Channel.

Do we need to provide the GitHub link to only our code corresponding to the homework questions?
Yes. You are required to provide the URL to your repo in order to receive a grade
"
Machine Learning Zoomcamp FAQ;2023;Submitting learning in public links ;"When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).

For posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points. 

The same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)

For midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.

"
Machine Learning Zoomcamp FAQ;2023;The course has already started. Can I still join it?;"Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.

In order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate. 
"
Machine Learning Zoomcamp FAQ;2023;The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021? ;"We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.

If you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.
"
Machine Learning Zoomcamp FAQ;2023;What are the deadlines in this course?;"For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)
"
Machine Learning Zoomcamp FAQ;2023;What if I miss a session?;"Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.
"
Machine Learning Zoomcamp FAQ;2023;What’s the difference between the previous iteration of the course (2022) and this one (2023)?;"There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different. 
"
Machine Learning Zoomcamp FAQ;2023;When does the next iteration start?;"The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).
"
Machine Learning Zoomcamp FAQ;2023;Will I get a certificate if I missed the midterm project?;"Yes, it's possible. See the previous answer.
"
Machine Learning Zoomcamp FAQ;2023;Will I get a certificate?;"Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.
"
Machine Learning Zoomcamp FAQ;2023;'403 Forbidden' error message when you try to push to a GitHub repository ;"Type the following command:

git config -l | grep url

The output should look like this:

remote.origin.url=https://github.com/github-username/github-repository-name.git

Change this to the following format and make sure the change is reflected using command in step 1:

git remote set-url origin ""https://github-username@github.com/github-username/github-repository-name.git""
									      (Added by Dheeraj Karra)"
Machine Learning Zoomcamp FAQ;2023;Conda Environment Setup;"With regards to creating an environment for the project, do we need to run the command ""conda create -n ......."" and ""conda activate ml-zoomcamp"" everytime we open vs code to work on the project?

Answer:
""conda create -n ...."" is just run the first time to create the environment. Once created, you just need to run ""conda activate ml-zoomcamp"" whenever you want to use it.
(Added by Wesley Barreto)

conda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml
"
Machine Learning Zoomcamp FAQ;2023;Conda is not an internal command;"I have a problem with my terminal. Command 

conda create -n ml-zoomcamp python=3.9

doesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine

If you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.

If you don’t have Anaconda or Miniconda, you should install it first

(Tatyana Mardvilko)

"
Machine Learning Zoomcamp FAQ;2023;Error launching Jupyter notebook ;"If you face an error kind of ImportError: cannot import name 'contextfilter' from 'jinja2' (anaconda\lib\site-packages\jinja2\__init__.py) when launching a new notebook for a brand new environment.

Switch to the main environment and run ""pip install nbconvert --upgrade"". 

Added by George Chizhmak
"
Machine Learning Zoomcamp FAQ;2023;Fatal: Authentication failed for 'https://github.com/username;"I had a problem when I tried to push my code from Git Bash:

remote: Support for password authentication was removed on August 13, 2021.
remote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.
fatal: Authentication failed for 'https://github.com/username

Solution:
Create a personal access token from your github account and use it when you make a push of your last changes. 

https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent 

Bruno Bedón"
Machine Learning Zoomcamp FAQ;2023;Floating Point Precision;"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:

Inverse * Original: 
[[ 1.00000000e+00 -1.38777878e-16]
 [ 3.16968674e-13  1.00000000e+00]]

Solution:
It's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken 
(Added by Wesley Barreto)

"
Machine Learning Zoomcamp FAQ;2023;How to avoid Value errors with array shapes in homework?;"First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!

Dimension Mismatch
To perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.
										Added by Leah Gotladera


Question 5: How and why do we replace the NaN values with average of the column?

You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.

This method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.

Added by Anneysha Sarkar

"
Machine Learning Zoomcamp FAQ;2023;How to identify the shape of dataset in Pandas;"There are many ways to identify the shape of dataset, one of them is using .shape attribute!

df.shape
df.shape[0] # for identify the number of rows
df.shape[1] # for identify the number of columns

Added by Radikal Lukafiardi"
Machine Learning Zoomcamp FAQ;2023;How to output only a certain number of decimal places ;"You can use round() function or f-strings
 round(number, 4)  - this will round number up to 4 decimal places
 print(f'Average mark for the Homework is {avg:.3f}') - using F string

Also there is pandas.Series. round idf you need to round values in the whole Series
Please check the documentation
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round
Added by Olga Rudakova

"
Machine Learning Zoomcamp FAQ;2023;How to select column by dtype;"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?  

df.select_dtypes(include=np.number).columns.tolist() 
df.select_dtypes(include='object').columns.tolist() 

Added by Gregory Morris"
Machine Learning Zoomcamp FAQ;2023;In case you are using mac os and having trouble with WGET;"Wget doesn't ship with macOS, so there are other alternatives to use.
No worries, we got curl:
example: 
curl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv   
Explanations:
curl: a utility for retrieving information from the internet.
-o: Tell it to store the result as a file.
filename: You choose the file's name.
Links: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.
More about it at:
Curl Documentation

Added by David Espejo
"
Machine Learning Zoomcamp FAQ;2023;NameError: name 'np' is not defined;"Pandas and numpy libraries are not being imported

NameError: name 'np' is not defined
NameError: name 'pd' is not defined


If you're using numpy or pandas, make sure you use the first few lines before anything else.

import pandas as pd
import numpy as np

Added by Manuel Alejandro Aponte
"
Machine Learning Zoomcamp FAQ;2023;Question 7: FINAL MULTIPLICATION not having 5 column;"This is most likely that you interchanged the first step of the multiplication
You used  instead of 
										Added by Emmanuel Ikpesu
"
Machine Learning Zoomcamp FAQ;2023;Question 7: Mathematical formula for linear regression;"In Question 7 we are asked to calculate 




The initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.

Additional reading and videos:
Ordinary least squares
Multiple Linear Regression in Matrix Form
Pseudoinverse Solution to OLS
 
Added by Sylvia Schmitt 
with commends from Dmytro Durach
"
Machine Learning Zoomcamp FAQ;2023;Question 7: Multiplication operators.;"Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).
numpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).
If multiplying by a scalar numpy.multiply() or * is preferred.

Added by Andrii Larkin
"
Machine Learning Zoomcamp FAQ;2023;Read-in the File in Windows OS;"How do I read the dataset with Pandas in Windows?

I used the code below but not working

df = pd.read_csv('C:\Users\username\Downloads\data.csv')

Unlike Linux/Mac OS, Windows uses the backslash (\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the '\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, ""\n"" to add a new line or ""\t"" to add spaces, etc. To avoid the issue we just need to add ""r"" before the file path and Python will treat it as a literal string (not an escape sequence).

Here’s how we should be loading the file instead:

df = pd.read_csv(r'C:\Users\username\Downloads\data.csv')

(Muhammad Awon)
"
Machine Learning Zoomcamp FAQ;2023;Retrieving csv inside notebook;"You can use

!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv

To download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . . 

For instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:
!mkdir -p ../data/
!mv housing.csv ../data/

"
Machine Learning Zoomcamp FAQ;2023;Setting up an environment using VS Code;"I found this video quite helpful: Creating Virtual Environment for Python from VS Code

[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks .
[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview  
                                                                                                                 (Added by Ivan Brigida)
"
Machine Learning Zoomcamp FAQ;2023;Singular Matrix Error;"I'm trying to invert the matrix but I got error that the matrix is singular matrix

The singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.
"
Machine Learning Zoomcamp FAQ;2023;Uploading the homework to Github;"This is my first time using Github to upload a code. I was getting the below error message when I type 

git push -u origin master:

error: src refspec master does not match any
error: failed to push some refs to 'https://github.com/XXXXXX/1st-Homework.git'

Solution:

The error message got fixed by running below commands:

git commit -m ""initial commit"" 
git push origin main

If this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart 

(Asia Saeed)

You can also use the “upload file” functionality from GitHub for that





If you write your code on Google colab you can also directly share it on your Github.





                                                                                                       (By Pranab Sarma)"
Machine Learning Zoomcamp FAQ;2023;What does pandas.DataFrame.info() do? ;"Answer:
It prints the information about the dataset like:
Index datatype
No. of entries
Column information with not-null count and datatype
Memory usage by dataset 

We use it as:

df.info()

(Added by Aadarsha Shrestha & Emoghena Itakpe)

"
Machine Learning Zoomcamp FAQ;2023;"Windows WSL and VS Code
If you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.";"(Tyler Simpson)

"
Machine Learning Zoomcamp FAQ;2023;wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1;"If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again
"
Machine Learning Zoomcamp FAQ;2023;wget is not recognized as an internal or external command;"If you get “wget is not recognized as an internal or external command”, you need to install it.

On Ubuntu, run

sudo apt-get install wget

On Windows, the easiest way to install wget is to use Chocolatey:

choco install wget

Or you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)

On Mac, the easiest way to install wget is to use brew.

Brew install wget

Alternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use 

python -m wget

You need to install it with pip first:

pip install wget

And then in your python code, for example in your jupyter notebook, use:

import wget
wget.download(""URL"")

This should download whatever is at the URL in the same directory as your code. 

(Memoona Tahira)

Alternatively, you can read a CSV file from a URL directly with pandas:

url = ""https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv""
df = pd.read_csv(url)

Valid URL schemes include http, ftp, s3, gs, and file.

In some cases you might need to bypass https checks:

import ssl
ssl._create_default_https_context = ssl._create_unverified_context


Or you can use the built-in Python functionality for downloading the files:

import urllib.request

url = ""https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv""

urllib.request.urlretrieve(url, ""housing.csv"")

Urllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.

The urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.

On any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.
                                                                                                               (Mohammad Emad Sharifi)

"
Machine Learning Zoomcamp FAQ;2023;wget: unable to resolve host address 'raw.githubusercontent.com';"
In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:

Getting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv
--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.
wget: unable to resolve host address 'raw.githubusercontent.com'

Solution:

In your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.


"
Machine Learning Zoomcamp FAQ;2023;'kind' is not recognized as an internal or external command,;"

operable program or batch file. (In Windows)
Problem: I download kind from the next command:
curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64

When I try
kind --version 
I get: 'kind' is not recognized as an internal or external command, operable program or batch file


Solution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH
Alejandro Aponte"
Machine Learning Zoomcamp FAQ;2023;Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”;"
As per AWS documentation: 

https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html

You need to do: (change the fields in red)

aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com

Alternatively you can run the following command without changing anything given you have a default region configured

aws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin ""$(aws sts get-caller-identity --query ""Account"" --output text).dkr.ecr.$(aws configure get region).amazonaws.com""

									Added by Humberto Rodriguez
"
Machine Learning Zoomcamp FAQ;2023;Connection refused;"
I ran into an issue where kubectl wasn't working.
I kept getting the following error:

kubectl get service
The connection to the server localhost:8080 was refused - did you specify the right host or port?

I searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.
All hogwash.

The solution to my problem was to just start over.

kind delete cluster
rm -rf ~/.kube
kind create cluster

Now when I try the same command again:

kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s


Added by Martin Uribe
"
Machine Learning Zoomcamp FAQ;2023;Could not install packages due to an OSError: [WinError 5] Access is denied ;"
When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error : 
ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Asia\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\_api_implementation.cp39-win_amd64.pyd'
Consider using the `--user` option or check the permissions.
 
Solution description : 
I was able to install the libraries using below command:
 pip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0

Asia Saeed

"
Machine Learning Zoomcamp FAQ;2023;Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac   ;"
While trying to run the docker code on M1:
docker run --platform linux/amd64 -it --rm \
 -p 8500:8500 \
 -v $(pwd)/clothing-model:/models/clothing-model/1 \
 -e MODEL_NAME=""clothing-model"" \
 tensorflow/serving:2.7.0
It outputs the error:
Error:
Status: Downloaded newer image for tensorflow/serving:2.7.0
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:
terminate called after throwing an instance of 'google::protobuf::FatalException'
 what():  CHECK failed: file != nullptr:
qemu: uncaught target signal 6 (Aborted) - core dumped
/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} ""$@""

Solution
docker pull emacski/tensorflow-serving:latest


docker run -it --rm \
 -p 8500:8500 \
 -v $(pwd)/clothing-model:/models/clothing-model/1 \
 -e MODEL_NAME=""clothing-model"" \
 emacski/tensorflow-serving:latest-linux_arm64

See more here: https://github.com/emacski/tensorflow-serving-arm 

Added by Daniel Egbo
"
Machine Learning Zoomcamp FAQ;2023;General issues with eksctl;"Make sure you are on AWS CLI v2 (check with aws --version)
https://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html

"
Machine Learning Zoomcamp FAQ;2023;Getting: Allocator ran out of memory errors?;"
If you are running tensorflow on your own machine and you start getting the following errors:

Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
 
Try adding this code in a cell at the beginning of your notebook:

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)

After doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.
 

Added by Martin Uribe

"
Machine Learning Zoomcamp FAQ;2023;HPA doesn’t show CPU metrics;"

Problem: CPU metrics Shows Unknown

NAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
credit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s


FailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:


Solution:
-> Delete HPA (kubectl delete hpa credit-hpa)
-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml
-> Create HPA

This should solve the cpu metrics report issue.

                                                                                 Added by Priya V
"
Machine Learning Zoomcamp FAQ;2023;HPA instance doesn’t run properly;"
In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:  
>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

And the targets still appear as <unknown> 

Run >>kubectl edit deploy -n kube-system metrics-server

And search for this line:
args:
 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname

Add this line in the middle:  - --kubelet-insecure-tls

So that it stays like this:
args:
 - --kubelet-insecure-tls
 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname

Save and run again >>kubectl get hpa 

Added by Marilina Orihuela
"
Machine Learning Zoomcamp FAQ;2023;HPA instance doesn’t run properly (easier solution);"
In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:  
>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

And the targets still appear as <unknown> 

Run the following command:
kubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml
Which uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.

Added by Giovanni Pecoraro
"
Machine Learning Zoomcamp FAQ;2023;How to get started with Week 10?;
Machine Learning Zoomcamp FAQ;2023;How to install Tensorflow in Ubuntu WSL2;"
Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware. 
 
I was able to get it working by using the following resources:

CUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)
Install TensorFlow with pip
Start Locally | PyTorch 

I included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.

Added by Martin Uribe
"
Machine Learning Zoomcamp FAQ;2023;How to install easily kubectl on windows ?;"
To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff
I first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows 
At step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.
Then I added this folder path to PATH in my environment variables.

Kind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.

Added by Mélanie Fouesnard
"
Machine Learning Zoomcamp FAQ;2023;Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)    ;"Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)
Problem:
While trying to run the docker code on Mac M2 apple silicon:
docker run --platform linux/amd64 -it --rm \
 -p 8500:8500 \
 -v $(pwd)/clothing-model:/models/clothing-model/1 \
 -e MODEL_NAME=""clothing-model"" \
 tensorflow/serving
You get an error:
/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} ""$@""

Solution:
Use bitnami/tensorflow-serving base image
Launch it either using docker run
docker run -d \
  --name tf_serving \
  -p 8500:8500 \
  -p 8501:8501 \
  -v $(pwd)/clothing-model:/bitnami/model-data/1 \
  -e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \
  bitnami/tensorflow-serving:2

Or the following docker-compose.yaml
version: '3'
services:
 tf_serving:
   image: bitnami/tensorflow-serving:2
   volumes:
     - ${PWD}/clothing-model:/bitnami/model-data/1
   ports:
     - 8500:8500
     - 8501:8501
   environment:
     - TENSORFLOW_SERVING_MODEL_NAME=clothing-model

And run it with
docker compose up
Added by Alex Litvinov"
Machine Learning Zoomcamp FAQ;2023;In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?;"
Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework. 


Pastor Soto"
Machine Learning Zoomcamp FAQ;2023;Install Kind via Go package;"If you are having challenges installing Kind through the Windows Powershell as provided on the website and Choco Library as I did, you can simply install Kind through Go. 

> Download and Install Go (https://go.dev/doc/install)

> Confirm installation by typing the following in Command Prompt -  go version

> Proceed by installing Kind by following this command - go install sigs.k8s.io/kind@v0.20.0

>Confirm Installation kind --version

It works perfectly.

"
Machine Learning Zoomcamp FAQ;2023;Install kind through choco library;"First you need to launch a powershell terminal with administrator privilege.
For this we need to install choco library first through the following syntax in powershell:

Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1')) 

Krishna Anand
"
Machine Learning Zoomcamp FAQ;2023;Kind cannot load docker image ;"Problem: Failing to load docker-image to cluster (when you’ved named a cluster)
kind load docker-image zoomcamp-10-model:xception-v4-001
ERROR: no nodes found for cluster ""kind""

Solution: Specify cluster name with -n
kind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001

Andrew Katoch
"
Machine Learning Zoomcamp FAQ;2023;Kubernetes-dashboard;"Deploy and Access the Kubernetes Dashboard

Luke"
Machine Learning Zoomcamp FAQ;2023;Problem with kind;"

Solution: run from the folder in which you put  kind.exe and use cmd. I have a problem with PowerShell and this command. 

OR after installing Kind on Windows using these guide: https://kind.sigs.k8s.io/docs/user/quick-start/

You may add folder where Kind.exe is located to your environment path using these guide: https://linuxhint.com/add-directory-to-path-environment-variables-windows/

Reboot your computer and you will have straight access to kind from any terminal

Edited Vitaly U  
"
Machine Learning Zoomcamp FAQ;2023;Problem with recent version of protobuf;"
In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:

TypeError: Descriptors cannot not be created directly.                                                                                               	 
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.                             	 
If you cannot immediately regenerate your protos, some other possible workarounds are:                                                               	 
 1. Downgrade the protobuf package to 3.20.x or lower.                                                                                               	 
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).                               	 
                                                                                                                                                     	 
More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

This will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:

pipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \    
           	keras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6

Added by Ángel de Vicente

"
Machine Learning Zoomcamp FAQ;2023;Regarding the ports which can be used or assign?;"

"
Machine Learning Zoomcamp FAQ;2023;Running kind on Linux with Rootless Docker or Rootless Podman;"Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).
Sylvia Schmitt
"
Machine Learning Zoomcamp FAQ;2023;Running out of storage after building many docker images;"Problem description
Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.

My first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.

Solution description
> docker images
revealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi 
a bunch of those — but to no avail!

It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run
> docker system prune
See also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind

Added by Konrad Mühlberg
"
Machine Learning Zoomcamp FAQ;2023;TypeError: Descriptors cannot not be created directly.;"
Problem description 
I was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :
 File ""C:\Users\Asia\Data_Science_Code\Zoompcamp\Kubernetes\gat.py"", line 9, in <module>
    from tensorflow_serving.apis import predict_pb2
  File ""C:\Users\Asia\.virtualenvs\Kubernetes-Ge6Ts1D5\lib\site-packages\tensorflow_serving\apis\predict_pb2.py"", line 14, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""C:\Users\Asia\.virtualenvs\Kubernetes-Ge6Ts1D5\lib\site-packages\tensorflow\core\framework\tensor_pb2.py"", line 14, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""C:\Users\Asia\.virtualenvs\Kubernetes-Ge6Ts1D5\lib\site-packages\tensorflow\core\framework\resource_handle_pb2.py"", line 14, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File ""C:\Users\Asia\.virtualenvs\Kubernetes-Ge6Ts1D5\lib\site-packages\tensorflow\core\framework\tensor_shape_pb2.py"", line 36, in <module>
    _descriptor.FieldDescriptor(
  File ""C:\Users\Asia\.virtualenvs\Kubernetes-Ge6Ts1D5\lib\site-packages\google\protobuf\descriptor.py"", line 560, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

Solution description:
Issue has been resolved by downgrading protobuf to version 3.20.1.
  pipenv install protobuf==3.20.1

Asia Saeed
"
Machine Learning Zoomcamp FAQ;2023;TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask ;"Problem Description: 
In video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.

Solution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask. 
By running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.  
Added by Bhaskar Sarma

"
Machine Learning Zoomcamp FAQ;2023;WSL Cannot Connect To Docker Daemon;"
Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:
”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”

Solution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:



Just enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro. 

Odimegwu David
"
Machine Learning Zoomcamp FAQ;2023;"Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does ""m"" mean?";"In Kubernetes resource specifications, such as CPU requests and limits, the ""m"" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.

cpu: ""100m"" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.
cpu: ""500m"" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.

These values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.

Added by Andrii Larkin"
Machine Learning Zoomcamp FAQ;2023;Errors with istio during installation;"
Problem description:
Running this:
curl -s ""https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh"" | bash
Fails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.
Check kubectl version with kubectl version

Solution description
Edit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.
Run the bash script now.

Added by Andrew Katoch


"
Machine Learning Zoomcamp FAQ;2023;Problem title;"
Problem description 
Solution description 

(optional) Added by Name



"
Machine Learning Zoomcamp FAQ;2023;Alternative way to load the data using requests;"Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:

# Get data for homework
import requests

url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'
response = requests.get(url)

if response.status_code == 200:
    with open('housing.csv', 'wb') as file:
        file.write(response.content)
else:
    print(""Download failed."")

Tyler Simpson"
Machine Learning Zoomcamp FAQ;2023;California housing dataset;"You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html 

KS
"
Machine Learning Zoomcamp FAQ;2023;Can I use LinearRegression from Scikit-Learn for this week?;"Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.
"
Machine Learning Zoomcamp FAQ;2023;Can I use Scikit-Learn’s train_test_split for this week?;"Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it
"
Machine Learning Zoomcamp FAQ;2023;Checking long tail of data;"We can use histogram:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'
df = pd.read_csv(url)

# EDA
sns.histplot(df['median_house_value'], kde=False)
plt.show()

OR ceck skewness and describe:
print(df['median_house_value'].describe())

# Calculate the skewness of the 'median_house_value' variable
skewness = df['median_house_value'].skew()

# Print the skewness value
print(""Skewness of 'median_house_value':"", skewness)"
Machine Learning Zoomcamp FAQ;2023;Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization);"What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.

Corresponding function for model without regularization:
sklearn.linear_model.LinearRegression
Corresponding function for model with regularization: 
sklearn.linear_model.Ridge

The linear model from Scikit-Learn are explained  here:
https://scikit-learn.org/stable/modules/linear_model.html
Added by Sylvia Schmitt
"
Machine Learning Zoomcamp FAQ;2023;Filter a dataset by using its values;"We can filter a dataset by using its values as below.

df = df[(df[""ocean_proximity""] == ""<1H OCEAN"") | (df[""ocean_proximity""] == ""INLAND"")]
You can use | for ‘OR’, and & for ‘AND’

Alternative:
df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]
Radikal Lukafiardi"
Machine Learning Zoomcamp FAQ;2023;Getting NaNs after applying .mean();"
I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan. 

I found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.
  
Added by Sasmito Yudha Husada

"
Machine Learning Zoomcamp FAQ;2023;How do I get started with Week 2?;"
Here are the crucial links for this Week 2 that starts September 18, 2023
Ask questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions
Calendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1
Week 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md
Submit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)
All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/
GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp
Youtube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12
FAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j 
								
~~Nukta Bhatia~~"
Machine Learning Zoomcamp FAQ;2023;How to copy a dataframe without changing the original dataframe?;"Copy of a dataframe is made with X_copy = X.copy().

This is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.

Any changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”. 
											
											(Memoona Tahira)"
Machine Learning Zoomcamp FAQ;2023;LinAlgError: Singular matrix;"It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.

You can also have an error because you did the inverse of X once in your code and you’re doing it a second time.
(Added by Cécile Guillot)
"
Machine Learning Zoomcamp FAQ;2023;Loading the dataset directly through Kaggle Notebooks;"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential

!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv

Once the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command
df = pd.read_csv('housing.csv')

Harish Balasundaram"
Machine Learning Zoomcamp FAQ;2023;Meaning of mean in homework 2, question 3;"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean? 

It means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean

df_train['column_name'].mean( )

Another option:

df_train[‘column_name’].describe()
(Bhaskar Sarma)

"
Machine Learning Zoomcamp FAQ;2023;Null column is appearing even if I applied .fillna();"
When creating a duplicate of your dataframe by doing the following:

X_train = df_train
X_val = df_val

You’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:

X_train = df_train.copy()
X_val = df_val.copy()

Added by Ixchel García

"
Machine Learning Zoomcamp FAQ;2023;Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?;"`r` is a regularization parameter.
It’s similar to `alpha` in sklearn.Ridge(), as both control the ""strength"" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here's how both are used:
sklearn.Ridge()
||y - Xw||^2_2 + alpha * ||w||^2_2
lesson’s notebook (`train_linear_regression_reg` function)
XTX = XTX + r * np.eye(XTX.shape[0])

`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.
"
Machine Learning Zoomcamp FAQ;2023;Random seed 42;"One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?

The purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.
"
Machine Learning Zoomcamp FAQ;2023;Reading the dataset directly from github;"The dataset can be read directly to pandas dataframe from the github link using the technique shown below

dfh=pd.read_csv(""https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv"")

Krishna Anand
"
Machine Learning Zoomcamp FAQ;2023;Shuffling the initial dataset using pandas built-in function;"It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:
Setting frac=1 will result in returning a shuffled version of the complete Dataset.
Setting random_state=seed will result in the same randomization as used in the course resources.

df_shuffled = df.sample(frac=1, random_state=seed)
df_shuffled.reset_index(drop=True, inplace=True)

Added by Sylvia Schmitt

"
Machine Learning Zoomcamp FAQ;2023;Target variable transformation;"
Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?

Only if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable. 

This can help to understand skewness and how it can be applied to the distribution of your data set. 

https://en.wikipedia.org/wiki/Skewness 

Pastor Soto"
Machine Learning Zoomcamp FAQ;2023;The answer I get for one of the homework questions doesn't match any of the options. What should I do?;"That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.

If it’s the case, just select the option that’s closest to your answer
"
Machine Learning Zoomcamp FAQ;2023;ValueError: shapes not aligned;"

If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.

If this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.


                                                                                                             (Santhosh Kumar)

"
Machine Learning Zoomcamp FAQ;2023;When should we transform the target variable to logarithm distribution?;"
When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work

"
Machine Learning Zoomcamp FAQ;2023;Why linear regression doesn’t provide a “perfect” fit?;"Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”

A: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:

As our model is linear, how would you draw a line to fit all the ""dots""?
You could ""fit"" all the ""dots"" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data. 

Added by Andrii Larkin
"
Machine Learning Zoomcamp FAQ;2023;AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names' ;"The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html 

George Chizhmak
"
Machine Learning Zoomcamp FAQ;2023;Coloring the background of the pandas.DataFrame.corr correlation matrix directly;"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.
Here an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used. 
# ensure to have only numerical values in the dataframe before calling 'corr'
corr_mat = df_numerical_only.corr()
corr_mat.style.background_gradient(cmap='viridis')

Here is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.
np.random.seed = 3
df_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))
df_random.style.background_gradient(cmap='viridis')

Added by Sylvia Schmitt


Identifying highly correlated feature pairs easily through unstack

data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))
data_corr.head(10)

						Added by Harish Balasundaram

You can also use seaborn to create a heatmap with the correlation. The code for doing that: 

sns.heatmap(df[numerical_features].corr(),
            annot=True,
            square=True,
            fmt="".2g"",
            cmap=""crest"")

Added by Cecile Guillot


You can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:

Which outputs, in the case of churn dataset:


(Mélanie Fouesnard)

"
Machine Learning Zoomcamp FAQ;2023;Convergence Problems in W3Q6;"Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge

Play with different scalers. See notebook-scaling-ohe.ipynb

Dmytro Durach

(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps."
Machine Learning Zoomcamp FAQ;2023;Correlation before or after splitting the data;"Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.

Answer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values. 
"
Machine Learning Zoomcamp FAQ;2023;Could not convert string to float:’Nissan’rt string to float: 'Nissan';"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.
To resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.
Here’s an example of how you can perform one-hot encoding using pandas:
import pandas as pd

# Assuming 'data' is your DataFrame and 'brand' is the column with car brands
data_encoded = pd.get_dummies(data, columns=['brand'])

In this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.


                                                                                                                -Mohammad Emad Sharifi-"
Machine Learning Zoomcamp FAQ;2023;Dealing with Convergence in Week 3 q6;"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:
Feature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a 	similar scale, preventing convergence issues.
 	
Categorical Feature Encoding: If your dataset includes categorical features, apply 	categorical encoding techniques such as OneHotEncoder (OHE) to 	convert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.
 	
Combine Features: After 	normalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.

By following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.
You can find an example here.
 												Osman Ali
"
Machine Learning Zoomcamp FAQ;2023;Encoding Techniques;"This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02

											Hrithik Kumar Advani
"
Machine Learning Zoomcamp FAQ;2023;Error in use of accuracy_score from sklearn in jupyter (sometimes) ;"
   I got this error multiple times here is the code: 
	“accuracy_score(y_val, y_pred >= 0.5)”
TypeError: 'numpy.float64' object is not callable

I solve it using 
from sklearn import metrics

metrics.accuracy_score(y_train, y_pred>= 0.5)

OMAR Wael"
Machine Learning Zoomcamp FAQ;2023;Feature elimination;"For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)? 

We should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.

If the difference is negative, it means that the model actually became better when we removed the feature.
"
Machine Learning Zoomcamp FAQ;2023;Features for homework Q5;"Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?

You need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.  


While calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?

Since order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)
"
Machine Learning Zoomcamp FAQ;2023;Features in Ridge Regression Model;"Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.
Drop all categorical features first before proceeding.
(Aileah Gotladera)

While it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so. 
												(Erjon)
"
Machine Learning Zoomcamp FAQ;2023;Fitting DictVectorizer on validation;"Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.

Edidiong Esu

Below is an extract of Alexey's book explaining this point. Hope is useful

When we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.

With this context, if we apply the fit to the validation model, we are ""giving the answers"" and we are not letting the ""fit"" do its job for data that we haven't seen. By not applying the fit to the validation model we can know how well it was trained.

Below is an extract of Alexey's book explaining this point.

Humberto Rodriguez

There is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.

The correct way is to fit_transform the train set, and only transform the validation and test sets.	

Memoona Tahira
"
Machine Learning Zoomcamp FAQ;2023;"FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2";"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning

Santhosh Kumar
"
Machine Learning Zoomcamp FAQ;2023;Handling Column Information for Homework 3 Question 6;"
You need to use all features. and price for target. Don't include the average variable we created before.
If you use DictVectorizer then make sure to use sparce=True to avoid convergence errors
I also used StandardScalar for numerical variable you can try running with or without this
												(Peter Pan)
"
Machine Learning Zoomcamp FAQ;2023;How  to Disable/avoid Warnings in Jupyter Notebooks;"The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:

Import warnings
warnings.filterwarnings(“ignore”)

  Krishna Anand"
Machine Learning Zoomcamp FAQ;2023;How do I get started with Week 3?;"
Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md
Submit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform
All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/
Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml
GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp
Youtube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29

~~Nukta Bhatia~~
"
Machine Learning Zoomcamp FAQ;2023;How to calculate Root Mean Squared Error? ;"We can use sklearn & numpy packages to calculate Root Mean Squared Error

from sklearn.metrics import mean_squared_error
import numpy as np

Rmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)

Added by Radikal Lukafiardi

You can also refer to Alexey’s notebook for Week 2:

https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb

which includes the following code:

def rmse(y, y_pred):
    error = y_pred - y
    mse = (error ** 2).mean()
    return np.sqrt(mse)

(added by Rileen Sinha)
"
Machine Learning Zoomcamp FAQ;2023;How to select the alpha parameter in Q6;"Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.

Answer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.

Asia Saeed
"
Machine Learning Zoomcamp FAQ;2023;If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.;"Added by Akshar Goyal
"
Machine Learning Zoomcamp FAQ;2023;Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way? ;"(Question by Connie S.)

The reason it's good/recommended practice to do it after splitting is to avoid data leakage - you don't want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on ""Common pitfalls and recommended practices"": https://scikit-learn.org/stable/common_pitfalls.html

Answered/added by Rileen Sinha

"
Machine Learning Zoomcamp FAQ;2023;Logistic regression crashing Jupyter kernel;"Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.

Make sure that the target variable for the logistic regression is binary.

Konrad Muehlberg
"
Machine Learning Zoomcamp FAQ;2023;Root Mean Squared Error;"To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.

from sklearn.metrics import mean_squared_error
rms = mean_squared_error(y_actual, y_predicted, squared=False)
See details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python

Ahmed Okka
"
Machine Learning Zoomcamp FAQ;2023;Second variable that we need to use to calculate the mutual information score ;"Question: Could you please help me with HW3 Q3: ""Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only."" What is the second variable that we need to use to calculate the mutual information score?

Answer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.


Asia Saeed
"
Machine Learning Zoomcamp FAQ;2023;Sparse matrix compared dense matrix;"A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.
The default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.
 												Quinn Avila
"
Machine Learning Zoomcamp FAQ;2023;Transforming Non-Numerical Columns into Numerical Columns;"Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.
"
Machine Learning Zoomcamp FAQ;2023;Understanding Ridge;"Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.

sag Solver: The sag solver stands for ""Stochastic Average Gradient."" It's particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.
Alpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting. 

from sklearn.linear_model import Ridge 
ridge = Ridge(alpha=alpha, solver='sag', random_state=42) 
ridge.fit(X_train, y_train)


Aminat Abolade"
Machine Learning Zoomcamp FAQ;2023;Use of random seed in HW3;"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?

Answer: for both splits random_state = 42 should be used

(Bhaskar Sarma)
"
Machine Learning Zoomcamp FAQ;2023;What data should be used for EDA?;"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?

It's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data

Alena Kniazeva

"
Machine Learning Zoomcamp FAQ;2023;What data should we use for correlation matrix;"
Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.

Yes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.

Pastor Soto
 "
Machine Learning Zoomcamp FAQ;2023;What is the better option FeatureHasher or DictVectorizer;"These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features. 

When you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.
You can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html

											Olga Rudakova
"
Machine Learning Zoomcamp FAQ;2023;What is the difference between OneHotEncoder and DictVectorizer?;"
Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.  

Both will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.
Tanya Mard

"
Machine Learning Zoomcamp FAQ;2023;What is the difference between pandas get_dummies and sklearn OnehotEncoder ? (Mélanie Fouesnard);"
They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]


"
Machine Learning Zoomcamp FAQ;2023;What’s the difference between using pandas.get_dummies() and sklearn’s DictVectorizer(sparse=False/True)?	What’s CSR format? Why does fitting in Q6 take so long and I get “ConvergenceWarning: The max_iter was reached which means the coef_ did not converge warnings.warn()”?;"pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:

DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).
Using “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.
Larkin Andrii"
Machine Learning Zoomcamp FAQ;2023;Why did we change the targets to binary format when calculating mutual information score in the homework?;" 
Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation. 

—Odimegwu David—-
"
Machine Learning Zoomcamp FAQ;2023;Are there other ways to compute Precision, Recall and F1 score?;"
Scikit-learn offers another way: precision_recall_fscore_support

Example:
from sklearn.metrics import precision_recall_fscore_support

precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)
					(Gopakumar Gopinathan)


When do I use ROC vs Precision-Recall curves?

- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.
- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.
-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.
- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.

					(Anudeep Vanjavakam)

"
Machine Learning Zoomcamp FAQ;2023;Compute Recall, Precision, and F1 Score using scikit-learn library;"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.

from sklearn.metrics import precision_score, recall_score, f1_score

precision_score(y_true, y_pred, average='binary')
recall_score(y_true, y_pred, average='binary')
f1_score(y_true, y_pred, average='binary')
Radikal Lukafiardi

"
Machine Learning Zoomcamp FAQ;2023;Dependence of the F-score on class imbalance;"Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.

(George Chizhmak)
"
Machine Learning Zoomcamp FAQ;2023;Difference between predict(X) and predict_proba(X)[:, 1];"In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.

The solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.

Vladimir Yesipov
Predict_proba shows probailites per class.
Ani Mkrtumyan
"
Machine Learning Zoomcamp FAQ;2023;Evaluate the Model using scikit learn metrics;"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.

from sklearn.metrics import (accuracy_score, 
   precision_score, 
 				   recall_score, 
                                              f1_score, 
   roc_auc_score
)

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred)
recall = recall_score(y_val, y_pred)
f1 = f1_score(y_val, y_pred)
roc_auc = roc_auc_score(y_val, y_pred)
 
print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1-Score: {f1}')
print(f'ROC AUC: {roc_auc}')

							(Harish Balasundaram)
"
Machine Learning Zoomcamp FAQ;2023;Help with understanding: “For each numerical value, use it as score and compute AUC”;"When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.
Sylvia Schmitt"
Machine Learning Zoomcamp FAQ;2023;How can I annotate a graph?;"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.
plt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\nOptimal F1 Score: {optimal_f1_score:.2f}',
xy=(optimal_threshold, optimal_f1_score),
xytext=(0.3, 0.5),
textcoords='axes fraction',
arrowprops=dict(facecolor='black', shrink=0.05))


Quinn Avila
"
Machine Learning Zoomcamp FAQ;2023;How do I get started with Week 4? ;"Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md
All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/
Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml
GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp
YouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40
Sci-Kit Learn on Evaluation:
https://scikit-learn.org/stable/model_selection.html


~~Nukta Bhatia~~
"
Machine Learning Zoomcamp FAQ;2023;How to evaluate feature importance for numerical variables with AUC?;"You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.

(Denys Soloviov)
"
Machine Learning Zoomcamp FAQ;2023;How to find the intercept between precision and recall curves by using numpy ?;"
You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):

I suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:

You want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):
 idx = np.argwhere(
        np.diff(
            np.sign(np.array(df_scores[""precision""]) - np.array(df_scores[""recall""]))
        )
    ).flatten()

You can print the result to easily read it:
print(
        f""The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx]['threshold']}.""
    )

(Mélanie Fouesnard)

"
Machine Learning Zoomcamp FAQ;2023;How to get all classification metrics?;"How to get classification metrics - precision, recall, f1 score, accuracy simultaneously

Use classification_report from sklearn. For more info check here.

Abhishek N
"
Machine Learning Zoomcamp FAQ;2023;I didn’t fully understand the ROC curve. Can I move on?;"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.

Nonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.

"
Machine Learning Zoomcamp FAQ;2023;I’m not getting the exact result in homework ;"That’s fine, use the closest option
"
Machine Learning Zoomcamp FAQ;2023;Monitoring Wait times and progress of the code execution can be done with:;"from tqdm.auto import tqdm

Tqdm - terminal progress bar


Krishna Anand

"
Machine Learning Zoomcamp FAQ;2023;Multiple thresholds for Q4;"
I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?

Choose the one closest to any of the options

Added by Azeez Enitan Edunwale

You can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.

Added by Rileen Sinha"
Machine Learning Zoomcamp FAQ;2023;Quick way to plot Precision-Recall Curve ;"We can import precision_recall_curve from scikit-learn and plot the graph as follows:


from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(y_val, y_predict)
plt.plot(thresholds, precision[:-1], label='Precision')
plt.plot(thresholds, recall[:-1], label='Recall')
plt.legend()

										Hrithik Kumar Advani"
Machine Learning Zoomcamp FAQ;2023;Use AUC to evaluate feature importance of numerical variables;"
Check the solutions from the 2021 iteration of the course. You should use roc_auc_score. 
"
Machine Learning Zoomcamp FAQ;2023;Using a variable to score;"https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119

Metrics can be used on a series or a dataframe
~~Ella Sahnan~~	"
Machine Learning Zoomcamp FAQ;2023;ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0;"Solution description: duplicating the

df.churn = (df.churn == 'yes').astype(int)

This is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.
It is telling us that it only contains 0's.
Delete one of the below cells and you will get the accuracy

Humberto Rodriguez
"
Machine Learning Zoomcamp FAQ;2023;ValueError: multi_class must be in ('ovo', 'ovr');"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.

I was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])

Asia Saeed
"
Machine Learning Zoomcamp FAQ;2023;What dataset should I use to compute the metrics in Question 3;"You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.
Diego Giraldo "
Machine Learning Zoomcamp FAQ;2023;What does KFold do?;"What does this line do?

KFold(n_splits=n_splits, shuffle=True, random_state=1)

If I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!

Did you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val). 

https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html

In my case changing random state changed results
(Arthur Minakhmetov)

Changing the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop
(Bhaskar Sarma)


In case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.
(Ani Mkrtumyan)

"
Machine Learning Zoomcamp FAQ;2023;What is Stratified k-fold? ;"For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.

Please check the realisation in sk-learn library:
https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold

Olga Rudakova

Precision-recall curve, and thus"
Machine Learning Zoomcamp FAQ;2023;What is the use of inverting or negating the variables less than the threshold ?;"Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.


Aileah Gotladera
"
Machine Learning Zoomcamp FAQ;2023;Why are FPR and TPR equal to 0.0, when threshold = 1.0?;"For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:

The threshold is 1.0
FPR is 0.0
And TPR is 0.0

When the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.

That is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0

Alena Kniazeva
"
Machine Learning Zoomcamp FAQ;2023;Why do I have different values of accuracy than the options in the homework?;"
One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.

Although the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.

1) 

df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)
df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)

2) 

df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)
df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)

Therefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.

Ibraheem Taha"
Machine Learning Zoomcamp FAQ;2023;Why do we sometimes use random_state and not at other times?;"
Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979

Refer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.
~~Ella Sahnan~~	"
Machine Learning Zoomcamp FAQ;2023;Why do we use cross validation?;"
Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.
""C"" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression. 
Smaller ""C"" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely. 
Larger ""C"" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.
Aminat Abolade"
Machine Learning Zoomcamp FAQ;2023;"
Completed creating the environment locally but could not find the environment on AWS.";"
Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.

Added by Edidiong Esu


How to use Docker for Windows? 
I installed the Docker- Desktop on Windows laptop, but I cannot use Docker Images

Ani Mkrtumyan


Installing waitress on Windows via GitBash: “waitress-serve” command not found
Running 'pip install waitress' as a command on GitBash was not downloading the executable file 'waitress-serve.exe'. You need this file to be able to run commands with waitress in Git Bash. To solve this:
open a Jupyter notebook and run the same command ' pip install waitress'. This way the executable file will be downloaded. The notebook may give you this warning : 'WARNING: The script waitress-serve.exe is installed in 'c:\Users\....\anaconda3\Scripts' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.'
Add the path where 'waitress-serve.exe' is installed into gitbash's PATH as such:
enter the following command in gitbash: nano ~/.bashrc
add the path to 'waitress-serve.exe' to PATH using this command: export PATH=""/path/to/waitress:$PATH""
close gitbash and open it again and you should be good to go
										Added by Bachar Kabalan
"
Machine Learning Zoomcamp FAQ;2023; Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.;"While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.

It is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:

https://www.youtube.com/watch?v=IXSiYkP23zo

Note that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).

Alternative ways are sketched here:
https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md
"
Machine Learning Zoomcamp FAQ;2023;'pipenv' is not recognized as an internal or external command, operable program or batch file.;"
This error happens because pipenv is already installed but you can't access it from the path.

This error comes out if you run.

pipenv  --version
pipenv shell

Solution for Windows

Open this option 

Click here

Click in Edit Button

Make sure the next two locations are on the PATH, otherwise, add it.
C:\Users\AppData\....\Python\PythonXX\
C:\Users\AppData\....\Python\PythonXX\Scripts\

Added by Alejandro Aponte

Note: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.

"
Machine Learning Zoomcamp FAQ;2023;AttributeError: module ‘collections’ has no attribute ‘MutableMapping’ ;"Following the instruction from video week-5.6, using pipenv to install python libraries throws below error


Solution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10. 
Added by Hareesh Tummala
"
Machine Learning Zoomcamp FAQ;2023;Basic Ubuntu Commands:;"Cd .. (go back)
Ls (see current folders)
Cd ‘path’/ (go to this path)
Pwd (home)
Cat “file name’ --edit txt file in ubuntu
Aileah Gotladera
"
Machine Learning Zoomcamp FAQ;2023;Bind for 0.0.0.0:9696 failed: port is already allocated;"
I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine. 
Error message:

Error response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.

Solution description 

Issue has been resolved running the following command:

docker kill $(docker ps -q)

https://github.com/docker/for-win/issues/2722 

Asia Saeed

"
Machine Learning Zoomcamp FAQ;2023;Bind for 127.0.0.1:5000 showing error;"I was getting the error on client side with this
Client Side:
File ""C:\python\lib\site-packages\urllib3\connectionpool.py"", line 703, in urlopen …………………..
 raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

Sevrer Side:
It showed error for gunicorn 
The waitress  cmd was running smoothly from server side

Solution:
Use the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times
		
												Aamir Wani



"
Machine Learning Zoomcamp FAQ;2023;Cannot connect to the docker daemon. Is the Docker daemon running?;"Working on getting Docker installed - when I try running hello-world I am getting the error. 

Docker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?
 
Solution description 

If you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).

On Linux, start the docker daemon with either of these commands:
sudo dockerd
sudo service docker start

Added by Ugochukwu Onyebuchi


"
Machine Learning Zoomcamp FAQ;2023;ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'));"Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.

(Theresa S.)
"
Machine Learning Zoomcamp FAQ;2023;Dockerfile missing when creating the AWS ElasticBean environment;"
I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env   
ERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.

I did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.

Added by Mélanie Fouesnard
"
Machine Learning Zoomcamp FAQ;2023;Dumping/Retrieving only the size of for a specific Docker image;"Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:
docker image ls <image name> 
Or alternatively:
docker images <image name>

In action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:
docker image ls --format ""{{.Size}}"" <image name>
Or alternatively:
docker images --format ""{{.Size}}"" <image name>
Sylvia Schmitt
 "
Machine Learning Zoomcamp FAQ;2023;Error building Docker images on Mac with M1 silicon;"Do you get errors building the Docker image on the Mac M1 chipset? 

The error I was getting was:

Could not open '/lib64/ld-linux-x86-64.so.2': No such file or directory

The fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

Open mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile
Replace line 1 with

FROM --platform=linux/amd64 ubuntu:latest

Now build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end. 

David Colton
"
Machine Learning Zoomcamp FAQ;2023;"Error: failed to compute cache key: ""/model2.bin"" not found: not found";"Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using

COPY [""model2.bin"", ""dv.bin"", ""./""]

then I got the error above in MINGW64 (git bash) on Windows.

The temporary solution I found was to use

COPY [""*"", ""./""]

which I assume combines all the files from the original docker image and the files in your working directory.

Added by Muhammed Tan
"
Machine Learning Zoomcamp FAQ;2023;Failed to read Dockerfile;"When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly. 

Added by Pastor Soto
"
Machine Learning Zoomcamp FAQ;2023;Failed to write the dependencies to pipfile and piplock file;"
Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file

Krishna Anand


"
Machine Learning Zoomcamp FAQ;2023;Fix error during installation of Pipfile inside Docker container;"

I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked

RUN pipenv install --system --deploy --ignore-pipfile
"
Machine Learning Zoomcamp FAQ;2023;Getting the same result;"While running the docker image if you get the same result check which model you are using.
Remember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.

										Added by Ahmed Okka"
Machine Learning Zoomcamp FAQ;2023;How do I copy files from a different folder into docker container’s working directory?;"You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:

In the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:

COPY [""src/predict.py"", ""models/xgb_model.bin"", ""./""]											Gopakumar Gopinathan"
Machine Learning Zoomcamp FAQ;2023;How do I copy files from my local machine to docker container?;"You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:

To copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:

docker cp /path/to/local/file_or_directory container_id:/path/in/container

											Hrithik Kumar Advani
"
Machine Learning Zoomcamp FAQ;2023;How do I debug a docker container?;"Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command. 

docker run -it --entrypoint bash <image>

If the container is already running, execute a command in the specific container:

docker ps (find the container-id)
docker exec -it <container-id> bash
(Marcos MJD)
"
Machine Learning Zoomcamp FAQ;2023;How do I get started with Week 5?;"
Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md
All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/
HW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb
Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml
GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp
YouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49

~~~ Nukta Bhatia ~~~"
Machine Learning Zoomcamp FAQ;2023;How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience;"You’ll need a kaggle account 
Go to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information
In the same location as your Jupyter NB, place the `kaggle.json` file
Run `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`
Make sure to import os via `import os` and then run:
os.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>
Finally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring` 
And then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`

>>> Michael Fronda <<<
"
Machine Learning Zoomcamp FAQ;2023;How to fix error after running the Docker run command;"

Solution
This error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following 
Running the following commands

docker ps -a <to list all docker containers>
docker images <to list images>
docker stop <container ID>
docker rm <container ID>
docker rmi image

I rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.
"
Machine Learning Zoomcamp FAQ;2023;How to install WSL on Windows 10 and 11 ?;"It is quite simple, and you can follow these instructions here:
https://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine 
Make sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.
In the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it
Once it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.
You are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.
To go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:

Python should be already installed but you can check it by running sudo apt install python3 command.
You can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo ""cd ../../mnt/your/folder/path"" >> ~/.bashrc
You can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc
You have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.
You can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).
You will need to install pip by running this command sudo apt install python3-pip

NB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):
/sbin/ldconfig.real: Can't link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1
/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

So I had to create the following symbolic link:
sudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so

(Mélanie Fouesnard)
"
Machine Learning Zoomcamp FAQ;2023;How to run a script while a web-server is working?;"Problem description:
I started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?

Solution description:
Just open another terminal (command window, powershell, etc.) and run a python script.
Alena Kniazeva

"
Machine Learning Zoomcamp FAQ;2023;I cannot pull the image with docker pull command;"Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:  

Using default tag: latest
Error response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown

Solution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:

docker pull svizor/zoomcamp-model:3.10.12-slim 

Added by Vladimir Yesipov
"
Machine Learning Zoomcamp FAQ;2023;I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video;"
I struggled with the command : 

eb init -p docker tumor-diagnosis-serving -r eu-west-1

Which resulted in an error when running : eb local run --port 9696
ERROR: NotSupportedError - You can use ""eb local"" only with preconfigured, generic and multicontainer Docker platforms. 

I replaced it with : 

eb init -p ""Docker running on 64bit Amazon Linux 2"" tumor-diagnosis-serving -r eu-west-1

This allowed the recognition of the Dockerfile and the build/run of the docker container.

Added by Mélanie Fouesnard
"
Machine Learning Zoomcamp FAQ;2023;Install docker on MacOS;"Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.
"
Machine Learning Zoomcamp FAQ;2023;Installing and updating to the python version 3.10 and higher;"Open terminal and type the code below to check the version on your laptop
python3 --version
For windows,
Visit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation
Run the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts

Or
For Python 3,
Open your command prompt or terminal and run the following command:
            pip install --upgrade python


Aminat Abolade
"
Machine Learning Zoomcamp FAQ;2023;Installing md5sum on Macos;"Install it by using command 

% brew install md5sha1sum 

Then run command to check hash for file to check if they the same with the provided

% md5sum model1.bin dv.bin
												Olga Rudakova

"
Machine Learning Zoomcamp FAQ;2023;Method to find the version of any install python libraries in jupyter notebook;"Import waitress
print(waitress.__version__)

Krishna Anand
"
Machine Learning Zoomcamp FAQ;2023;Module5 HW Question 6;"The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. ""model2.bin"", ""dv.bin""
Added by Quinn Avila

"
Machine Learning Zoomcamp FAQ;2023;"NotSupportedError - You can use ""eb local"" only with preconfigured, generic and multicontainer Docker platforms.";"Question:
When executing 
eb local run  --port 9696
I get the following error:
ERROR: NotSupportedError - You can use ""eb local"" only with preconfigured, generic and multicontainer Docker platforms.
Answer:
There are two options to fix this:
Re-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine). 
Edit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023
The disadvantage of the second approach is that the option might not be available the following years
Added by Alex Litvinov
"
Machine Learning Zoomcamp FAQ;2023;Python_version and Python_full_version error after running pipenv install:;" 
If you install packages via pipenv install, and get an error that ends like this: 

pipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}
python_full_version: 'python_version' must not be present with 'python_full_version'
python_version: 'python_full_version' must not be present with 'python_version'

Do this:

open Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed
Type pipenv lock to create the Pipfile.lock.
Done. Continue what you were doing
 "
Machine Learning Zoomcamp FAQ;2023;Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts');"After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it. 

It can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:
# for Windows
set VIRTUAL_ENV """"
# for Unix
export VIRTUAL_ENV=""""
Also manually re-creating removed folder at `C:\Users\username\.virtualenvs\removed-envname` can help, removed-envname can be seen at the error message.
Added by Andrii Larkin"
Machine Learning Zoomcamp FAQ;2023;Requests Error: No connection adapters were found for 'localhost:9696/predict'.;"You need to include the protocol scheme: 'http://localhost:9696/predict'.
Without the http:// part, requests has no idea how to connect to the remote server.
Note that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.

Added by George Chizhmak
"
Machine Learning Zoomcamp FAQ;2023;Running “pipenv install sklearn==1.0.2” gives errors. What should I do?;"
When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors. 

The solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment. 

Odimegwu David
Homework asks you to install 1.3.1
Pipenv install scikit-learn==1.3.1
Use Pipenv to install Scikit-Learn version 1.3.1
							Gopakumar Gopinathan"
Machine Learning Zoomcamp FAQ;2023;Terminal Used in Week 5 videos:;"https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO
					               Added by Dawuta Smit		"
Machine Learning Zoomcamp FAQ;2023;Testing HTTP POST requests from command line using curl;"I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. 
(Used with WSL2 on Windows, should also work on Linux and MacOS) 

curl --json '<json data>' <url>

# piping the structure to the command
cat <json file path> | curl --json @- <url>
echo '<json data>' | curl --json @- <url>

# example using piping
echo '{""job"": ""retired"", ""duration"": 445, ""poutcome"": ""success""}'\
    | curl --json @- http://localhost:9696/predict


Added by Sylvia Schmitt


"
Machine Learning Zoomcamp FAQ;2023;The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1;"After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.

In your Dockerfile, change the Python version in the first line the Python version installed in your system:

FROM python:3.7.5-slim

To find your python version, use the command python --version. For example:

python --version

>> Python 3.9.7

Then, change it on your Dockerfile:

FROM python:3.9.7-slim

Added by Filipe Melo
"
Machine Learning Zoomcamp FAQ;2023;The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash);"$ docker exec -it 1e5a1b663052 bash
the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'

Fix: 

winpty docker exec -it 1e5a1b663052 bash

A TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.
Winpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.

More info on terminal, shell, console applications hi and so on: 
https://conemu.github.io/en/TerminalVsShell.html

(Marcos MJD)

"
Machine Learning Zoomcamp FAQ;2023;Trying to run a docker image I built but it says it’s unable to start the container process;"Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal
"
Machine Learning Zoomcamp FAQ;2023;Version-conflict in pipenv;"Problem description:
In video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:

UserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.
 
Solution description:
When you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.
Bhaskar Sarma
"
Machine Learning Zoomcamp FAQ;2023;Warning: the environment variable LANG is not set!;"Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1
This is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:

https://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma

But one can proceed without addressing it.
Added by Abhirup Ghosh
"
Machine Learning Zoomcamp FAQ;2023;Where does pipenv create environments and how does it name them?;"It creates them in 

OSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash
Windows: C:\Users\<USERNAME>\.virtualenvs\folder-name_cyrptic-hash

Eg: C:\Users\Ella\.virtualenvs\code-qsdUdabf (for module-05 lesson)

The environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX. 

All libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.

(Memoona Tahira)
"
Machine Learning Zoomcamp FAQ;2023;Why do we need the --rm flag;"
What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?


For best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.
They consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.
The right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.
The option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.
During development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers. 

Added by Muhammad Awon
"
Machine Learning Zoomcamp FAQ;2023;You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.;"Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.     
                                                                         Added by 🅱🅻🅰🆀"
Machine Learning Zoomcamp FAQ;2023;docker  build ERROR [x/y] COPY …;"

Solution:
This error occurred because I used single quotes around the filenames. Stick to double quotes
"
Machine Learning Zoomcamp FAQ;2023;f-strings;"f-String not properly keyed in: does anyone knows why i am getting error after import pickle?

The first error showed up because your f-string is using () instead of {} around C. So, should be: f’model_C={C}.bin’
The second error as noticed by Sriniketh, your are missing one parenthesis it should be pickle.dump((dv, model), f_out)

(Humberto R.)
"
Machine Learning Zoomcamp FAQ;2023;https://saturncloud.io/blog/how-to-resolve-the-invalid-load-key-error-when-loading-a-pytorch-model-from-a-url/#:~:text=UnpicklingError%3A%20invalid%20load%20key%2C%20'%5Cx0a'%20error%20occurs%20when,load()%20function.Your Pipfile.lock (221d14) is out of date (during Docker build);"
If during running the  docker build command, you get an error like this:

Your Pipfile.lock (221d14) is out of date. Expected: (939fe0).
Usage: pipenv install [OPTIONS] [PACKAGES]...

ERROR:: Aborting deploy

Option 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command. 

Option 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:

pipenv  --rm
rm Pipfile*
"
Machine Learning Zoomcamp FAQ;2023;waitress-serve shows Malformed application ;"Question:
When running 
pipenv run waitress-serve --listen=localhost:9696 q4-predict:app 
I get the following:
There was an exception (ValueError) importing your module.
It had these arguments:
1. Malformed application 'q4-predict:app'
Answer:
Waitress doesn’t accept a dash in the python file name.
The solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py

Added by Alex Litvinov"
Machine Learning Zoomcamp FAQ;2023;Capture stdout for each iterations of a loop separately;"I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.
Using the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.

# This would be the content of the Jupyter Notebook cell
from IPython.utils.capture import capture_output
import sys

different_outputs = {}
for i in range(3):
    with capture_output(sys.stdout) as output:
        print(i)
        print(""testing capture"")
        different_outputs[i] = output.stdout

# different_outputs
# {0: '0\ntesting capture\n',
#  1: '1\ntesting capture\n',
#  2: '2\ntesting capture\n'}
  
Added by Sylvia Schmitt"
Machine Learning Zoomcamp FAQ;2023;Data Leakage;"
Filling in missing values using an entire dataset before splitting for training/testing/validation causes
"
Machine Learning Zoomcamp FAQ;2023;DictVectorizer feature names ;"The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.

 Quinn Avila
"
Machine Learning Zoomcamp FAQ;2023;Different values of auc, each time code is re-run;"When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.

Solution: try setting the random seed e.g

dt = DecisionTreeClassifier(random_state=22)

Bhaskar Sarma
"
Machine Learning Zoomcamp FAQ;2023;Does it matter if we let the Python file create the server or if we run gunicorn directly?;"They both do the same, it's just less typing from the script.

Asked by Andrew Katoch, Added by Edidiong Esu
They both do the same, it's just less typing from the script.
"
Machine Learning Zoomcamp FAQ;2023;Features Importance graph;"
I like this visual implementation of features importance in scikit-learn library:
https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html 

It actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.
Ivan Brigida
"
Machine Learning Zoomcamp FAQ;2023;How to Install Xgboost;"To install Xgboost, use the code below directly in your jupyter notebook:

(Pip 21.3+ is required)
pip install xgboost

You can update your pip by using the code below:
pip install --upgrade pip

For more about xgbboost and installation, check here:
https://xgboost.readthedocs.io/en/stable/install.html

Aminat Abolade"
Machine Learning Zoomcamp FAQ;2023;How to get started with Week 6?;"
Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md
All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/
HW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb
Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml
GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp
YouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57
FAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j

~~~Nukta Bhatia~~~"
Machine Learning Zoomcamp FAQ;2023;How to get the training and validation metrics from XGBoost?;"
During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.

We can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.




Added by Daniel Coronel
"
Machine Learning Zoomcamp FAQ;2023;How to solve regression problems with random forest in scikit-learn?;"
You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information. 

Alena Kniazeva
"
Machine Learning Zoomcamp FAQ;2023;Information Gain;"Information gain  in Y due to X, or the mutual information of Y and X

		

Where  is the entropy of Y. 

If X is completely uninformative about Y: 
If X is completely informative about Y: )

                                                                                  Hrithik Kumar Advani 
"
Machine Learning Zoomcamp FAQ;2023;No module named ‘ping’? ;"When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:

from [file name] import ping 

 Olga Rudakova
"
Machine Learning Zoomcamp FAQ;2023;One of the method to visualize the decision trees;"dot_data = tree.export_graphviz(regr, out_file=None, 
                                feature_names=boston.feature_names,  
                                filled=True)
graphviz.Source(dot_data, format=""png"")
                                                                                                                Krishna Anand

from sklearn import tree

tree.plot_tree(dt,feature_names=dv.feature_names_)

Added By Ryan Pramana"
Machine Learning Zoomcamp FAQ;2023;Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=);"If you’re getting TypeError: 
“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”, 
probably you’ve done this:
features = dv.get_feature_names_out()
It gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.

If you’re getting ValueError:
“ValueError: feature_names must be string, and may not contain [, ] or <”,
probably you’ve either done:
features = list(dv.get_feature_names_out())
or:
features = dv.feature_names_
reason is what you get from DictVectorizer here looks like this:
['households',
 'housing_median_age',
 'latitude',
 'longitude',
 'median_income',
 'ocean_proximity=<1H OCEAN',
 'ocean_proximity=INLAND',
 'population',
 'total_bedrooms',
 'total_rooms']
it has symbols XGBoost doesn’t like ([, ] or <).
What you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:
import re
features = dv.feature_names_
pattern = r'[\[\]<>]'
features = [re.sub(pattern, '  ', f) for f in features]
Added by Andrii Larkin"
Machine Learning Zoomcamp FAQ;2023;Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?;"
When rmse stops improving means, when it stops to decrease or remains almost similar.


Pastor Soto"
Machine Learning Zoomcamp FAQ;2023;RMSE using metrics.root_meas_square() ;"Instead of using np.sqrt() as the second step. You can extract it using like this way : 
mean_squared_error(y_val, y_predict_val,squared=False)
												Ahmed Okka
"
Machine Learning Zoomcamp FAQ;2023;Serialized Model Xgboost error;"Save model by calling ‘booster.save_model’, see eg 

Load model: 

Dawuta Smit"
Machine Learning Zoomcamp FAQ;2023;ValueError: Unknown label type: 'continuous';"
Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.

Alejandro Aponte

"
Machine Learning Zoomcamp FAQ;2023;ValueError: continuous format is not supported;"Calling roc_auc_score() to get auc is throwing the above error. 

Solution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument. 

roc_auc_score(y_train, y_pred) 

Hareesh Tummala
"
Machine Learning Zoomcamp FAQ;2023;ValueError: feature_names must be string, and may not contain [, ] or <;"
In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation 

Solution description 
The cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:
features= [i.replace(""=<"", ""_"").replace(""="",""_"") for i in features]

Asia Saeed

Alternative Solution:
In my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(""=<"", ""_"") should work as well.

For me this works:
features = []
for f in dv.feature_names_:
	string = f.replace(“=<”, “-le”)
	features.append(string)
Peter Ernicke


This error occurs because the list of feature names contains some characters like ""<"" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:
You can address this error by replacing problematic characters in the feature names with underscores, like so:
features = [f.replace('=<', '_').replace('=', '_') for f in features]

This code will go through the list of features and replace any instances of ""=<"" with """", as well as any ""="" with """", ensuring that the feature names only consist of supported characters.
"
Machine Learning Zoomcamp FAQ;2023;Visualize Feature Importance by using horizontal bar chart;"To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.
1. # extract the feature importances from the model
feature_importances = list(zip(features_names, rdr_model.feature_importances_))

importance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])

2. # sort descending the dataframe by using feature_importances value
importance_df = importance_df.sort_values(by='feature_importances', ascending=False)

3. # create a horizontal bar chart
plt.figure(figsize=(8, 6))
sns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')
plt.xlabel('Feature Importance')
plt.ylabel('Feature Names')
plt.title('Feature Importance Chart')
Radikal Lukafiardi
"
Machine Learning Zoomcamp FAQ;2023;What is eta in XGBoost;"
Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model. 

ETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.  

"
Machine Learning Zoomcamp FAQ;2023;What is the difference between bagging and boosting ?;"For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.
Random Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.
XGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.

Note that boosting is not necessarily better than bagging. 

Mélanie Fouesnard

Bagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.
Boosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.
Rileen
"
Machine Learning Zoomcamp FAQ;2023;`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.;"If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.


												Ali Osman"
Machine Learning Zoomcamp FAQ;2023;xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.;"
Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.
George Chizhmak"
Machine Learning Zoomcamp FAQ;2023;Can we use pytorch for this lesson/homework ?;"Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :
https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/ 
The functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!

Mélanie Fouesnard
"
Machine Learning Zoomcamp FAQ;2023;Checking GPU and CPU utilization using ‘nvitop’ ;"The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.
https://pypi.org/project//

Image source: https://pypi.org/project//
Added by Sylvia Schmitt
"
Machine Learning Zoomcamp FAQ;2023;Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?;"											
 It's fine, some small changes are expected
Alexey Grigorev
"
Machine Learning Zoomcamp FAQ;2023;Error with scipy missing module in SaturnCloud;"Problem:
I created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy 

Solution:
Install the module in a new cell: !pip install scipy
Restart the kernel and fit the model again

Added by Erick Calderin"
Machine Learning Zoomcamp FAQ;2023;Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model. ;"
Problem description:
When loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights. 

Solution description: 
Before loading model need to evaluate the model on input data: model.evaluate(train_ds)	

Added by Vladimir Yesipov"
Machine Learning Zoomcamp FAQ;2023;Getting error module scipy not found during model training in Saturn Cloud tensorflow image;"The above error happens since module scipy is not installed in the saturn cloud tensorflow image. While creating the Jupyter server resource, in the “Extra Packages” section under pip in the textbox write scipy. Below the textbox, the pip install scipy command will be displayed. This will ensure when the resource spins up, the scipy package will be automatically installed. This approach can also be followed for additional python packages. 

Sumeet Lalla"
Machine Learning Zoomcamp FAQ;2023;Getting error when connect git on Saturn Cloud: permission denied;"Problem description:
When follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`

Solution description: 
Alternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/

Added by Ryan Pramana"
Machine Learning Zoomcamp FAQ;2023;Host key verification failed.;"
Problem description:
Getting an error using <git clone git@github.com:alexeygrigorev/clothing-dataset-small.git>
The error:
Cloning into 'clothing-dataset'...
Host key verification failed.
fatal: Could not read from remote repository.
Please make sure you have the correct access rights
and the repository exists.

Solution description: 
when cloning the repo, you can also chose https - then it should work. This happens when you don't have your ssh key configured.
<git clone https://github.com/alexeygrigorev/clothing-dataset-small.git>
Added by Gregory Morris"
Machine Learning Zoomcamp FAQ;2023;How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:;"
The command to read folders in the dataset in the tensorflow source code is: 

for subdir in sorted(os.listdir(directory)):
	…

Reference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563

This means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.
 
When a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by: 

prob(class(0)) = 1- prob(class(1))

In case of using from_logits to get results, you will get two values for each of the labels.
 
A prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2. 

										(Added by Memoona Tahira)
"
Machine Learning Zoomcamp FAQ;2023;How do I push from Saturn Cloud to Github?;"Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:

Solution description: Follow the instructions in these github docs to create an SSH private and public key:
https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke
y-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui
Then the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.

Or alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:

Click on your username and on manage
Down below you will see the Git SSH keys section. 
Copy the default public key provided by Saturn Cloud
Paste these key into the SSH keys section of your github repo
Open a terminal on Saturn Cloud and run this command “ssh -T git@github.com”
You will receive a successful authentication notice.  

Odimegwu David
"
Machine Learning Zoomcamp FAQ;2023;How keras flow_from_directory know the names of classes in images?;"Problem:
When we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?

Solution:
The name of class is the folder name
If you just create some random folder with the name ""xyz"", it will also be considered as a class!! The name itself is saying flow_from_directory
a clear explanation below:
https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720

Added by Bhaskar Sarma 
"
Machine Learning Zoomcamp FAQ;2023;How to get started with Week 8?;
Machine Learning Zoomcamp FAQ;2023;How to install CUDA & cuDNN on Ubuntu 22.04;"In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn. 
The process can be overwhelming. Here’s a simplified guide
												Osman Ali

"
Machine Learning Zoomcamp FAQ;2023;How to unzip a folder with an image dataset and suppress output?;"Problem:
A dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output

Solution:
Execute the next cell:
%%capture
! unzip zipped_folder_name.zip -d destination_folder_name

Added by Alena Kniazeva


Inside a Jupyter Notebook:

import zipfile

local_zip = 'data.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('data')
zip_ref.close()

"
Machine Learning Zoomcamp FAQ;2023;How to upload kaggle data to Saturn Cloud?;"
Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.

You can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud. 
 
On your notebook run:

!pip install -q kaggle
 
Go to Kaggle website (you need to have an account for this):

Click on your profile image -> Account
Scroll down to the API box
Click on Create New API token

It will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder
 
On the notebook click on folder icon on the left upper corner
This will take you to the root folder
Click on the .kaggle folder
Once inside of the .kaggle folder upload the kaggle.json file that you downloaded
 
Run this command on your notebook:

!chmod 600 /home/jovyan/.kaggle/kaggle.json
 
Download the data using this command:

!kaggle datasets download -d agrigorev/dino-or-dragon
 
Create a folder to unzip your files:

!mkdir data
 
Unzip your files inside that folder

!unzip dino-or-dragon.zip -d data

Pastor Soto"
Machine Learning Zoomcamp FAQ;2023;How to use Google Colab for Deep Learning?;"Create or import your notebook into Google Colab.
Click on the Drop Down at the top right hand side
Click on “Change runtime type”
Choose T4 GPU


                                                                                                                                Khurram Majeed
"
Machine Learning Zoomcamp FAQ;2023;How to use Kaggle for Deep Learning?;"Create or import your notebook into Kaggle.
Click on the Three dots at the top right hand side
Click on Accelerator
Choose T4 GPU


                                                                                                                                Khurram Majeed"
Machine Learning Zoomcamp FAQ;2023;Keras model training fails with “Failed to find data adapter”;"While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model

train_gen = ImageDataGenerator(rescale=1./255)

train_ds = train_gen.flow_from_directory(…)

history_after_augmentation = model.fit(
   train_gen, # this should be train_ds!!!
   epochs=10,
   validation_data=test_gen # this should be test_ds!!!
)

The fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory

Added by Tzvi Friedman
"
Machine Learning Zoomcamp FAQ;2023;Missing channel value error while reloading model:;"While doing:

import tensorflow as tf
from tensorflow import keras
model = tf.keras.models.load_model('model_saved.h5')

If you get an error message like this:

ValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.

Solution:

Saving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:

# model architecture:

inputs = keras.Input(shape=(input_size, input_size, 3))
base = base_model(inputs, training=False)
vectors = keras.layers.GlobalAveragePooling2D()(base)
inner = keras.layers.Dense(size_inner, activation='relu')(vectors)
drop = keras.layers.Dropout(droprate)(inner)
outputs = keras.layers.Dense(10)(drop)
model = keras.Model(inputs, outputs)


(Memoona Tahira)
"
Machine Learning Zoomcamp FAQ;2023;Model breaking after augmentation – high loss + bad accuracy;"Problem:
When resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.

Solution:
Check that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.

Added by Konrad Mühlberg

"
Machine Learning Zoomcamp FAQ;2023;Model training very slow in google colab with T4 GPU;"When training the models, in the fit function, you can specify the number of workers/threads.
The number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1. 
I changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu) 
										       
 Added by Ibai Irastorza
"
Machine Learning Zoomcamp FAQ;2023;Out of memory errors when running tensorflow;"
I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.
https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth

```
physical_devices = tf.configlist_physical_devices('GPU')

try:
  tf.config.experimental.set_memory_growth(physical_devices[0],True)

except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass
```
"
Machine Learning Zoomcamp FAQ;2023;Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?;"Let’s say we define our Conv2d layer like this:
>> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))
It means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.
If we check model.summary() we will get this:
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 148, 148, 32)      896       

So where does 896 params come from? It’s computed like this:
>>> (3*3*3 +1) * 32
896
# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters

What about the number of “features” we get after the Flatten layer?
For our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
max_pooling2d_3       (None, 7, 7, 128)         0
flatten (Flatten)           (None, 6272)              0

So where do 6272 vectors come from? It’s computed like this:
>>> 7*7*128
6272
# 7x7 “image shape” after several convolutions and poolings, 128 filters
Added by Andrii Larkin"
Machine Learning Zoomcamp FAQ;2023;Reproducibility with TensorFlow using a seed point;"Reproducibility for training runs can be achieved following these instructions: 
https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism

seed = 1234
tf.keras.utils.set_random_seed(seed)
tf.config.experimental.enable_op_determinism()

This will work for a script, if this gets executed multiple times.

Added by Sylvia Schmitt

"
Machine Learning Zoomcamp FAQ;2023;Running ‘nvidia-smi’ in a loop without using ‘watch’;"The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.
nvidia-smi -l <N seconds>

The following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.
nvidia-smi -l 2
Added by Sylvia Schmitt"
Machine Learning Zoomcamp FAQ;2023;Sequential vs. Functional Model Modes in Keras (TF2);"It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class). 

You can simply start from an “empty” model and add more and more layers in a sequential order. 
This mode is called “Sequential Model API”  (easier)

In Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”. 
Maybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.

You can read more about it in this TF2 tutorial.
A really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook 
   
										        Added by Ivan Brigida

Fresh Run on Neural Nets

While correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.

Added by Abhijit Chakraborty

"
Machine Learning Zoomcamp FAQ;2023;The same accuracy on epochs;"Problem description 
The accuracy and the loss are both still the same or nearly the same while training.

Solution description
In the homework, you should set class_mode='binary' while reading the data. 
Also, problem occurs when you choose the wrong optimizer, batch size, or learning rate 

Added by Ekaterina Kutovaia



"
Machine Learning Zoomcamp FAQ;2023;Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images;"From the keras documentation:
Deprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.
 
 											Hrithik Kumar Advani



	"
Machine Learning Zoomcamp FAQ;2023;Using multi-threading for data generation in “model.fit()”;"When running “model.fit(...)” an additional parameter “workers” can be specified for speeding up the data loading/generation. The default value is “1”. Try out which value between 1 and the cpu count on your system performs best.
https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit
Added by Sylvia Schmitt

"
Machine Learning Zoomcamp FAQ;2023;What if your accuracy and std training loss don’t match HW? ;"Problem:
I found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used. 

Solution:
Try running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU 

Added by Quinn Avila
"
Machine Learning Zoomcamp FAQ;2023;Where is the Python TensorFlow template on Saturn Cloud?;"This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud
but the location shown in the video is no longer correct. 

This template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.

Steven Christolis
"
Machine Learning Zoomcamp FAQ;2023;"""Unable to import module 'lambda_function': No module named 'tensorflow'"" when run python test.py";"Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite

Added by Ryan Pramana
"
Machine Learning Zoomcamp FAQ;2023;Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”;see here
Machine Learning Zoomcamp FAQ;2023;Docker Temporary failure in name resolution;"Add the next lines to vim /etc/docker/daemon.json

{
  ""dns"": [""8.8.8.8"", ""8.8.4.4""]
}

Then, restart docker:  sudo service docker restart


									Ibai Irastorza
"
Machine Learning Zoomcamp FAQ;2023;Docker run error ;"docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.

You need to restart the docker services to get rid of the above error

Krishna Anand
"
Machine Learning Zoomcamp FAQ;2023;Error building docker image on M1 Mac;"Problem:
While trying to build docker image in Section 9.5 with the command:
docker build -t clothing-model .
It throws a pip install error for the tflite runtime whl
ERROR: failed to solve: process ""/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl"" did not complete successfully: exit code: 1

Try to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl
If the link above does not work:
The problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS. 
Or try the code bellow. 

Added by Dashel Ruiz Perez

Problem:
While trying to build docker image in Section 9.5 with the command:
docker build -t clothing-model .
It throws a pip install error for the tflite runtime whl
#6 0.528 ERROR: tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl is not a supported wheel on this platform
Solution:
To build the Docker image, use the command:
docker build --platform linux/amd64 -t clothing-model .
To run the built image, use the command:
docker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest
Added by Daniel Egbo
"
Machine Learning Zoomcamp FAQ;2023;Error invoking API Gateway deploy API locally;"Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py
With error message:
{'message': 'Missing Authentication Token'}
Solution:
Need to get the deployed API URL for the specific path you are invoking. Example:
https://<random string>.execute-api.us-east-2.amazonaws.com/test/predict

Added by Andrew Katoch
"
Machine Learning Zoomcamp FAQ;2023;Error with the line “interpreter.set_tensor(input_index, X”);"I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.
ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0

This is because the X is an int but a float is expected.

Solution:
I found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :

# Need to convert to float32 before set_tensor
X = np.float32(X)
Then, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?

Added by Mélanie Fouesnard
"
Machine Learning Zoomcamp FAQ;2023;Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none);"Problem: When trying to install tflite_runtime with
!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime
one gets an error message above.

Solution:
fflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/ 
your combination must be missing here
you can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite 
and install the needed one using pip
eg
pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl
as it is done in the lectures code:
https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4

Alternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).

Added by Alena Kniazeva, modified by Alex Litvinov
"
Machine Learning Zoomcamp FAQ;2023;Exertfgcuting the command echo ${REMOTE_URI} returns nothing.;"Solution description 

In the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.

I also had the same problem on Ubuntu terminal. I executed the following two commands:

$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001
$ echo $REMOTE_URI
111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001
Note: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,
2. Replace REMOTE_URI with your URI

(Bhaskar Sarma)
"
Machine Learning Zoomcamp FAQ;2023;Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8 ;"
This error is produced sometimes when building your docker image from the Amazon python base image. 
 
Solution description: The following could solve the problem. 

Update your docker desktop if you haven’t done so. 
Or restart docker desktop and terminal and then build the image all over again. 
Or if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image. 

(optional) Added by Odimegwu David

"
Machine Learning Zoomcamp FAQ;2023;Getting a syntax error while trying to get the password from aws-cli;"
The command aws ecr get-login --no-include-email returns an invalid choice error:
  
The solution is to use the following command instead:  aws ecr get-login-password
Could simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:

export PASSWORD=`aws ecr get-login-password`
docker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images


Added by Martin Uribe
"
Machine Learning Zoomcamp FAQ;2023;How do Lambda container images work?;"
I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation

https://docs.aws.amazon.com/lambda/latest/dg/images-create.html
https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html

Added by Alejandro aponte

"
Machine Learning Zoomcamp FAQ;2023;How to do AWS configure after installing awscli;"
Problem description:
In video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay? 

Solution description:
Yes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)

Added by Bhaskar Sarma"
Machine Learning Zoomcamp FAQ;2023;How to easily get file size in powershell terminal ?;"To check your file size using the powershell terminal, you can do the following command lines:
$File = Get-Item -Path path_to_file
$FileSize = (Get-Item -Path $FilePath).Length

Now you can check the size of your file, for example in MB:
Write-host ""MB"":($FileSize/1MB)

Source: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size. 

Added by Mélanie Fouesnard
"
Machine Learning Zoomcamp FAQ;2023;How to get started with Week 9?;
Machine Learning Zoomcamp FAQ;2023;How to test AWS Lambda + Docker locally?;"This deployment setup can be tested locally using AWS RIE (runtime interface emulator).
Basically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:
docker run -it --rm -p 9000:8080 name
This command runs the image as a container and starts up an endpoint locally at: 
localhost:9000/2015-03-31/functions/function/invocations
Post an event to the following endpoint using a curl command:
curl -XPOST ""http://localhost:9000/2015-03-31/functions/function/invocations"" -d '{}'
Examples of curl testing:
* windows testing:
curl -XPOST ""http://localhost:9000/2015-03-31/functions/function/invocations"" -d ""{\""url\"": \""https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\""}""
* unix testing:
curl -XPOST ""http://localhost:9000/2015-03-31/functions/function/invocations"" -d '{""url"": ""https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg""}'

If during testing you encounter an error like this:
# {""errorMessage"": ""Unable to marshal response: Object of type float32 is not JSON serializable"", ""errorType"": ""Runtime.MarshalError"", ""requestId"": ""7ea5d17a-e0a2-48d5-b747-a16fc530ed10"", ""stackTrace"": []}
just turn your response at lambda_handler() to string - str(result).
Added by Andrii Larkin
"
Machine Learning Zoomcamp FAQ;2023;How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?;"
The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.

https://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d


Added by Sumeet Lalla

"
Machine Learning Zoomcamp FAQ;2023;"ImportError: generic_type: type ""InterpreterWrapper"" is already registered!";"
When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type ""InterpreterWrapper"" is already registered!”

Solution description 

This error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter "" import tflite_runtime.interpreter as tflite"".

Asia Saeed

"
Machine Learning Zoomcamp FAQ;2023;Install Docker (udocker) in Google Colab;"I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab: 
https://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885 


%%shell
pip install udocker
udocker --allow-root install
!udocker --allow-root run hello-world
Added by Ivan Brigida


Lambda API Gateway errors:
`Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header.`
`Missing Authentication Token`

import boto3

client = boto3.client('apigateway')
response = client.test_invoke_method(
    restApiId='your_rest_api_id',
    resourceId='your_resource_id',
    httpMethod='POST',
    pathWithQueryString='/test/predict', #depend how you set up the api
    body='{""url"": ""https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg""}'
)

print(response['body'])

                                                                                        Yishan Zhan

Unable to run pip install tflite_runtime from github wheel links?
To overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:

COPY <file-name> .
RUN pip install <file-name>

Abhijit Chakraborty"
Machine Learning Zoomcamp FAQ;2023;Jupyter notebook not seeing package ;"On vscode running jupyter notebook. After I ‘pip install pillow’, my notebook did not recognize using the import for example from PIL import image. After restarting the jupyter notebook the imports worked. 
Quinn Avila
"
Machine Learning Zoomcamp FAQ;2023;Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer` ;"Solution: add compile = False to the load_model function
keras.models.load_model('model_name.h5', compile=False)
	Nadia Paz"
Machine Learning Zoomcamp FAQ;2023;Object of type float32 is not JSON serializable;"Problem: 
While passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like

{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}

This happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.
 
Solution:
In my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):
preds = [interpreter.get_tensor(output_index)[0][0], \
                1-interpreter.get_tensor(output_index)[0][0]]
In which case the above described solution will look like this:
preds = [float(interpreter.get_tensor(output_index)[0][0]), \
             float(1-interpreter.get_tensor(output_index)[0][0])]

The rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.

Added by Konrad Muehlberg

"
Machine Learning Zoomcamp FAQ;2023;Pass many parameters in the model at once;"We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.

Krishna Anand
"
Machine Learning Zoomcamp FAQ;2023;Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.;"
When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.

 
Solution description : 
Instead of !ls -lh , you can use this command !dir , and you will get similar output



Asia Saeed
"
Machine Learning Zoomcamp FAQ;2023;Running out of space for AWS instance.;"Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune

"
Machine Learning Zoomcamp FAQ;2023;Save Docker Image to local machine and view contents ;"The docker image can be saved/exported to tar format in local machine using the below command:

docker image save <image-name> -o <name-of-tar-file.tar>

The individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.

Sumeet Lalla
"
Machine Learning Zoomcamp FAQ;2023;Using Tensorflow 2.15 for AWS deployment;"
Using the 2.14 version with python 3.11 works fine.
In case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4

Added by Abhijit Chakraborty
"
Machine Learning Zoomcamp FAQ;2023;"WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available";"
When running docker build -t dino-dragon-model it returns the above error

The most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format: 

https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl

Pastor Soto
"
Machine Learning Zoomcamp FAQ;2023;What IAM permission policy is needed to complete Week 9: Serverless?;"
Sign in to the AWS Console: Log in to the AWS Console.
Navigate to IAM: Go to the IAM service by clicking on ""Services"" in the top left corner and selecting ""IAM"" under the ""Security, Identity, & Compliance"" section.
Create a new policy: In the left navigation pane, select ""Policies"" and click on ""Create policy.""
Select the service and actions:
Click on ""JSON"" and copy and paste the JSON policy you provided earlier for the specific ECR actions.
Review and create the policy:
Click on ""Review policy.""
Provide a name and description for the policy.
Click on ""Create policy.""

JSON policy:
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
   	 {
   		 ""Sid"": ""VisualEditor0"",
   		 ""Effect"": ""Allow"",
   		 ""Action"": [
   			 ""ecr:CreateRepository"",
   			 ""ecr:GetAuthorizationToken"",
   			 ""ecr:BatchCheckLayerAvailability"",
   			 ""ecr:BatchGetImage"",
   			 ""ecr:InitiateLayerUpload"",
   			 ""ecr:UploadLayerPart"",
   			 ""ecr:CompleteLayerUpload"",
   			 ""ecr:PutImage""
   		 ],
   		 ""Resource"": ""*""
   	 }
    ]
}

Added by: Daniel Muñoz-Viveros


ERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: ""docker-credential-desktop.exe"": executable file not found in $PATH, out: ``

(WSL2 system)
Solved: Delete the file ~/.docker/config.json
										Yishan Zhan"
Machine Learning Zoomcamp FAQ;2023;Windows version might not be up-to-date;"
Problem description:
In command line try to do $ docker build -t dino_dragon
got this Using default tag: latest
[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.
error during connect: This error may indicate that the docker daemon is not running.: Post
.
Solution description:
You need to make sure that Docker is not stopped by a third-party program. 

Andrei Ilin
"
Machine Learning Zoomcamp FAQ;2023;About getting the wrong result when multiplying matrices ;"When multiplying matrices, the order of multiplication is important. 
For example:

A (m x n) * B (n x p) = C (m x p)
B (n x p) * A (m x n) = D (n x n)

C and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.

														Baran Akın
"
Machine Learning Zoomcamp FAQ;2023;BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container;"
You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.

Potential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.


(Memoona Tahira) "
Machine Learning Zoomcamp FAQ;2023;CUDA toolkit and cuDNN Install for Tensorflow;"Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.
Windows:
Install Anaconda prompt https://www.anaconda.com/
Two options:
Install package ‘tensorflow-gpu’ in Anaconda
Install the Tensorflow way https://www.tensorflow.org/install/pip#windows-native
WSL/Linux:
WSL: Use the Windows Nvida drivers, do not touch that.
Two options:
Install the Tensorflow way https://www.tensorflow.org/install/pip#linux_1
Make sure to follow step 4 to install CUDA by environment
Also run:
echo ‘export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
Install CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive
Install https://developer.nvidia.com/rdp/cudnn-download

Now you should be able to do training/inference with GPU in Tensorflow

 (Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 point. Please make sure the posts are valid URLs starting with ""https://"" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (
ANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.

ezehcp7482@gmail.com:
PROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to ‘google’ my way out.
ANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)

"
Machine Learning Zoomcamp FAQ;2023;Can I do the course in other languages, like R or Scala?;"
Technically, yes. Advisable? Not really. Reasons:
Some homework(s) asks for specific python library versions. 
Answers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)
And as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?

You can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.
tx[source]
"
Machine Learning Zoomcamp FAQ;2023;Chart for classes and predictions;"
How to visualize the predictions per classes after training a neural net
Solution description 
classes, predictions = zip(*dict(zip(classes, predictions)).items())
plt.figure(figsize=(12, 3))
plt.bar(classes, predictions)


Luke
"
Machine Learning Zoomcamp FAQ;2023;Convert dictionary values to Dataframe table;"You can convert the prediction output values to a datafarme using 
df = pd.DataFrame.from_dict(dict, orient='index' , columns=[""Prediction""])

												Edidiong Esu
"
Machine Learning Zoomcamp FAQ;2023;Deploying to Digital Ocean;"
You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.

Steps:
Register in DigitalOcean
Go to Apps -> Create App.
You will need to choose GitHub as a service provider.
Edit Source Directory (if your project is not in the repo root)
IMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root
Remember to add model files if they are not built automatically during the container build process.


By Dmytro Durach

"
Machine Learning Zoomcamp FAQ;2023;Do we have to run everything?;"
Problem description:

Do we have to run everything?

You are encouraged, if you can, to run them. As this provides another opportunity to learn from others.

Not everyone will be able to run all the files, in particular the neural networks. 
 
Solution description:

Alternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on. 

Related slack conversation here.
(Gregory Morris)
"
Machine Learning Zoomcamp FAQ;2023;Does the github repository need to be public?;"
Yes. Whoever corrects the homework will only be able to access the link if the repository is public. 

(added by Tano Bugelli)

How to install Conda environment in my local machine?

Which ide is recommended for machine learning?


"
Machine Learning Zoomcamp FAQ;2023;Error UnidentifiedImageError: cannot identify image file  ;"In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:

url = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'
X = preprocessor.from_url(url)
I got the error:
UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>
Solution:
Add ?raw=true after .jpg in url. E.g. as below
url = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’

Bhaskar Sarma

"
Machine Learning Zoomcamp FAQ;2023;Error decoding JSON response: Expecting value: line 1 column 1 (char 0);"Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.
The problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.
												Ahmed Okka


"
Machine Learning Zoomcamp FAQ;2023;Error when running ImageDataGenerator.flow_from_dataframe;"Error: ImageDataGenerator name 'scipy' is not defined.
Check that scipy is installed in your environment.
Restart jupyter kernel and try again.
Marcos MJD
"
Machine Learning Zoomcamp FAQ;2023;"Failed loading Bento from directory /home/bentoml/bento: Failed to import module ""service"": No module named 'sklearn' ";"
I was getting the below error message when I was trying to create docker image using bentoml
[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module ""service"": No module named 'sklearn'
 
Solution description 
The cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.

  packages: # Additional pip packages required by the service
    - xgboost
    - scikit-learn
    - pydantic


Asia Saeed
"
Machine Learning Zoomcamp FAQ;2023;Features in scikit-learn?;"
Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.
Use reshape to reshape a 1D array to a 2D.
							(-Aileah) :>

(added by Tano

filtered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]
# Select only the desired columns
selected_columns = [
    'latitude',
    'longitude',
    'housing_median_age',
    'total_rooms',
    'total_bedrooms',
    'population',
    'households',
    'median_income',
    'median_house_value'
]
filtered_df = filtered_df[selected_columns]
# Display the first few rows of the filtered DataFrame
print(filtered_df.head())
Kris

When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass?

FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead
"
Machine Learning Zoomcamp FAQ;2023;Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?;"The TF and TF Serving versions have to match (as per solution from the slack channel)

Added by Chiedu Elue"
Machine Learning Zoomcamp FAQ;2023;Free cloud alternatives;"Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.
I think .5GB RAM is not enough, is there any other free alternative available ?

A: aws (amazon), gcp (google), saturn. 
Both aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff. 

Saturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:
“You can sign up here: https://bit.ly/saturn-mlzoomcamp
When you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”
Added by Andrii Larkin"
Machine Learning Zoomcamp FAQ;2023;Get_feature_names() not found;"
Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:
Old: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names
New: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names

Solution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))
Ibai Irastorza"
Machine Learning Zoomcamp FAQ;2023;Getting day of the year from day and month column;"
Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?  

Solution description: 
convert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)
convert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()
convert day and month into a datetime object with:

df['date_formatted'] = pd.to_datetime(
dict(
year='2055',
month=df['month'],
day=df['day']
)
)

get day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear

(Bhaskar Sarma)
"
Machine Learning Zoomcamp FAQ;2023;How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?;"You can consider several different approaches:
Sampling: In the exploratory phase, you can use random samples of the data.
Chunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.
Optimizing data types: Pandas’ automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.
Using Dask, an open-source python project which parallelizes Numpy and Pandas.

(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)

By Rileen Sinha
"
Machine Learning Zoomcamp FAQ;2023;How to get feature importance for XGboost model;"Using model.feature_importances_ can gives you an error:

AttributeError: 'Booster' object has no attribute 'feature_importances_'

Answer: if you train the model like this: model = xgb.train you should use get_score() instead

Ekaterina Kutovaia
"
Machine Learning Zoomcamp FAQ;2023;How to handle outliers in a dataset?;"
There are different techniques, but the most common used are the next:
Dataset transformation (for example, log transformation)
Clipping high values
Dropping these observations

Alena Kniazeva

"
Machine Learning Zoomcamp FAQ;2023;How to pass BentoML content / docker container to Amazon Lambda;"Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:
https://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97

Konrad Muehlberg


"
Machine Learning Zoomcamp FAQ;2023;How to use wget with Google Colab?;"
Install w get:
!which wget
Download data:
!wget -P /content/drive/My\ Drive/Downloads/ URL

(added by Paulina Hernandez)

"
Machine Learning Zoomcamp FAQ;2023;I may end up submitting the assignment late. Would it be evaluated?;"
Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.

(Added by Rileen Sinha, based on answer by Alexey on Slack)


"
Machine Learning Zoomcamp FAQ;2023;Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?;"Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.
"
Machine Learning Zoomcamp FAQ;2023;Is it best to train your model only on the most important features?;"
I’m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?

Not necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).
By Rileen Sinha
"
Machine Learning Zoomcamp FAQ;2023;Is there a way to serve up a form for users to enter data for the model to crunch on?;"Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.

You can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md 

Alejandro Aponte
"
Machine Learning Zoomcamp FAQ;2023;"Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be ""too much help""?";"Yes, it’s allowed (as per Alexey).
Added By Rileen Sinha"
Machine Learning Zoomcamp FAQ;2023;Kitchenware Classification Competition Dataset Generator;"
The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them

It can be found here: kitchenware-dataset-generator | Kaggle


Martin Uribe"
Machine Learning Zoomcamp FAQ;2023;Loading the Image with PILLOW library and converting to numpy array;"Pip install pillow - install pillow library
from PIL import Image 
img = Image.open('aeroplane.png') 

From numpy import asarray
numdata=asarray(img)


Krishna Anand


"
Machine Learning Zoomcamp FAQ;2023;Model too big;"If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed. 

                                                                                  Quinn Avila "
Machine Learning Zoomcamp FAQ;2023;None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?;"Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md

(added by Rileen Sinha)
"
Machine Learning Zoomcamp FAQ;2023;Permissions to push docker to Google Container Registry;"When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:

gcloud auth configure-docker
 

(Jesus Acuña)"
Machine Learning Zoomcamp FAQ;2023;Pickle error: can’t get attribute XXX on module __main__;"When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.

This does not happen when Flask is used directly, i.e. not through waitress. 

The problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.

When using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.

Solution: 
Put the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)

Note: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__). 

Detailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules

Marcos MJD
		"
Machine Learning Zoomcamp FAQ;2023;Reproducibility in different OS;"
When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:
```
Warning: Python 3.11 was not found on your system…
Neither ‘pipenv’ nor ‘asdf’ could be found to install Python.
You can specify specific versions of Python with:
$ pipenv –python path\to\python
```

The solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.

(Added by Abhijit Chakraborty)
"
Machine Learning Zoomcamp FAQ;2023;Tflite_runtime unable to install;"I am getting this error message when I tried to install tflite in a pipenv environment

Error:  An error occurred while installing tflite_runtime!
Error text:
ERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)
ERROR: No matching distribution found for tflite_runtime

This version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.

Pastor Soto

Check all available versions here:

https://google-coral.github.io/py-repo/tflite-runtime/

If you don’t find a combination matching your setup, try out the options at

https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite

which you can install as shown in the lecture, e.g. 

pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl

Finally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.

Rileen Sinha (based on discussions on Slack)"
Machine Learning Zoomcamp FAQ;2023;Why do I need to provide a train.py file when I already have the notebook.ipynb file? ;"
Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv. 

Odimegwu David"
Machine Learning Zoomcamp FAQ;2023;[Errno 12] Cannot allocate memory in AWS Elastic Container Service;"
In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.

Just increase the RAM and CPU in your task definition.

Humberto Rodriguez
"
Machine Learning Zoomcamp FAQ;2023;[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies;"
Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.
Solution: Run: ` pipenv lock` for fix this problem and dependency files 
Alejandro Aponte
"
Machine Learning Zoomcamp FAQ;2023;Are projects solo or collaborative/group work?;"
Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]
"
Machine Learning Zoomcamp FAQ;2023;Computing the hash for project review;"
See the answer here.

"
Machine Learning Zoomcamp FAQ;2023;Crucial Links;"These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.

Midterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project
MidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects
Submit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform
Datasets:
https://www.kaggle.com/datasets and https://www.kaggle.com/competitions
https://archive.ics.uci.edu/ml/index.php
https://data.europa.eu/en
https://www.openml.org/search?type=data
https://newzealand.ai/public-data-sets
https://datasetsearch.research.google.com
What to do and Deliverables
Think of a problem that's interesting for you and find a dataset for that
Describe this problem and explain how a model could be used
Prepare the data and doing EDA, analyze important features
Train multiple models, tune their performance and select the best model
Export the notebook into a script
Put your model into a web service and deploy it locally with Docker
Bonus points for deploying the service to the cloud
"
Machine Learning Zoomcamp FAQ;2023;How does the project evaluation work for you as a peer reviewer?;" I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.
Answer:
The link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.
To calculate your hash value run the python code below:
from hashlib import sha1

def compute_hash(email):
    return sha1(email.lower().encode('utf-8')).hexdigest()

# Example usage **** enter your email below (Example1@gmail.com)****
email = ""Example1@gmail.com""
hashed_email = compute_hash(email)
print(""Original Email:"", email)
print(""Hashed Email (SHA-1):"", hashed_email)
Edit the above code to replace Example1@gmail.com as your email address
Store and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value

You then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true
Lastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.

										By Emmanuel Ayeni

  "
Machine Learning Zoomcamp FAQ;2023;How many models should I train?;"Regarding Point 4 in the midterm deliverables, which states, ""Train multiple models, tune their performance, and select the best model,"" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term ""multiple"" implies having more than one model, so as long as you have more than one, you're on the right track."
Machine Learning Zoomcamp FAQ;2023;How to conduct peer reviews for projects?;"Answer: Previous cohorts projects page has instructions (youtube).
https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project
Alexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.

									      ~~~ Added by Nukta Bhatia ~~~"
Machine Learning Zoomcamp FAQ;2023;Learning in public links for the projects;"For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?

14 posts, one for each day
"
Machine Learning Zoomcamp FAQ;2023;My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?;"
You can use git-lfs (https://git-lfs.com/) for upload large file to github repository. 
Ryan Pramana
"
Machine Learning Zoomcamp FAQ;2023;What If I submitted only two projects and failed to submit the third?;"
If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate. 
(optional) David Odimegwu
"
Machine Learning Zoomcamp FAQ;2023;What are the project deadlines?;"Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.
"
Machine Learning Zoomcamp FAQ;2023;What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?;"Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class. 
Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.
More discussions:
[source1] [source2] [source3]"
